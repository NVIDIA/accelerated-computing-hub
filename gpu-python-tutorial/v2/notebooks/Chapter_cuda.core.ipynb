{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9a5bf9",
   "metadata": {},
   "source": [
    "# CUDA Core Tutorial - Low-Level GPU Programming\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to cuda.core\n",
    "2. Setting Up Your Environment\n",
    "3. Understanding CUDA Concepts\n",
    "4. Memory Management\n",
    "5. Kernel Compilation and Execution\n",
    "6. Streams and Synchronization\n",
    "7. Error Handling\n",
    "8. Performance Optimization\n",
    "9. Practical Examples\n",
    "10. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a456f7",
   "metadata": {},
   "source": [
    "## 1. Introduction to cuda.core\n",
    "The `cuda.core` module provides direct access to the CUDA driver API, giving you maximum control over GPU programming. Unlike high-level APIs, cuda.core requires you to manage everything manually:\n",
    "\n",
    "* Context management: Creating and managing execution contexts\n",
    "* Memory allocation: Explicitly allocating and freeing GPU memory\n",
    "* Kernel compilation: Compiling CUDA C/C++ code at runtime\n",
    "* Synchronization: Managing streams and events\n",
    "\n",
    "### When to use cuda.core:\n",
    "**Great for:**\n",
    "* Learning GPU programming in a Python-friendly way\n",
    "* Prototyping GPU algorithms quickly\n",
    "* Custom parallel algorithms that existing libraries don't provide\n",
    "* When you need fine control over GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b043e1",
   "metadata": {},
   "source": [
    "## 2. Setting Up Your Environment\n",
    "### Prerequisites\n",
    "\n",
    "* NVIDIA GPU with CUDA capability\n",
    "* CUDA driver version 12.2 or higher\n",
    "* Python 3.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dbcf1",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0771178",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2111655890.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 -m pip install cuda-python numpy\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install cuda-core numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ba0c0",
   "metadata": {},
   "source": [
    "**What this does**:\n",
    "* `cuda-core`: Installs the CUDA Core Python package\n",
    "* `numpy`: Installs NumPy for array operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2622788",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea7dbe",
   "metadata": {},
   "source": [
    "The following Python snippet checks whether CUDA is everything is set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import system, Device\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA driver version\n",
    "print(f\"CUDA driver version: {system.driver_version}\")\n",
    "\n",
    "# Get number of available devices  \n",
    "print(f\"Number of CUDA devices: {system.num_devices}\")\n",
    "\n",
    "# Get device information\n",
    "if system.num_devices > 0:\n",
    "    device = Device(0)  # Get the first GPU\n",
    "    device.set_current()  # Tell CUDA we want to use this GPU\n",
    "    print(f\"Device name: {device.name}\")\n",
    "    print(f\"Device UUID: {device.uuid}\")\n",
    "    print(f\"PCI Bus ID: {device.pci_bus_id}\")\n",
    "else:\n",
    "    print(\"No CUDA devices found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df933877",
   "metadata": {},
   "source": [
    "This is a good first step before running any GPU workloads. If this fails, your drivers or installation may be incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76daf10e",
   "metadata": {},
   "source": [
    "## 3. Understanding CUDA Concepts\n",
    "### Key Terminology\n",
    "**Host vs Device:**\n",
    "\n",
    "* **Host**: Your CPU and system memory\n",
    "* **Device**: Your GPU and video memory\n",
    "\n",
    "**Execution Model**:\n",
    "\n",
    "* **Kernel**: A function that runs on the GPU\n",
    "* **Thread**: Individual execution unit\n",
    "* **Block**: Group of threads that can cooperate\n",
    "* **Grid**: Collection of blocks\n",
    "\n",
    "**Memory Hierarchy**:\n",
    "\n",
    "* **Global Memory**: Main GPU memory (slow but large)\n",
    "* **Shared Memory**: Fast memory shared within a block\n",
    "* **Registers**: Fastest memory, private to each thread\n",
    "\n",
    "Think of registers as your desk (fast access, limited space), shared memory as your team's filing cabinet (fast for team members, more space), and global memory as the company warehouse (lots of space, but takes time to fetch things)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e7496",
   "metadata": {},
   "source": [
    "### Device Management and Context\n",
    "**What is a Device and Context?**\n",
    "In CUDA, a **Device** represents your GPU hardware. A **Context** is like a workspace on that GPU where your programs can run. Think of the Device as the physical GPU card, and the Context as your personal workspace on that card."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2368b7",
   "metadata": {},
   "source": [
    "Try starting with basic device management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device\n",
    "\n",
    "# Get the first GPU device (device 0)\n",
    "device = Device(0)\n",
    "\n",
    "# Set as current device (this creates and activates a context)\n",
    "device.set_current()\n",
    "\n",
    "print(f\"Using device: {device.name}\")\n",
    "print(f\"Device ordinal: {device.ordinal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d9594",
   "metadata": {},
   "source": [
    "**What this does**:\n",
    "1. `Device(0)`: Creates a Device object representing the first GPU (GPU numbering starts at 0)\n",
    "1. `device.set_current()`: Tells CUDA \"I want to use this GPU for my operations\"\n",
    "\n",
    "If you have multiple GPUs, CUDA needs to know which one you want to use, which is why we need `set_current`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeef144",
   "metadata": {},
   "source": [
    "### Getting Device Properties\n",
    "Let's learn more about our GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device\n",
    "\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Get device properties\n",
    "props = device.properties\n",
    "print(f\"Device name: {device.name}\")\n",
    "print(f\"Compute capability: {props.major}.{props.minor}\")\n",
    "print(f\"Total global memory: {props.total_global_mem // (1024**3)} GB\")\n",
    "print(f\"Multiprocessor count: {props.multi_processor_count}\")\n",
    "print(f\"Max threads per block: {props.max_threads_per_block}\")\n",
    "print(f\"Max block dimensions: ({props.max_block_dim_x}, {props.max_block_dim_y}, {props.max_block_dim_z})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034ea5b",
   "metadata": {},
   "source": [
    "**What these properties mean**:\n",
    "\n",
    "* Compute capability: Like a GPU \"version number\", where higher numbers support more features\n",
    "* Total global memory: How much video RAM your GPU has\n",
    "* Multiprocessor count: How many \"processor groups\" your GPU has (more = more parallel power)\n",
    "* Max threads per block: Maximum number of threads you can put in one block\n",
    "* Max block dimensions: How you can arrange threads in a block (1D, 2D, or 3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04351b",
   "metadata": {},
   "source": [
    "### Device Synchronization\n",
    "Sometimes you need to wait for the GPU to finish all its work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# Wait for all previous operations on this device to complete\n",
    "device.synchronize()\n",
    "print(\"All GPU operations are now complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd5dbf",
   "metadata": {},
   "source": [
    "**When do you need synchronization?**\n",
    "* Before copying results back from GPU to CPU\n",
    "* Before timing how long GPU operations took\n",
    "* Before shutting down your program\n",
    "\n",
    "Almost as if you're saying: \"don't continue until all workers finish their current tasks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340cd1b6",
   "metadata": {},
   "source": [
    "## 4. Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b179d94",
   "metadata": {},
   "source": [
    "### Understanding GPU Memory\n",
    "GPU memory is separate from your computer's main memory (RAM). To use the GPU, you need to:\n",
    "* Allocate space in GPU memory\n",
    "* Copy your data from CPU to GPU\n",
    "* Process the data on the GPU\n",
    "* Copy results back from GPU to CPU\n",
    "\n",
    "CPUs and GPUs have separate memory systems optimized for their different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda.core.experimental import Device\n",
    "\n",
    "# Initialize our GPU\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Calculate how much memory we need\n",
    "# We want to store 1000 float32 numbers\n",
    "# Each float32 takes 4 bytes, so we need 1000 * 4 = 4000 bytes\n",
    "size_bytes = 1000 * 4  \n",
    "\n",
    "# Allocate memory on the GPU\n",
    "device_buffer = device.allocate(size_bytes)\n",
    "\n",
    "print(f\"Allocated {size_bytes} bytes on GPU\")\n",
    "print(f\"Buffer memory address: {device_buffer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf168b",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "1. Calculate size: We figure out how many bytes we need (1000 floats × 4 bytes each)\n",
    "2. Allocate memory: device.allocate() reserves space on the GPU\n",
    "3. Get a buffer: The returned device_buffer is like a \"handle\" to our GPU memory\n",
    "\n",
    "**Important**: Just like with regular Python programming, allocating memory doesn't put any meaningful data there yet. It's just reserved empty space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef81c7",
   "metadata": {},
   "source": [
    "### Memory Transfer\n",
    "\n",
    "Below is a complete example of moving data between CPU and GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93417b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda.core.experimental import Device\n",
    "\n",
    "def demonstrate_memory_operations():\n",
    "    # Step 1: Initialize device\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    # Step 2: Create some data on the CPU (host)\n",
    "    host_data = np.arange(1000, dtype=np.float32)  # Creates [0, 1, 2, ..., 999]\n",
    "    print(f\"Created host data with shape: {host_data.shape}\")\n",
    "    \n",
    "    # Step 3: Allocate memory on the GPU (device)\n",
    "    device_buffer = device.allocate(host_data.nbytes)  # nbytes = number of bytes\n",
    "    print(f\"Allocated {host_data.nbytes} bytes on GPU\")\n",
    "    \n",
    "    # Step 4: Copy data from CPU to GPU\n",
    "    device_buffer.copy_from(host_data)\n",
    "    print(\"Data copied from CPU to GPU\")\n",
    "    \n",
    "    # Step 5: Allocate space for results\n",
    "    result_buffer = device.allocate(host_data.nbytes)\n",
    "    print(\"Allocated result buffer on GPU\")\n",
    "    \n",
    "    # Step 6: Copy data back to CPU (after processing)\n",
    "    result = np.zeros_like(host_data)  # Create empty array same size as input\n",
    "    device_buffer.copy_to(result)  # Copy from GPU to CPU\n",
    "    print(\"Data copied from GPU back to CPU\")\n",
    "    \n",
    "    # Step 7: Verify the data made the round trip correctly\n",
    "    print(f\"Data matches: {np.array_equal(host_data, result)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the demonstration\n",
    "result = demonstrate_memory_operations()\n",
    "print(f\"Final result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b8e22",
   "metadata": {},
   "source": [
    "**Step-by-step explanation:**\n",
    "1. Create host data: We make a NumPy array with 1000 numbers\n",
    "2. Allocate GPU memory: Reserve space on the GPU for our data\n",
    "3. Copy to GPU: Move our data from CPU memory to GPU memory\n",
    "4. Allocate result space: Reserve space for the results of our computation\n",
    "5. Copy back: Move the processed data from GPU back to CPU\n",
    "6. Verify: Check that our data survived the round trip\n",
    "\n",
    "**Key Methods:**\n",
    "* `.nbytes`: NumPy property that tells you how many bytes an array uses\n",
    "* `.copy_from()`: Copies data FROM the CPU TO the GPU\n",
    "* `.copy_to()`: Copies data FROM the GPU TO the CPU\n",
    "* `np.zeros_like()`: Creates an empty array with the same shape and type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86c5a",
   "metadata": {},
   "source": [
    "## 5. Kernel Compilation and Execution\n",
    "### First Kernel: Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device, Program\n",
    "import numpy as np\n",
    "\n",
    "# CUDA C source code for our kernel\n",
    "vector_add_source = \"\"\"\n",
    "extern \"C\" __global__ void vector_add(float *a, float *b, float *c, int n) {\n",
    "    // Each thread calculates its unique index\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Make sure we don't go beyond our array bounds\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] + b[i];  // Add corresponding elements\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize our GPU\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Compile the CUDA code into a program\n",
    "program = Program(vector_add_source)\n",
    "compiled_program = program.compile()\n",
    "\n",
    "# Get the specific kernel function we want to use\n",
    "kernel = compiled_program.get_kernel(\"vector_add\")\n",
    "\n",
    "print(\"Kernel compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c9216",
   "metadata": {},
   "source": [
    "**Breakdown:**\n",
    "\n",
    "1. `extern \"C\"`: Tells the compiler to use C-style function names\n",
    "2. `__global__`: Marks this as a kernel function (runs on GPU)\n",
    "3. `float *a, float *b, float *c`: Pointers to arrays in GPU memory\n",
    "4. `int i = blockIdx.x * blockDim.x + threadIdx.x`: Calculates unique thread ID\n",
    "* blockIdx.x: Which block this thread belongs to\n",
    "* blockDim.x: How many threads are in each block\n",
    "* threadIdx.x: Position of this thread within its block\n",
    "5. `if (i < n)`: Safety check to avoid accessing invalid memory\n",
    "6. `c[i] = a[i] + b[i]`: The actual computation each thread performs\n",
    "\n",
    "**Why the complex index calculation?**\n",
    "Imagine you have 1000 elements to process with blocks of 256 threads:\n",
    "* Block 0: threads 0-255 handle elements 0-255\n",
    "* Block 1: threads 0-255 handle elements 256-511\n",
    "* Block 2: threads 0-255 handle elements 512-767\n",
    "* Block 3: threads 0-255 handle elements 768-999\n",
    "\n",
    "Each thread needs to know which element it should work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc566b",
   "metadata": {},
   "source": [
    "### Executing the Kernel\n",
    "Now we can use our compiled kernel to actually add two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import launch, LaunchConfig\n",
    "\n",
    "def execute_vector_add():\n",
    "    # Step 1: Initialize device\n",
    "    device = Device(0) \n",
    "    device.set_current()\n",
    "    \n",
    "    # Step 2: Prepare our test data\n",
    "    N = 1000  # Number of elements\n",
    "    a = np.arange(N, dtype=np.float32)      # [0, 1, 2, ..., 999]\n",
    "    b = np.arange(N, dtype=np.float32)      # [0, 1, 2, ..., 999]\n",
    "    print(f\"Input arrays have {N} elements each\")\n",
    "    \n",
    "    # Step 3: Allocate GPU memory for all our arrays\n",
    "    d_a = device.allocate(a.nbytes)  # GPU memory for array 'a'\n",
    "    d_b = device.allocate(b.nbytes)  # GPU memory for array 'b'  \n",
    "    d_c = device.allocate(a.nbytes)  # GPU memory for result 'c'\n",
    "    \n",
    "    # Step 4: Copy input data from CPU to GPU\n",
    "    d_a.copy_from(a)\n",
    "    d_b.copy_from(b)\n",
    "    print(\"Input data copied to GPU\")\n",
    "    \n",
    "    # Step 5: Configure how to launch the kernel\n",
    "    block_size = 256  # Number of threads per block\n",
    "    grid_size = (N + block_size - 1) // block_size  # Number of blocks needed\n",
    "    \n",
    "    print(f\"Launch configuration: {grid_size} blocks of {block_size} threads each\")\n",
    "    print(f\"Total threads: {grid_size * block_size}\")\n",
    "    \n",
    "    # Create the launch configuration\n",
    "    config = LaunchConfig(grid=(grid_size,), block=(block_size,))\n",
    "    \n",
    "    # Step 6: Launch the kernel!\n",
    "    launch(kernel, config, d_a, d_b, d_c, np.int32(N))\n",
    "    print(\"Kernel launched and executed\")\n",
    "    \n",
    "    # Step 7: Copy the result back from GPU to CPU\n",
    "    c = np.zeros(N, dtype=np.float32)  # Create empty result array\n",
    "    d_c.copy_to(c)\n",
    "    print(\"Results copied back to CPU\")\n",
    "    \n",
    "    return c\n",
    "\n",
    "# Execute our vector addition\n",
    "result = execute_vector_add()\n",
    "\n",
    "# Verify the result\n",
    "expected = np.arange(1000, dtype=np.float32) * 2  # [0, 2, 4, ..., 1998]\n",
    "success = np.allclose(result, expected)\n",
    "print(f\"Kernel execution successful: {success}\")\n",
    "print(f\"First 10 results: {result[:10]}\")\n",
    "print(f\"Expected first 10: {expected[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987fc87",
   "metadata": {},
   "source": [
    "**Launch Configuration Deep Dive:**\n",
    "* Block size: 256 threads per block (common choice, powers of 2 work well)\n",
    "* Grid size: `(N + block_size - 1) // block_size` ensures we have enough threads\n",
    "    * For N=1000 and block_size=256: grid_size = (1000 + 255) // 256 = 4 blocks\n",
    "    * Total threads = 4 × 256 = 1024 threads (more than our 1000 elements, which is fine)\n",
    "\n",
    "**Why Use grid_size calculation?**\n",
    "\n",
    "This formula ensures we always have enough threads:\n",
    "* If N=1000 and block_size=256, we need at least 4 blocks\n",
    "* If N=256 and block_size=256, we need exactly 1 block\n",
    "* If N=257 and block_size=256, we need 2 blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d06f97",
   "metadata": {},
   "source": [
    "### Advanced Kernel Example\n",
    "\n",
    "We can now try multiplying two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication kernel\n",
    "matmul_source = \"\"\"\n",
    "extern \"C\" __global__ void matrix_multiply(float *A, float *B, float *C, int N) {\n",
    "    // Calculate which row and column this thread handles\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Make sure we're within the matrix bounds\n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        // Compute dot product of row from A and column from B\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        \n",
    "        // Store the result\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def matrix_multiply_gpu(A, B):\n",
    "    # Step 1: Initialize device and verify inputs\n",
    "    device = Device(0)\n",
    "    device.set_current() \n",
    "    \n",
    "    N = A.shape[0]\n",
    "    assert A.shape == (N, N) and B.shape == (N, N), \"Matrices must be square and same size\"\n",
    "    print(f\"Multiplying {N}x{N} matrices\")\n",
    "    \n",
    "    # Step 2: Compile the matrix multiplication kernel\n",
    "    program = Program(matmul_source)\n",
    "    kernel = program.compile().get_kernel(\"matrix_multiply\")\n",
    "    \n",
    "    # Step 3: Allocate GPU memory\n",
    "    d_A = device.allocate(A.nbytes)\n",
    "    d_B = device.allocate(B.nbytes)\n",
    "    d_C = device.allocate(A.nbytes)\n",
    "    \n",
    "    # Step 4: Copy input matrices to GPU\n",
    "    d_A.copy_from(A)\n",
    "    d_B.copy_from(B)\n",
    "    \n",
    "    # Step 5: Configure 2D launch (threads arranged in a 2D grid)\n",
    "    block_size = 16  # 16x16 = 256 threads per block\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    \n",
    "    print(f\"Launch config: {grid_size}x{grid_size} blocks of {block_size}x{block_size} threads\")\n",
    "    \n",
    "    # Create 2D launch configuration\n",
    "    config = LaunchConfig(grid=(grid_size, grid_size), block=(block_size, block_size))\n",
    "    \n",
    "    # Step 6: Launch the kernel\n",
    "    launch(kernel, config, d_A, d_B, d_C, np.int32(N))\n",
    "    \n",
    "    # Step 7: Copy result back\n",
    "    C = np.zeros_like(A)\n",
    "    d_C.copy_to(C)\n",
    "    \n",
    "    print(\"Matrix multiplication completed on GPU\")\n",
    "    return C\n",
    "\n",
    "# Test matrix multiplication\n",
    "print(\"Testing matrix multiplication...\")\n",
    "A = np.random.random((64, 64)).astype(np.float32)\n",
    "B = np.random.random((64, 64)).astype(np.float32)\n",
    "\n",
    "# Compare GPU result with CPU result\n",
    "C_gpu = matrix_multiply_gpu(A, B)\n",
    "C_cpu = np.dot(A, B)  # NumPy's optimized matrix multiplication\n",
    "\n",
    "# Check if results match (within floating-point precision)\n",
    "matches = np.allclose(C_gpu, C_cpu, atol=1e-5)\n",
    "print(f\"GPU and CPU results match: {matches}\")\n",
    "\n",
    "if matches:\n",
    "    print(\"Success! Matrix multiplication kernel works correctly.\")\n",
    "else:\n",
    "    print(\"Results don't match - there might be a bug in the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f5601",
   "metadata": {},
   "source": [
    "**Kernel Explanation:**\n",
    "\n",
    "1. 2D Thread Layout: Each thread handles one element of the result matrix\n",
    "* row = blockIdx.y * blockDim.y + threadIdx.y: Which row this thread computes\n",
    "* col = blockIdx.x * blockDim.x + threadIdx.x: Which column this thread computes\n",
    "2. Dot Product Calculation: For element C[row][col], we compute:\n",
    "* Sum of A[row][k] × B[k][col] for all k\n",
    "* This is the mathematical definition of matrix multiplication\n",
    "3. 2D Launch Configuration:\n",
    "* Blocks are arranged in a 2D grid to match the 2D nature of matrices\n",
    "* Each block is 16×16 threads (256 total threads per block)\n",
    "\n",
    "**Why 2D layout?**\n",
    "\n",
    "Matrix multiplication naturally maps to 2D: each thread computes one output element, and output elements are arranged in a 2D grid (the result matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ec75a",
   "metadata": {},
   "source": [
    "## 6. Streams and Synchronization\n",
    "### Understanding Streams\n",
    "A Stream in CUDA is like a queue of operations that execute in order on the GPU. Think of it as a to-do list that the GPU works through sequentially.\n",
    "\n",
    "By creating your own streams, you can:\n",
    "* Overlap operations: Copy data while computing on other data\n",
    "* Multiple tasks: Run different kernels at the same time\n",
    "* Better GPU utilization: Keep the GPU busy with work\n",
    "\n",
    "Default behavior: If you don't specify a stream, CUDA uses a \"default stream\" where all operations happen one after another.\n",
    "\n",
    "Let's see how to create and use streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d25de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device, Stream, launch, LaunchConfig\n",
    "\n",
    "def demonstrate_streams():\n",
    "    # Step 1: Initialize device\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    # Step 2: Create a custom stream\n",
    "    stream = Stream()\n",
    "    print(\"Created custom stream\")\n",
    "    \n",
    "    # Step 3: Prepare test data\n",
    "    N = 1000\n",
    "    a = np.arange(N, dtype=np.float32)\n",
    "    b = np.arange(N, dtype=np.float32)\n",
    "    \n",
    "    # Step 4: Allocate GPU memory\n",
    "    d_a = device.allocate(a.nbytes)\n",
    "    d_b = device.allocate(b.nbytes)\n",
    "    d_c = device.allocate(a.nbytes)\n",
    "    \n",
    "    # Step 5: Copy data to GPU (these operations go into the stream)\n",
    "    d_a.copy_from(a)\n",
    "    d_b.copy_from(b)\n",
    "    print(\"Data copied to GPU\")\n",
    "    \n",
    "    # Step 6: Launch kernel in our custom stream\n",
    "    config = LaunchConfig(grid=(4,), block=(256,))\n",
    "    launch(stream, config, kernel, d_a, d_b, d_c, np.int32(N))\n",
    "    print(\"Kernel launched in custom stream\")\n",
    "    \n",
    "    # Step 7: Copy result back (also goes into the stream)\n",
    "    c = np.zeros(N, dtype=np.float32)\n",
    "    d_c.copy_to(c)\n",
    "    \n",
    "    # Step 8: Wait for all operations in the stream to complete\n",
    "    stream.synchronize()\n",
    "    print(\"Stream operations completed\")\n",
    "    \n",
    "    return c\n",
    "\n",
    "result = demonstrate_streams()\n",
    "print(f\"Stream execution successful: {np.allclose(result, np.arange(1000) * 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400d9de",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "1. Create stream: `Stream()` creates a new operation queue\n",
    "2. Queue operations: Memory copies and kernel launches go into the stream\n",
    "3. Asynchronous execution: Operations start immediately but may not finish right away\n",
    "4. Synchronization: `stream.synchronize()` waits for everything to complete\n",
    "\n",
    "**Why synchronize?**\n",
    "\n",
    "Without synchronization, your Python program might try to use results before the GPU finishes computing them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9e031",
   "metadata": {},
   "source": [
    "### Events for Timing\n",
    "\n",
    "Events allow you to measure hwo long GPU operations take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebde116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Event\n",
    "\n",
    "def time_kernel_execution():\n",
    "    # Step 1: Setup\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    # Step 2: Create timing events\n",
    "    start_event = Event()\n",
    "    end_event = Event()\n",
    "    print(\"Created timing events\")\n",
    "    \n",
    "    # Step 3: Prepare larger dataset for timing\n",
    "    N = 1000000  # 1 million elements\n",
    "    a = np.random.random(N).astype(np.float32)\n",
    "    b = np.random.random(N).astype(np.float32)\n",
    "    print(f\"Prepared data with {N} elements\")\n",
    "    \n",
    "    # Step 4: Allocate and copy data\n",
    "    d_a = device.allocate(a.nbytes)\n",
    "    d_b = device.allocate(b.nbytes)\n",
    "    d_c = device.allocate(a.nbytes)\n",
    "    \n",
    "    d_a.copy_from(a)\n",
    "    d_b.copy_from(b)\n",
    "    \n",
    "    # Step 5: Record start time\n",
    "    start_event.record()\n",
    "    \n",
    "    # Step 6: Launch kernel (the operation we want to time)\n",
    "    block_size = 256\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    config = LaunchConfig(grid=(grid_size,), block=(block_size,))\n",
    "    launch(kernel, config, d_a, d_b, d_c, np.int32(N))\n",
    "    \n",
    "    # Step 7: Record end time\n",
    "    end_event.record()\n",
    "    \n",
    "    # Step 8: Wait for the end event and calculate time\n",
    "    end_event.synchronize()\n",
    "    elapsed_time = Event.elapsed_time(start_event, end_event)\n",
    "    \n",
    "    print(f\"Kernel execution took: {elapsed_time:.2f} milliseconds\")\n",
    "    \n",
    "    # Step 9: Calculate performance metrics\n",
    "    elements_per_second = N / (elapsed_time / 1000.0)  # Convert ms to seconds\n",
    "    print(f\"Processed {elements_per_second:.0f} elements per second\")\n",
    "    \n",
    "    return elapsed_time\n",
    "\n",
    "# Time our kernel execution\n",
    "execution_time = time_kernel_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194bffb",
   "metadata": {},
   "source": [
    "**Understanding the timing:**\n",
    "1. Events as markers: Think of events as stopwatch clicks\n",
    "2. Record timing: `start_event.record()` marks the beginning\n",
    "3. Synchronize: `end_event.synchronize()` waits for the operation to finish\n",
    "4. Calculate: `Event.elapsed_time()` gives us the duration in milliseconds\n",
    "\n",
    "**Why use events instead of Python's time.time()?**\n",
    "* GPU operations are asynchronous - they start but don't block Python\n",
    "* Events measure actual GPU execution time, not Python overhead\n",
    "* More accurate for performance analysis\n",
    "\n",
    "Multiple Streams for Parallelism\n",
    "Advanced usage: using multiple streams to overlap operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ca2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_multiple_streams():\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    # Create multiple streams\n",
    "    stream1 = Stream()\n",
    "    stream2 = Stream()\n",
    "    print(\"Created two streams for parallel execution\")\n",
    "    \n",
    "    # Prepare data for both streams\n",
    "    N = 100000\n",
    "    data1_a = np.random.random(N).astype(np.float32)\n",
    "    data1_b = np.random.random(N).astype(np.float32)\n",
    "    data2_a = np.random.random(N).astype(np.float32)\n",
    "    data2_b = np.random.random(N).astype(np.float32)\n",
    "    \n",
    "    # Allocate memory for both computations\n",
    "    d1_a = device.allocate(data1_a.nbytes)\n",
    "    d1_b = device.allocate(data1_b.nbytes)\n",
    "    d1_c = device.allocate(data1_a.nbytes)\n",
    "    \n",
    "    d2_a = device.allocate(data2_a.nbytes)\n",
    "    d2_b = device.allocate(data2_b.nbytes)\n",
    "    d2_c = device.allocate(data2_a.nbytes)\n",
    "    \n",
    "    # Launch operations in parallel streams\n",
    "    print(\"Launching operations in parallel...\")\n",
    "    \n",
    "    # Stream 1 operations\n",
    "    d1_a.copy_from(data1_a)\n",
    "    d1_b.copy_from(data1_b)\n",
    "    config = LaunchConfig(grid=((N + 255) // 256,), block=(256,))\n",
    "    launch(stream1, config, kernel, d1_a, d1_b, d1_c, np.int32(N))\n",
    "    \n",
    "    # Stream 2 operations (can run concurrently with stream 1)\n",
    "    d2_a.copy_from(data2_a)\n",
    "    d2_b.copy_from(data2_b)\n",
    "    launch(stream2, config, kernel, d2_a, d2_b, d2_c, np.int32(N))\n",
    "    \n",
    "    # Wait for both streams to complete\n",
    "    stream1.synchronize()\n",
    "    stream2.synchronize()\n",
    "    print(\"Both streams completed\")\n",
    "    \n",
    "    # Copy results back\n",
    "    result1 = np.zeros(N, dtype=np.float32)\n",
    "    result2 = np.zeros(N, dtype=np.float32)\n",
    "    d1_c.copy_to(result1)\n",
    "    d2_c.copy_to(result2)\n",
    "    \n",
    "    return result1, result2\n",
    "\n",
    "# Demonstrate parallel execution\n",
    "result1, result2 = demonstrate_multiple_streams()\n",
    "print(\"Multiple stream execution completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cd6de",
   "metadata": {},
   "source": [
    "**Benefits of multiple streams:**\n",
    "1. Parallel execution: If your GPU has resources, both kernels can run simultaneously\n",
    "2. Better utilization: Keeps more of the GPU busy\n",
    "3. Overlapping operations: Data transfer in one stream while computing in another\n",
    "\n",
    "Important notes:\n",
    "* Not all operations can run truly in parallel (depends on GPU resources)\n",
    "* Each stream maintains its own order of operations\n",
    "* Synchronization ensures all work completes before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9258bd0",
   "metadata": {},
   "source": [
    "## 7. Error Handling\n",
    "When working at the low-level CUDA driver API level, you are responsible for checking erros after every API call\n",
    "\n",
    "GPU programming can fail in many ways:\n",
    "* Out of memory: Asking for more GPU memory than available\n",
    "* Invalid kernels: Bugs in CUDA C code\n",
    "* Device errors: Hardware problems or driver issues\n",
    "* Launch failures: Invalid grid/block configurations\n",
    "\n",
    "Good error handling helps you:\n",
    "* Debug problems quickly\n",
    "* Write robust applications\n",
    "* Provide helpful error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cuda_operation():\n",
    "    device = None\n",
    "    try:\n",
    "        print(\"Attempting CUDA operation...\")\n",
    "        \n",
    "        # Initialize device (this can fail)\n",
    "        device = Device(0)\n",
    "        device.set_current()\n",
    "        print(\"✓ Device initialized successfully\")\n",
    "        \n",
    "        # Allocate memory (this can fail if requesting too much)\n",
    "        buffer = device.allocate(1000 * 4)\n",
    "        print(\"✓ Memory allocated successfully\")\n",
    "        \n",
    "        # Your CUDA operations here\n",
    "        print(\"✓ All CUDA operations completed successfully\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"✗ CUDA Runtime Error: {e}\")\n",
    "        print(\"This usually means a problem with GPU drivers or hardware\")\n",
    "        \n",
    "    except MemoryError as e:\n",
    "        print(f\"✗ GPU Memory Error: {e}\")\n",
    "        print(\"Try reducing the size of your data or closing other GPU programs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Unexpected error occurred: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup happens automatically in cuda.core\n",
    "        # But you can add custom cleanup here if needed\n",
    "        if device is not None:\n",
    "            print(\"✓ Cleanup completed\")\n",
    "\n",
    "# Test error handling\n",
    "safe_cuda_operation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff3f16",
   "metadata": {},
   "source": [
    "Error handling best practices:\n",
    "1. Use try-except blocks: Wrap CUDA operations in try-except\n",
    "2. Specific exceptions: Catch specific error types when possible\n",
    "3. Helpful messages: Explain what went wrong and how to fix it\n",
    "4. Cleanup: Use finally blocks for cleanup code\n",
    "5. Don't ignore errors: Always handle or propagate exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391999d2",
   "metadata": {},
   "source": [
    "### Common Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_common_errors():\n",
    "    errors_encountered = []\n",
    "    \n",
    "    # Error 1: Invalid device number\n",
    "    print(\"Testing invalid device...\")\n",
    "    try:\n",
    "        invalid_device = Device(999)  # Device 999 probably doesn't exist\n",
    "        invalid_device.set_current()\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Invalid device error: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "    \n",
    "    # Error 2: Memory allocation failure\n",
    "    print(\"\\nTesting memory allocation failure...\")\n",
    "    try:\n",
    "        device = Device(0)\n",
    "        device.set_current()\n",
    "        \n",
    "        # Try to allocate an impossibly large amount of memory (1TB)\n",
    "        huge_amount = 1024 * 1024 * 1024 * 1024  # 1TB in bytes\n",
    "        huge_alloc = device.allocate(huge_amount)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Memory allocation error: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "    \n",
    "    # Error 3: Invalid kernel compilation\n",
    "    print(\"\\nTesting kernel compilation failure...\")\n",
    "    try:\n",
    "        # This CUDA code has syntax errors\n",
    "        bad_source = \"\"\"\n",
    "        extern \"C\" __global__ void broken_kernel( {\n",
    "            // Missing closing brace and parameter list\n",
    "            int i = this_variable_doesnt_exist;\n",
    "        \"\"\"\n",
    "        program = Program(bad_source)\n",
    "        program.compile()\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Kernel compilation error: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "    \n",
    "    # Error 4: Invalid launch configuration\n",
    "    print(\"\\nTesting invalid launch configuration...\")\n",
    "    try:\n",
    "        device = Device(0)\n",
    "        device.set_current()\n",
    "        \n",
    "        # Create a valid kernel first\n",
    "        valid_source = \"\"\"\n",
    "        extern \"C\" __global__ void test_kernel(float *data, int n) {\n",
    "            int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (i < n) data[i] = i;\n",
    "        }\n",
    "        \"\"\"\n",
    "        program = Program(valid_source)\n",
    "        kernel = program.compile().get_kernel(\"test_kernel\")\n",
    "        \n",
    "        # Try to launch with invalid configuration (0 threads per block)\n",
    "        bad_config = LaunchConfig(grid=(1,), block=(0,))  # 0 threads is invalid\n",
    "        \n",
    "        # This should fail\n",
    "        data = device.allocate(100 * 4)\n",
    "        launch(kernel, bad_config, data, np.int32(100))\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Invalid launch configuration: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "    \n",
    "    print(f\"\\nSummary: Caught {len(errors_encountered)} expected errors\")\n",
    "    return errors_encountered\n",
    "\n",
    "# Run error handling tests\n",
    "errors = handle_common_errors()\n",
    "print(\"\\nError handling demonstration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc87bff",
   "metadata": {},
   "source": [
    "What this demonstrates:\n",
    "1. **Device errors**: Wrong device numbers, missing GPUs\n",
    "2. **Memory errors**: Requesting too much memory\n",
    "3. **Compilation errors**: Syntax errors in CUDA C code\n",
    "4. **Launch errors**: Invalid thread/block configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4f555",
   "metadata": {},
   "source": [
    "## 8. Performance Optimization\n",
    "**Understanding GPU Performance**\n",
    "\n",
    "GPUs achieve high performance through **massive parallelism**, but they have specific requirements:\n",
    "\n",
    "**What makes GPUs fast:**\n",
    "* Thousands of threads running simultaneously\n",
    "* High memory bandwidth (when used correctly)\n",
    "* Specialized hardware for parallel operations\n",
    "\n",
    "**What makes GPUs slow:**\n",
    "* Thread divergence (threads taking different paths)\n",
    "* Poor memory access patterns\n",
    "* Insufficient parallelism\n",
    "* Frequent CPU-GPU data transfers\n",
    "\n",
    "\n",
    "### Memory Coalescing\n",
    "**Memory Coalescing** means arranging memory accesses so that neighboring threads access neighboring memory locations. This allows the GPU to fetch data efficiently.\n",
    "\n",
    "Let's look at good vs bad memory access patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Coalesced memory access pattern\n",
    "coalesced_kernel = \"\"\"\n",
    "extern \"C\" __global__ void coalesced_access(float *data, int n) {\n",
    "    // Each thread accesses consecutive memory locations\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        data[i] = data[i] * 2.0f;  // Sequential access - GOOD!\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Bad: Strided memory access pattern  \n",
    "strided_kernel = \"\"\"\n",
    "extern \"C\" __global__ void strided_access(float *data, int n, int stride) {\n",
    "    // Each thread skips 'stride' elements\n",
    "    int i = (blockIdx.x * blockDim.x + threadIdx.x) * stride;\n",
    "    if (i < n) {\n",
    "        data[i] = data[i] * 2.0f;  // Strided access - BAD!\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def compare_memory_patterns():\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    # Compile both kernels\n",
    "    coalesced_program = Program(coalesced_kernel)\n",
    "    coalesced_func = coalesced_program.compile().get_kernel(\"coalesced_access\")\n",
    "    \n",
    "    strided_program = Program(strided_kernel)\n",
    "    strided_func = strided_program.compile().get_kernel(\"strided_access\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    N = 1000000  # 1 million elements\n",
    "    data = np.ones(N, dtype=np.float32)\n",
    "    \n",
    "    # Test coalesced access\n",
    "    print(\"Testing coalesced memory access...\")\n",
    "    d_data = device.allocate(data.nbytes)\n",
    "    d_data.copy_from(data)\n",
    "    \n",
    "    start_event = Event()\n",
    "    end_event = Event()\n",
    "    \n",
    "    start_event.record()\n",
    "    config = LaunchConfig(grid=((N + 255) // 256,), block=(256,))\n",
    "    launch(coalesced_func, config, d_data, np.int32(N))\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    \n",
    "    coalesced_time = Event.elapsed_time(start_event, end_event)\n",
    "    print(f\"Coalesced access time: {coalesced_time:.2f} ms\")\n",
    "    \n",
    "    # Test strided access (accessing every 4th element)\n",
    "    print(\"Testing strided memory access...\")\n",
    "    d_data.copy_from(data)  # Reset data\n",
    "    \n",
    "    start_event.record()\n",
    "    launch(strided_func, config, d_data, np.int32(N), np.int32(4))  # stride=4\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    \n",
    "    strided_time = Event.elapsed_time(start_event, end_event)\n",
    "    print(f\"Strided access time: {strided_time:.2f} ms\")\n",
    "    \n",
    "    print(f\"Speedup from coalescing: {strided_time / coalesced_time:.1f}x\")\n",
    "\n",
    "# Compare memory access patterns\n",
    "compare_memory_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab20e8f",
   "metadata": {},
   "source": [
    "Why coalescing matters:****\n",
    "* Coalesced: Threads 0-31 access data[0-31] → GPU fetches in 1 transaction\n",
    "* Strided: Threads 0-31 access data[0, 4, 8, 12, ...] → GPU needs many transactions\n",
    "\n",
    "Rule of thumb: Arrange your data so that thread N accesses element N (or close to it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab3328",
   "metadata": {},
   "source": [
    "### Shared Memory Usage\n",
    "\n",
    "**Shared memory** is fast, on-chip memory that all threads in a block can access. It's ideal for:\n",
    "* Cache frequently accessed data\n",
    "* Share intermediate results between threads\n",
    "* Reduce global memory accesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_memory_kernel = \"\"\"\n",
    "extern \"C\" __global__ void shared_memory_example(float *input, float *output, int n) {\n",
    "    // Declare shared memory (allocated at kernel launch)\n",
    "    extern __shared__ float shared_data[];\n",
    "    \n",
    "    int tid = threadIdx.x;  // Thread ID within block\n",
    "    int i = blockIdx.x * blockDim.x + tid;  // Global thread ID\n",
    "    \n",
    "    // Step 1: Load data from global memory to shared memory\n",
    "    if (i < n) {\n",
    "        shared_data[tid] = input[i];\n",
    "    } else {\n",
    "        shared_data[tid] = 0.0f;  // Pad with zeros\n",
    "    }\n",
    "    \n",
    "    // Step 2: Wait for all threads in block to finish loading\n",
    "    __syncthreads();  // This is crucial!\n",
    "    \n",
    "    // Step 3: Process data using shared memory (3-point moving average)\n",
    "    if (tid > 0 && tid < blockDim.x - 1 && i < n) {\n",
    "        // Use shared memory instead of global memory - much faster!\n",
    "        output[i] = (shared_data[tid-1] + shared_data[tid] + shared_data[tid+1]) / 3.0f;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_shared_memory():\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    print(\"Demonstrating shared memory usage...\")\n",
    "    \n",
    "    # Prepare test data (some noise to smooth)\n",
    "    N = 10000\n",
    "    data = np.random.random(N).astype(np.float32)\n",
    "    \n",
    "    # Compile kernel\n",
    "    program = Program(shared_memory_kernel)\n",
    "    kernel = program.compile().get_kernel(\"shared_memory_example\")\n",
    "    \n",
    "    # Allocate GPU memory\n",
    "    d_input = device.allocate(data.nbytes)\n",
    "    d_output = device.allocate(data.nbytes)\n",
    "    \n",
    "    d_input.copy_from(data)\n",
    "    \n",
    "    # Configure launch with shared memory\n",
    "    block_size = 256\n",
    "    shared_mem_size = block_size * 4  # 4 bytes per float32\n",
    "    \n",
    "    config = LaunchConfig(\n",
    "        grid=((N + block_size - 1) // block_size,),\n",
    "        block=(block_size,),\n",
    "        shared_memory_size=shared_mem_size  # Allocate shared memory\n",
    "    )\n",
    "    \n",
    "    print(f\"Using {shared_mem_size} bytes of shared memory per block\")\n",
    "    \n",
    "    # Launch kernel\n",
    "    launch(kernel, config, d_input, d_output, np.int32(N))\n",
    "    \n",
    "    # Get results\n",
    "    result = np.zeros_like(data)\n",
    "    d_output.copy_to(result)\n",
    "    \n",
    "    print(\"Shared memory kernel completed successfully\")\n",
    "    print(f\"Input data range: [{data.min():.3f}, {data.max():.3f}]\")\n",
    "    print(f\"Smoothed data range: [{result[1:-1].min():.3f}, {result[1:-1].max():.3f}]\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Demonstrate shared memory\n",
    "smoothed_data = demonstrate_shared_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f3d53",
   "metadata": {},
   "source": [
    "**Key shared memory concepts:**\n",
    "1. Declaration: `extern __shared__ float shared_data[]` creates shared memory\n",
    "2. Allocation: Specify `shared_memory_size` in LaunchConfig\n",
    "3. Synchronization: `__syncthreads()` ensures all threads finish before proceeding\n",
    "4. Access: Much faster than global memory for repeated access\n",
    "\n",
    "**When to use shared memory:**\n",
    "* When multiple threads need the same data\n",
    "* For algorithms that reuse data (convolution, matrix multiplication)\n",
    "* To implement custom caching strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56af27",
   "metadata": {},
   "source": [
    "## 9. Practical Examples {tag here}\n",
    "### Example 1: Image Convolution\n",
    "Image convolution is a common operation in computer vision for tasks like blurring, edge detection, and sharpening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_kernel = \"\"\"\n",
    "extern \"C\" __global__ void convolution_2d(float *input, float *output, float *kernel, \n",
    "                                         int width, int height, int kernel_size) {\n",
    "    // Calculate which pixel this thread processes\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    // Make sure we're within image bounds\n",
    "    if (col < width && row < height) {\n",
    "        float sum = 0.0f;\n",
    "        int half_kernel = kernel_size / 2;\n",
    "        \n",
    "        // Apply convolution kernel\n",
    "        for (int i = -half_kernel; i <= half_kernel; i++) {\n",
    "            for (int j = -half_kernel; j <= half_kernel; j++) {\n",
    "                int input_row = row + i;\n",
    "                int input_col = col + j;\n",
    "                \n",
    "                // Handle image boundaries (clamp to edge)\n",
    "                input_row = max(0, min(input_row, height - 1));\n",
    "                input_col = max(0, min(input_col, width - 1));\n",
    "                \n",
    "                int input_idx = input_row * width + input_col;\n",
    "                int kernel_idx = (i + half_kernel) * kernel_size + (j + half_kernel);\n",
    "                \n",
    "                sum += input[input_idx] * kernel[kernel_idx];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[row * width + col] = sum;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def gpu_convolution(image, conv_kernel):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution on an image using GPU\n",
    "    \n",
    "    Args:\n",
    "        image: 2D numpy array (height, width) \n",
    "        conv_kernel: 2D convolution kernel (must be square, odd size)\n",
    "    \n",
    "    Returns:\n",
    "        Convolved image as 2D numpy array\n",
    "    \"\"\"\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    \n",
    "    # Validate inputs\n",
    "    height, width = image.shape\n",
    "    kernel_size = conv_kernel.shape[0]\n",
    "    assert conv_kernel.shape == (kernel_size, kernel_size), \"Kernel must be square\"\n",
    "    assert kernel_size % 2 == 1, \"Kernel size must be odd\"\n",
    "    \n",
    "    print(f\"Convolving {width}x{height} image with {kernel_size}x{kernel_size} kernel\")\n",
    "    \n",
    "    # Flatten arrays for GPU processing\n",
    "    image_flat = image.flatten().astype(np.float32)\n",
    "    kernel_flat = conv_kernel.flatten().astype(np.float32)\n",
    "    \n",
    "    # Compile kernel\n",
    "    program = Program(convolution_kernel)\n",
    "    kernel_func = program.compile().get_kernel(\"convolution_2d\")\n",
    "    \n",
    "    # Allocate GPU memory\n",
    "    d_input = device.allocate(image_flat.nbytes)\n",
    "    d_output = device.allocate(image_flat.nbytes)\n",
    "    d_kernel = device.allocate(kernel_flat.nbytes)\n",
    "    \n",
    "    # Copy data to GPU\n",
    "    d_input.copy_from(image_flat)\n",
    "    d_kernel.copy_from(kernel_flat)\n",
    "    \n",
    "    # Configure 2D launch (one thread per output pixel)\n",
    "    block_size = 16  # 16x16 = 256 threads per block\n",
    "    grid_x = (width + block_size - 1) // block_size\n",
    "    grid_y = (height + block_size - 1) // block_size\n",
    "    \n",
    "    print(f\"Launch configuration: {grid_x}x{grid_y} blocks of {block_size}x{block_size} threads\")\n",
    "    \n",
    "    # Time the convolution\n",
    "    start_event = Event()\n",
    "    end_event = Event()\n",
    "    \n",
    "    start_event.record()\n",
    "    config = LaunchConfig(grid=(grid_x, grid_y), block=(block_size, block_size))\n",
    "    launch(kernel_func, config, d_input, d_output, d_kernel, \n",
    "           np.int32(width), np.int32(height), np.int32(kernel_size))\n",
    "    end_event.record()\n",
    "    end_event.synchronize()\n",
    "    \n",
    "    time_ms = Event.elapsed_time(start_event, end_event)\n",
    "    \n",
    "    # Copy result back and reshape\n",
    "    output_flat = np.zeros_like(image_flat)\n",
    "    d_output.copy_to(output_flat)\n",
    "    \n",
    "    print(f\"Convolution completed in {time_ms:.2f} ms\")\n",
    "    \n",
    "    return output_flat.reshape(height, width)\n",
    "\n",
    "# Test convolution with different kernels\n",
    "print(\"=== Image Convolution Demo ===\")\n",
    "\n",
    "# Create a test image (simple pattern)\n",
    "test_image = np.zeros((100, 100), dtype=np.float32)\n",
    "test_image[40:60, 40:60] = 1.0  # White square in center\n",
    "\n",
    "# Edge detection kernel (Laplacian)\n",
    "edge_kernel = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Blur kernel (Gaussian approximation)\n",
    "blur_kernel = np.array([\n",
    "    [1, 2, 1],\n",
    "    [2, 4, 2], \n",
    "    [1, 2, 1]\n",
    "], dtype=np.float32) / 16.0  # Normalize\n",
    "\n",
    "# Apply convolutions\n",
    "edges = gpu_convolution(test_image, edge_kernel)\n",
    "blurred = gpu_convolution(test_image, blur_kernel)\n",
    "\n",
    "print(f\"Original image range: [{test_image.min():.1f}, {test_image.max():.1f}]\")\n",
    "print(f\"Edge detection range: [{edges.min():.1f}, {edges.max():.1f}]\")\n",
    "print(f\"Blurred image range: [{blurred.min():.1f}, {blurred.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bcb9d",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "1. 2D thread indexing: Each thread processes one pixel\n",
    "2. Boundary handling: Clamping to avoid out-of-bounds access\n",
    "3. Real-world application: Actual image processing algorithm\n",
    "4. Performance timing: Measuring GPU execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470a44f",
   "metadata": {},
   "source": [
    "## 10. Lab\n",
    "### Exercise 1: Vector Operations\n",
    "Write a CUDA kernel that performs element-wise multiplication of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "multiply_kernel_source = \"\"\"\n",
    "// TODO: Implement vector multiplication kernel\n",
    "\"\"\"\n",
    "\n",
    "def vector_multiply(a, b):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e2001",
   "metadata": {},
   "source": [
    "### Exercise 2: Reduction Operation\n",
    "Implement a parallel reduction to find the maximum value in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8444f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "max_reduction_source = \"\"\"\n",
    "// TODO: Implement reduction kernel\n",
    "\"\"\"\n",
    "\n",
    "def find_max_gpu(arr):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb5891",
   "metadata": {},
   "source": [
    "### Exercise 3: Matrix Transpose\n",
    "Write a kernel that transposes a matrix efficiently using shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "transpose_kernel_source = \"\"\"\n",
    "// TODO: Implement matrix transpose kernel with shared memory\n",
    "\"\"\"\n",
    "\n",
    "def matrix_transpose_gpu(matrix):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d85b09",
   "metadata": {},
   "source": [
    "### Exercise 4: Performance Comparison\n",
    "Compare the performance of your GPU implementations with their CPU counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operations():\n",
    "    # TODO: Implement benchmarking code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f38be",
   "metadata": {},
   "source": [
    "### Best Practices Summary\n",
    "\n",
    "1. **Always initialize CUDA properly** with cuInit(0)\n",
    "2. **Manage memory carefully** - allocate, copy, free in proper order\n",
    "3. **Handle errors gracefully** - wrap CUDA calls in try-catch blocks\n",
    "4. **Use appropriate block sizes** - typically 128, 256, or 512 threads\n",
    "5. **Consider memory access patterns** - coalesced access is faster\n",
    "6. **Use shared memory** for data reuse within blocks\n",
    "7. **Profile your code** - use events for timing\n",
    "8. **Clean up resources** - always free memory and destroy contexts\n",
    "\n",
    "This tutorial provides a solid foundation for using cuda.core effectively. Remember that low-level CUDA programming requires careful attention to detail, but it offers maximum performance and flexibility for GPU computing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a743d1a",
   "metadata": {},
   "source": [
    "## Resources\n",
    "CUDA Python Reference: https://numba.pydata.org/numba-doc/dev/cuda-reference/\n",
    "\n",
    "Repository: https://github.com/NVIDIA/cuda-python "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
