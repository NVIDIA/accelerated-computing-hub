{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e93afc39",
      "metadata": {
        "id": "e93afc39"
      },
      "source": [
        "# 9.2. `nvmath-python`: Kernel fusion\n",
        "\n",
        "Some computational problems have lower compute to memory access instructions ratio, which can often be addressed by lowering the number of memory accesses via kernel fusion. This exercise illustrate how `nvmath-python` inherently fuses simpler operations into a single composite kernel.\n",
        "\n",
        "We illustrate this on the example of neural network propagation and the fast Fourier transfer example.\n",
        "\n",
        "## Advanced `matmul` with bias and epilog\n",
        "Based on **Exercise 9.1**, it is not clear why one would need to use `nvmath-python` for matrix multiplications. Indeed, for basic `matmul` operation using `nvmath-python` alongside CuPy does seem an overkill. However, in scientific computing applications and AI, `matmul`s are often used in combination with other operations. For example, in neural networks quite a common usage pattern is as follows.\n",
        "\n",
        "**Matrix-Matrix Multiplication with Bias and ReLU:**\n",
        "\n",
        "$$C = \\text{ReLU}(A \\cdot B + b^T)$$\n",
        "\n",
        "where:\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ is the input matrix\n",
        "- $B \\in \\mathbb{R}^{k \\times n}$ is the weight matrix  \n",
        "- $b \\in \\mathbb{R}^{m}$ is the bias vector (transposed and broadcasted to $m \\times n$)\n",
        "- $\\text{ReLU}(x) = \\max(0, x)$ is the Rectified Linear Unit activation function\n",
        "- $C \\in \\mathbb{R}^{m \\times n}$ is the output matrix\n",
        "\n",
        "The bias vector $b$ is transposed to $b^T \\in \\mathbb{R}^{m \\times 1}$ and automatically broadcasted across all columns of the result matrix. The ReLU function is applied element-wise to the result of the matrix multiplication plus bias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317b5ac3",
      "metadata": {
        "id": "317b5ac3"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# Define the ReLU function\n",
        "def relu(x):\n",
        "    return cp.maximum(0, x)\n",
        "\n",
        "# Matrix dimensions\n",
        "# m, k, n = 2, 5, 4\n",
        "m, k, n = 2000, 1000, 4000\n",
        "\n",
        "# Create input matrix A (m x k)\n",
        "A = cp.random.randn(m, k)\n",
        "\n",
        "# Create weight matrix B (k x n)\n",
        "B = cp.random.randn(k, n)\n",
        "\n",
        "# Create bias vector b (m,) that will be transposed and broadcasted to (m x n)\n",
        "b = cp.random.randn(m)\n",
        "\n",
        "# Implement the formula: C = ReLU(A * B + b^T)\n",
        "# Kernel 1: Matrix multiplication\n",
        "matmul_result = cp.matmul(A, B)\n",
        "\n",
        "# Kernel 2: Add bias (broadcasting happens automatically)\n",
        "bias_result = matmul_result + b.reshape(-1, 1)\n",
        "\n",
        "# Kernel 3: Apply ReLU activation\n",
        "C = relu(bias_result)\n",
        "\n",
        "# print(f\"A: {A}\")\n",
        "# print(f\"B: {B}\")\n",
        "# print(f\"b: {b}\")\n",
        "# print(f\"C: {C}\")\n",
        "\n",
        "\n",
        "print(f\"Input matrix A shape: {A.shape}\")\n",
        "print(f\"Weight matrix B shape: {B.shape}\")\n",
        "print(f\"Bias vector b shape: {b.shape}\")\n",
        "print(f\"Transposed bias b^T shape: {b.reshape(-1, 1).shape}\")\n",
        "print(f\"Output matrix C shape: {C.shape}\")\n",
        "print(f\"Output matrix C device: {C.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c03a45",
      "metadata": {
        "id": "39c03a45"
      },
      "source": [
        "**TODO: Validate the above implementation on small easy to comprehend inputs by manually initializing matrices and bias and by printing results step by step**\n",
        "\n",
        "`nvmath-python` leverages the power of `cuBLASLt` library that provides variety of options to implement such computational patterns. Here's an example of how one can implement the above example using `nvmath-python`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af245b0d",
      "metadata": {
        "id": "af245b0d"
      },
      "outputs": [],
      "source": [
        "import nvmath\n",
        "\n",
        "# Kernel 1, 2, and 3 are fused into a single kernel\n",
        "C = nvmath.linalg.advanced.matmul(A, B, epilog=nvmath.linalg.advanced.MatmulEpilog.RELU_BIAS, epilog_inputs={\"bias\": b})\n",
        "\n",
        "# print(f\"A: {A}\")\n",
        "# print(f\"B: {B}\")\n",
        "# print(f\"b: {b}\")\n",
        "# print(f\"C: {C}\")\n",
        "\n",
        "print(C.shape)\n",
        "print(C.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746a4d0e",
      "metadata": {
        "id": "746a4d0e"
      },
      "source": [
        "**TODO: Ensure that `nvmath-python` results are identical for the same small inputs**\n",
        "\n",
        "All three kernels are fused into a single kernel using JIT machinery behind `cuBLASLt`, which in certain problem settings may result in overall better performance due to better compute-to-memory access ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114319f3",
      "metadata": {
        "id": "114319f3"
      },
      "source": [
        "## Using custom FFT callbacks written in Python\n",
        "\n",
        "In previous example the epilog is a predefined set of activation functions and their gradients. This example illustrates the case when the epilog is a custom Python function, which is compiled into internal intermediate representation (LTO-IR) and then fused with the FFT operation into a single kernel.\n",
        "\n",
        "Specifically, we illustrate how to perform a convolution by providing a Python callback function as an epilog to the FFT operation.\n",
        "\n",
        "To begin with, let's create some input data. We will use the batched 1D FFT and apply a sine-form filter in the frequency domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbea6060",
      "metadata": {
        "id": "cbea6060"
      },
      "outputs": [],
      "source": [
        "# Create the data for the batched 1-D FFT.\n",
        "B, N = 256, 1024\n",
        "a = cp.random.rand(B, N) + 1j * cp.random.rand(B, N)\n",
        "\n",
        "# Create the data to use as a filter.\n",
        "filter_data = cp.sin(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4211da9",
      "metadata": {
        "id": "e4211da9"
      },
      "source": [
        "We also define the epilog function for forward FFT, a convolution, which corresponds to pointwise multiplication in the frequency domain. We also scale by the FFT size `N` here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e27f3c",
      "metadata": {
        "id": "e1e27f3c"
      },
      "outputs": [],
      "source": [
        "def convolve(data_out, offset, data, filter_data, unused):\n",
        "    data_out[offset] = data * filter_data[offset] / N"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39d8361",
      "metadata": {
        "id": "c39d8361"
      },
      "source": [
        "Note we are accessing `data_out` and `filter_data` with a single `offset` integer, even though the output and `filter_data` are 2D tensors (batches of samples). Care must be taken to ensure that both arrays accessed here have the same memory layout.\n",
        "\n",
        "Next thing is to compile the epilog to intermediate representation (LTO-IR). In a system with GPUs that have different compute capability, the `compute_capability` option must be specified to the `compile_prolog` or `compile_epilog` helpers. Alternatively, the epilog can be compiled in the context of the device where the FFT to which the epilog is provided is executed. In this case we use the current device context, where the operands have been created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eda86b7",
      "metadata": {
        "id": "1eda86b7"
      },
      "outputs": [],
      "source": [
        "with cp.cuda.Device():\n",
        "    epilog = nvmath.fft.compile_epilog(convolve, \"complex128\", \"complex128\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8418ea",
      "metadata": {
        "id": "4e8418ea"
      },
      "source": [
        "Finally, we perform the convolution as the forward FFT with the compiled epilog (filter) followed by the inverse FFT transformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0e951b",
      "metadata": {
        "id": "3c0e951b"
      },
      "outputs": [],
      "source": [
        "r = nvmath.fft.fft(a, axes=[-1], epilog={\"ltoir\": epilog, \"data\": filter_data.data.ptr})\n",
        "r = nvmath.fft.ifft(r, axes=[-1])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
     "display_name": "training-pyhpc-2025",
     "language": "python",
     "name": "pyhpc-2025"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
