{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9a5bf9",
   "metadata": {},
   "source": [
    "# Exercise - `cuda.core` - Devices, Streams, and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a456f7",
   "metadata": {},
   "source": [
    "`cuda.core` provides a Pythonic interface to the CUDA runtime and other functionality, including:\n",
    "\n",
    "* Compiling and launching CUDA kernels\n",
    "* Asynchronous concurrent execution with CUDA graphs, streams and events\n",
    "* Coordinating work across multiple CUDA devices\n",
    "* Allocating, transferring, and managing device memory\n",
    "* Runtime linking of device code with Link-Time Optimization (LTO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e7496",
   "metadata": {},
   "source": [
    "### Example: Getting Device Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000c2cf-ac17-48f2-96a6-d807c4740536",
   "metadata": {},
   "source": [
    "As a quick example of the `cuda.core` interface, let's look at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19aac3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Compute capability: 8.0\n",
      "Multiprocessor count: 108\n",
      "Max threads per block: 1024\n",
      "Max block dimensions: (1024, 1024, 64)\n"
     ]
    }
   ],
   "source": [
    "from cuda.core.experimental import Device\n",
    "\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Get device properties\n",
    "props = device.properties\n",
    "print(f\"Device name: {device.name}\")\n",
    "print(f\"Compute capability: {props.compute_capability_major}.{props.compute_capability_minor}\")\n",
    "print(f\"Multiprocessor count: {props.multiprocessor_count}\")\n",
    "print(f\"Max threads per block: {props.max_threads_per_block}\")\n",
    "print(f\"Max block dimensions: ({props.max_block_dim_x}, {props.max_block_dim_y}, {props.max_block_dim_z})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340cd1b6",
   "metadata": {},
   "source": [
    "### Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c359b-9511-4799-b32e-17198d0e9f48",
   "metadata": {},
   "source": [
    "The `Device.allocate()` method can be used to allocate memory on the specific CUDA device: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated 4000 bytes on GPU\n",
      "Buffer memory address: <CUdeviceptr 13421785088>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cuda.core.experimental import Device\n",
    "\n",
    "# Initialize our GPU\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Calculate how much memory we need\n",
    "# We want to store 1000 float32 numbers\n",
    "# Each float32 takes 4 bytes, so we need 1000 * 4 = 4000 bytes\n",
    "size_bytes = 1000 * 4\n",
    "\n",
    "# Allocate memory on the GPU\n",
    "device_buffer = device.allocate(size_bytes)\n",
    "\n",
    "print(f\"Allocated {size_bytes} bytes on GPU\")\n",
    "print(f\"Buffer memory address: {device_buffer.handle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86c5a",
   "metadata": {},
   "source": [
    "### Compiling and calling a CUDA C++ kernel from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a20874cd-27eb-418c-9cc1-7838156af510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cuda.core.experimental import Device, LaunchConfig, Program, ProgramOptions, launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029148c-f490-45ed-9bab-f2c8faa0e90c",
   "metadata": {},
   "source": [
    "First, we define a string containing the CUDA C++ kernel. Note that this is a templated kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47fb7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute c = a + b\n",
    "code = \"\"\"\n",
    "template<typename T>\n",
    "__global__ void vector_add(const T* A,\n",
    "                           const T* B,\n",
    "                           T* C,\n",
    "                           size_t N) {\n",
    "    const unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    for (size_t i=tid; i<N; i+=gridDim.x*blockDim.x) {\n",
    "        C[tid] = A[tid] + B[tid];\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020ad6d-3ecb-44f0-8058-9d0bce5800b1",
   "metadata": {},
   "source": [
    "Next, we create a `Device` object and a corresponding `Stream`. Don’t forget to use `Device.set_current()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2c4d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = Device()\n",
    "dev.set_current()\n",
    "s = dev.create_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640278c4-3dd9-4110-a109-3b90c271d41a",
   "metadata": {},
   "source": [
    "Next, we compile the CUDA C++ kernel from earlier using the `Program` class. The result of the compilation is saved as a CUBIN. Note the use of the `name_expressions` parameter to the `Program.compile()` method to specify which kernel template instantiations to compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "715c208f-32cb-4ed8-810c-6ca1526ff7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = str(f\"{dev.compute_capability.major}{dev.compute_capability.minor}\")\n",
    "program_options = ProgramOptions(std=\"c++17\", arch=f\"sm_{arch}\")\n",
    "prog = Program(code, code_type=\"c++\", options=program_options)\n",
    "mod = prog.compile(\"cubin\", name_expressions=(\"vector_add<float>\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2be42-5241-4516-ad19-167e2936ee39",
   "metadata": {},
   "source": [
    "Next, we retrieve the compiled kernel from the CUBIN and prepare the arguments and kernel configuration. We’re using CuPy arrays as inputs for this example, but you can use PyTorch tensors too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "271fa9e3-a62f-46c4-8f3d-6129ec686cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = mod.get_kernel(\"vector_add<float>\")\n",
    "\n",
    "# Prepare input/output arrays (using CuPy)\n",
    "size = 50000\n",
    "rng = cp.random.default_rng()\n",
    "a = rng.random(size, dtype=cp.float32)\n",
    "b = rng.random(size, dtype=cp.float32)\n",
    "c = cp.empty_like(a)\n",
    "\n",
    "# Configure launch parameters\n",
    "block = 256\n",
    "grid = (size + block - 1) // block\n",
    "config = LaunchConfig(grid=grid, block=block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033b929-c1df-4948-9dc8-f02853a6b60a",
   "metadata": {},
   "source": [
    "Finally, we use the launch() function to execute our kernel on the specified stream with the given configuration and arguments. Note the use of `.data.ptr` to get the pointer to the CuPy array's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84a2450c-65bb-472d-b444-a5ff17352761",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch(s, config, ker, a.data.ptr, b.data.ptr, c.data.ptr, cp.uint64(size))\n",
    "s.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2856509a-4bc2-4657-ac97-8c09b33f6694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.423765   0.9974886  0.8906149  ... 0.98856455 0.75328386 0.12381048]\n",
      "[0.6481802  0.05253503 0.05977066 ... 0.58469105 0.49861202 0.18568273]\n",
      "[1.0719452 1.0500237 0.9503856 ... 1.5732555 1.2518959 0.3094932]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
