{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Fix Data Race\n",
        "\n",
        "You can use `__syncthreads()` to synchronize threads within a block:\n",
        "\n",
        "<img src=Images/sync.png alt=\"Sync\" width=800>\n",
        "\n",
        "Fix the data race using thread-block synchronization.\n",
        "Optionally, switch to `cuda::atomic_ref` to reduce the scope of communication:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Original code if you need to refer back to it.</summary>\n",
        "\n",
        "```c++\n",
        "%%writefile Sources/sync.cpp\n",
        "#include \"ach.cuh\"\n",
        "\n",
        "constexpr float bin_width = 10;\n",
        "\n",
        "// 1. Use `__syncthreads()` to synchronize threads within a block and avoid data race\n",
        "__global__ void histogram_kernel(\n",
        "  cuda::std::span<float> temperatures, \n",
        "  cuda::std::span<int> block_histograms, \n",
        "  cuda::std::span<int> histogram) \n",
        "{\n",
        "  cuda::std::span<int> block_histogram = \n",
        "    block_histograms.subspan(blockIdx.x * histogram.size(), \n",
        "                             histogram.size());\n",
        "\n",
        "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
        "\n",
        "  cuda::std::atomic_ref<int> block_ref(block_histogram[bin]);\n",
        "  block_ref.fetch_add(1);\n",
        "\n",
        "  if (threadIdx.x < histogram.size()) {\n",
        "    // 2. Reduce scope of atomic operation using `cuda::atomic_ref`\n",
        "    cuda::std::atomic_ref<int> ref(histogram[threadIdx.x]);\n",
        "    ref.fetch_add(block_histogram[threadIdx.x]);\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "void histogram(\n",
        "  cuda::std::span<float> temperatures, \n",
        "  cuda::std::span<int> block_histograms, \n",
        "  cuda::std::span<int> histogram,\n",
        "  cudaStream_t stream) \n",
        "{\n",
        "  int block_size = 256;\n",
        "  int grid_size = cuda::ceil_div(temperatures.size(), block_size);\n",
        "  histogram_kernel<<<grid_size, block_size, 0, stream>>>(\n",
        "    temperatures, block_histograms, histogram);\n",
        "}\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Google Colab Setup\n",
        "!mkdir -p Sources\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.04-Synchronization/Sources/ach.cuh -nv -O Sources/ach.cuh\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.04-Synchronization/Sources/__init__.py -nv -O Sources/__init__.py\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.04-Synchronization/Sources/ach.py -nv -O Sources/ach.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Sources/sync.cpp\n",
        "#include \"ach.cuh\"\n",
        "\n",
        "constexpr float bin_width = 10;\n",
        "\n",
        "// 1. Use `__syncthreads()` to synchronize threads within a block and avoid data race\n",
        "__global__ void histogram_kernel(\n",
        "  cuda::std::span<float> temperatures, \n",
        "  cuda::std::span<int> block_histograms, \n",
        "  cuda::std::span<int> histogram) \n",
        "{\n",
        "  cuda::std::span<int> block_histogram = \n",
        "    block_histograms.subspan(blockIdx.x * histogram.size(), \n",
        "                             histogram.size());\n",
        "\n",
        "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
        "\n",
        "  cuda::std::atomic_ref<int> block_ref(block_histogram[bin]);\n",
        "  block_ref.fetch_add(1);\n",
        "\n",
        "  if (threadIdx.x < histogram.size()) {\n",
        "    // 2. Reduce scope of atomic operation using `cuda::atomic_ref`\n",
        "    cuda::std::atomic_ref<int> ref(histogram[threadIdx.x]);\n",
        "    ref.fetch_add(block_histogram[threadIdx.x]);\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "void histogram(\n",
        "  cuda::std::span<float> temperatures, \n",
        "  cuda::std::span<int> block_histograms, \n",
        "  cuda::std::span<int> histogram,\n",
        "  cudaStream_t stream) \n",
        "{\n",
        "  int block_size = 256;\n",
        "  int grid_size = cuda::ceil_div(temperatures.size(), block_size);\n",
        "  histogram_kernel<<<grid_size, block_size, 0, stream>>>(\n",
        "    temperatures, block_histograms, histogram);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Sources.ach\n",
        "Sources.ach.run(\"Sources/sync.cpp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you’re unsure how to proceed, consider expanding this section for guidance. Use the hint only after giving the problem a genuine attempt.\n",
        "\n",
        "<details>\n",
        "  <summary>Hints</summary>\n",
        "  \n",
        "  - You can synchronize threads within a thread block using `__syncthreads()`\n",
        "  - You need to synchronize after all threads have incorporated their changes to the block histogram\n",
        "  - `cuda::atomic_ref` has exactly the same interface as `cuda::std::atomic_ref` with a difference of accepting thread scope as a second template parameter\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open this section only after you’ve made a serious attempt at solving the problem. Once you’ve completed your solution, compare it with the reference provided here to evaluate your approach and identify any potential improvements.\n",
        "\n",
        "<details>\n",
        "  <summary>Solution</summary>\n",
        "\n",
        "  Key points:\n",
        "\n",
        "  - Synchronize before reading the block histogram\n",
        "\n",
        "  Solution:\n",
        "  ```c++\n",
        "  cuda::std::span<int> block_histogram =\n",
        "    block_histograms.subspan(blockIdx.x * histogram.size(), histogram.size());\n",
        "\n",
        "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
        "\n",
        "  cuda::atomic_ref<int, cuda::thread_scope_block> \n",
        "    block_ref(block_histogram[bin]);\n",
        "  block_ref.fetch_add(1);\n",
        "  __syncthreads();\n",
        "\n",
        "  if (threadIdx.x < histogram.size()) \n",
        "  {\n",
        "    cuda::atomic_ref<int, cuda::thread_scope_device> \n",
        "      ref(histogram[threadIdx.x]);\n",
        "    ref.fetch_add(block_histogram[threadIdx.x]);\n",
        "  }\n",
        "  ```\n",
        "\n",
        "  You can find full solution [here](Solutions/sync.cu).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Great job!  Move on to the [next section](../03.05-Shared-Memory/03.05.01-Shared.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },

    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}