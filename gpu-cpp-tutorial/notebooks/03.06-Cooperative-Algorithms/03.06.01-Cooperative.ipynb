{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [Device-Side Libraries](#Device-Side-Libraries)\n",
    "* [CUB Cooperative Algorithms](#CUB-Cooperative-Algorithms)\n",
    "* [Exercise: Coarsening](03.06.02-Exercise-Cooperative-Histogram.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Our kernel performance is now at acceptable levels. \n",
    "However, we’ve made a significant oversight in our strategy. \n",
    "While writing host-side code, we used accelerated libraries. \n",
    "However, once we started writing CUDA kernels, we ended up implementing everything from scratch. \n",
    "This isn’t the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device-Side Libraries\n",
    "\n",
    "CUDA offers many device-side libraries that can help you write more efficient kernels more quickly. \n",
    "For example:\n",
    "\n",
    "- cuBLASDx: provides cooperative linear algebra functions inside CUDA kernels\n",
    "- cuFFTDx: provides fast cooperative Fourier Transform inside CUDA kernels\n",
    "- CUB: provides cooperative general-purpose algorithms inside CUDA kernels\n",
    "\n",
    "... and there are more. \n",
    "But they all share a common trait. \n",
    "They're *cooperative*. \n",
    "What does that actually mean?\n",
    "\n",
    "To answer this question, let's revisit the algorithm types we’ve seen so far:\n",
    "1. Sequential algorithms (e.g., `std::transform`):\n",
    "   - Invoked and executed by a single thread.\n",
    "   - Even if multiple threads call the same algorithm, there's no way the input of one thread can affect the output of another.\n",
    "2. Cooperative algorithms:\n",
    "   - Invoked and executed by multiple threads working together to achieve a common goal.\n",
    "   - It’s as if they accept a single input that is divided among multiple threads.\n",
    "3. Parallel algorithms (e.g., those in CUB and Thrust):\n",
    "   - Invoked by a single thread, but internally executed by many threads.\n",
    "   - From the user’s perspective, they often look similar to sequential algorithms.\n",
    "   - Under the hood, they often rely on cooperative algorithms to perform the actual parallel work.\n",
    "\n",
    "<img src=\"Images/parallel-vs-cooperative.png\" alt=\"Cooperative vs Parallel\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUB Cooperative Algorithms\n",
    "\n",
    "CUB provides both parallel and cooperative algorithms. \n",
    "To learn the basics of cooperative libraries, let’s look at a block-level reduction example. \n",
    "The CUB block-level interface can be summarized as follows:\n",
    "\n",
    "```c++\n",
    "template <typename T, int BlockDimX>\n",
    "struct cub::BlockReduce\n",
    "{\n",
    "  struct TempStorage { ... };\n",
    "\n",
    "  __device__ BlockReduce(TempStorage& temp_storage) { ... }\n",
    "\n",
    "  __device__ T Sum(T input) { return ...; }\n",
    "}\n",
    "```\n",
    "\n",
    "Unlike traditional function-oriented interfaces, CUB exposes its cooperative algorithms as templated structs.\n",
    "Template parameters are used to specialize algorithms for the problem at hand:\n",
    "- data type (`int`, `float`, etc.)\n",
    "- number of threads in the thread block\n",
    "- grain size (number of items per thread)\n",
    "- and so on\n",
    "\n",
    "The nested `TempStorage` type provides a type of temporary storage needed by cooperative algorithms for thread communication.\n",
    "An instance of this type has to be allocated in shared memory.\n",
    "`TempStorage` allocation must be provided to construct an instance of the algorithm.\n",
    "\n",
    "Finally, member functions represent different flavors of a given cooperative algorithms.\n",
    "The simple usage of block-level reduction is as follows:\n",
    "\n",
    "```c++\n",
    "__shared__ cub::BlockReduce<int, 4>::TempStorage storage;\n",
    "int block_sum = cub::BlockReduce<int, 4>(storage).Sum(threadIdx.x);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the following diagram is only a conceptual model, it illustrate what’s happening under the hood in a cooperative algorithm such as block-level reduction:\n",
    "\n",
    "<img src=\"Images/coop-reduce.png\" alt=\"Cooperative Reduction\" width=600>\n",
    "\n",
    "The code starts by copying algorithm input that likely comes from registers into shared memory.\n",
    "Then, the cooperative algorithm has to synchronize the thread block to make sure all stores were completed.\n",
    "This means that if any of the threads don't call the algorithm, the entire thread block will deadlock.\n",
    "Besides that, the algorithm uses shared memory to communicate intermediate results between threads.\n",
    "Finally, the algorithm returns the result.\n",
    "Reduction is a bit specific in this regard, because it returns a valid result only to the first thread in the thread block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.06-Cooperative-Algorithms/Sources/ach.cuh -nv -O Sources/ach.cuh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/block-sum.cpp\n",
    "#include <cub/block/block_reduce.cuh>\n",
    "\n",
    "constexpr int block_threads = 128;\n",
    "\n",
    "__global__ void block_sum()\n",
    "{\n",
    "  using block_reduce_t = cub::BlockReduce<int, block_threads>;\n",
    "  using storage_t = block_reduce_t::TempStorage;\n",
    "\n",
    "  __shared__ storage_t storage;\n",
    "\n",
    "  int block_sum = block_reduce_t(storage).Sum(threadIdx.x);\n",
    "\n",
    "  if (threadIdx.x == 0)\n",
    "  {\n",
    "    printf(\"block sum = %d\\n\", block_sum);\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  block_sum<<<1, block_threads>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --extended-lambda -o /tmp/a.out Sources/block-sum.cpp -x cu -arch=native # build executable\n",
    "!/tmp/a.out # run executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Go to the [next exercise](03.06.02-Exercise-Cooperative-Histogram.ipynb) to replace our block histogram implementation with the block-level histogram from CUB."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
