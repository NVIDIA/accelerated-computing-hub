{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streams\n",
    "\n",
    "## Content\n",
    "\n",
    "* [Copying Memory Asynchronously](#Copying-Memory-Asynchronously)\n",
    "* [CUDA Stream](#CUDA-Stream)\n",
    "* [Exercise: Async Copy and Streams](02.03.02-Exercise-Async-Copy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "So far, you’ve learned how to use asynchronous APIs to overlap computation (on the GPU) and I/O (on the CPU). \n",
    "Here’s what our simulator code looks like when we overlap compute and I/O:\n",
    "\n",
    "```cpp\n",
    "void simulate(int width, int height, const thrust::device_vector<float> &in,\n",
    "              thrust::device_vector<float> &out)\n",
    "{\n",
    "  cuda::std::mdspan temp_in(thrust::raw_pointer_cast(in.data()), height, width);\n",
    "  cub::DeviceTransform::Transform(\n",
    "    thrust::make_counting_iterator(0), out.begin(), width * height,\n",
    "    [=] __host__ __device__(int id) { return ach::compute(id, temp_in); });\n",
    "}\n",
    "\n",
    "int main() \n",
    "{\n",
    "  int height = 2048;\n",
    "  int width = 8192;\n",
    "\n",
    "  thrust::device_vector<float> d_prev = ach::init(height, width);\n",
    "  thrust::device_vector<float> d_next(height * width);\n",
    "  thrust::host_vector<float> h_prev(height * width);\n",
    "\n",
    "  for (int write_step = 0; write_step < 3; write_step++) \n",
    "  {\n",
    "    thrust::copy(d_prev.begin(), d_prev.end(), h_prev.begin());\n",
    "\n",
    "    for (int compute_step = 0; compute_step < 750; compute_step++) \n",
    "    {\n",
    "      simulate(width, height, d_prev, d_next);\n",
    "      d_prev.swap(d_next);\n",
    "    }\n",
    "\n",
    "    ach::store(write_step, height, width, h_prev);\n",
    "\n",
    "    cudaDeviceSynchronize(); \n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This code is already fast, but there are still further optimizations we can make. <br>\n",
    "Right now, the simulator:\n",
    "1. Synchronously copies data from GPU to CPU memory.\n",
    "2. Overlaps computation and I/O to some extent.\n",
    "3. Waits for the copy to finish before proceeding with the computation.\n",
    "\n",
    "To improve performance even more, we can also overlap the data copy with the GPU computation, just as we did with the disk I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Memory Asynchronously\n",
    "\n",
    "To achieve this, we need an asynchronous version of `thrust::copy`. \n",
    "Because Thrust itself doesn’t have direct “magical” powers to copy between the CPU and GPU, it relies on the CUDA Runtime API.\n",
    "The CUDA Runtime API provides asynchronous memory copy functions such as `cudaMemcpyAsync`, which has the following interface:\n",
    "\n",
    "```c++\n",
    "cudaError_t cudaMemcpyAsync(\n",
    "  void*           dst,  // destination pointer\n",
    "  const void*     src,  // source pointer\n",
    "  size_t        count,  // number of bytes to copy\n",
    "  cudaMemcpyKind kind   // direction of copy\n",
    ");\n",
    "```\n",
    "\n",
    "Unlike Thrust, `cudaMemcpyAsync` works on raw pointers and operates in terms of bytes rather than elements.\n",
    "This means that we need to calculate the size of the data we want to copy in bytes.\n",
    "Besides that, `cudaMemcpyAsync` also requires an explicit copy direction, which can be one of the following:\n",
    "\n",
    "- `cudaMemcpyHostToDevice`: instructs `cudaMemcpyAsync` to copy data from CPU to GPU\n",
    "- `cudaMemcpyDeviceToHost`: instructs `cudaMemcpyAsync` to copy data from GPU to CPU\n",
    "- `cudaMemcpyDeviceToDevice`: instructs `cudaMemcpyAsync` to copy data from GPU to GPU\n",
    "\n",
    "You might have also noticed that `cudaMemcpyAsync` returns a `cudaError_t`.\n",
    "What kind of error can it be?\n",
    "Well, it can actually be any error from previous asynchronous operations.\n",
    "\n",
    "![Async Errors](Images/async-errors.png \"Async Errors\")\n",
    "\n",
    "In the diagram above, we have two asynchronous operations: `A` and `B` followed by a `cudaMemcpyAsync`.\n",
    "Since both `A` and `B` are computed asynchronously, `A` can start executing after `B` was launched.\n",
    "This means that if `A` fails, the error can be caught by `cudaMemcpyAsync`.\n",
    "\n",
    "Unfortunately, if we just use `cudaMemcpyAsync` in our code, we won't get any performance improvement.\n",
    "To figure out why, let's take a look at the following diagram:\n",
    "\n",
    "![Same Stream](Images/async-copy-same-stream.png \"Same Stream\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that all asynchronous operations are ordered on the GPU.\n",
    "Just as when we launched multiple asynchronous CUB calls, we expected the next invocation to start after the previous one finished, but the same thing happened with `cudaMemcpyAsync`.\n",
    "Subsequent CUB computations wait for `cudaMemcpyAsync` to finish, even though the copy operation is asynchronous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Stream\n",
    "\n",
    "This is an appropriate time to introduce a new concept called a _CUDA stream_.  You can think of a CUDA stream as an in-order work queue of things (commands, functions, etc.) that will be executed on the GPU.  In all the code we've been writing, we've been executing our GPU work in a stream - we just didn't know it.  If the programmer doesn't specify a stream (which we haven't up to this point), then the work is issued to something called the _default CUDA stream_.  \n",
    "\n",
    "Very importantly, the work issued to a specific CUDA stream is executed synchronously and in-order with respect to that stream.  This makes sense intuitively as a typical GPU programming flow is to do something like the following:\n",
    "\n",
    "1. Copy data from host to device\n",
    "2. Compute on the device\n",
    "3. Copy data from device to host\n",
    "\n",
    "For example, one would not want the compute in step 2 to begin before all the data from step 1 is copied to the device.  So again, work in the _same_ stream is executed synchronously with respect to that stream.  However, work in _different_ streams is not synchronized. This is how we can achieve proper concurrency among all the parts of the application that can be executed asynchronously.  In our example, specifically, we can use different streams to allow computation and data transfer to be executed concurrently.\n",
    "\n",
    "![Different Streams](Images/async-copy.png \"Different Streams\")\n",
    "\n",
    "On the language level, a CUDA stream is represented by a specific type:\n",
    "\n",
    "```c++\n",
    "cudaStream_t copy_stream, compute_stream;\n",
    "```\n",
    "\n",
    "To construct a stream, we use the following function:\n",
    "\n",
    "```c++\n",
    "cudaStreamCreate(&compute_stream);\n",
    "cudaStreamCreate(&copy_stream);\n",
    "```\n",
    "\n",
    "We can also synchronize the CPU with a given stream, instead of synchronizing with the entire GPU using `cudaDeviceSynchronize`:\n",
    "\n",
    "```c++\n",
    "cudaStreamSynchronize(compute_stream);\n",
    "cudaStreamSynchronize(copy_stream);\n",
    "```\n",
    "\n",
    "This is a recommended way to synchronize CPU with GPU, as it allows for more fine-grained control over the synchronization.\n",
    "\n",
    "Finally, you can destroy a stream using the following function:\n",
    "\n",
    "```c++\n",
    "cudaStreamDestroy(compute_stream);\n",
    "cudaStreamDestroy(copy_stream);\n",
    "```\n",
    "\n",
    "`cudaMemcpyAsync` actually has an additional parameter that allows you to specify a stream in which the copy operation should be executed:\n",
    "\n",
    "```c++\n",
    "cudaError_t \n",
    "cudaMemcpyAsync(\n",
    "  void*           dst, \n",
    "  const void*     src, \n",
    "  size_t        count, \n",
    "  cudaMemcpyKind kind,\n",
    "  cudaStream_t stream = 0 // <- \n",
    ");\n",
    "\n",
    "CUB also allows you to specify which stream to use.\n",
    "\n",
    "```c++\n",
    "cudaError_t \n",
    "cub::DeviceTransform::Transform(\n",
    "  IteratorIn \t  input, \n",
    "  IteratorOut  output,\n",
    "  int       num_items,\n",
    "  TransformOp \t   \top, \n",
    "  cudaStream_t stream = 0 // <-\n",
    ");\n",
    "```\n",
    "\n",
    "It's very common for accelerated libraries to provide an optional stream parameter.\n",
    "The idea is that you as a user of these libraries will likely want to overlap their operations with data transfers, CPU computations, or even other library calls.\n",
    "\n",
    "Returning to our simulator, if we just use `cudaMemcpyAsync` and `cub::DeviceTransform::Transform` with different streams, we'll end up with a data race.\n",
    "If you take a look at each iteration, you'll notice how the second iteration step overwrites `d_prev` while it's being copied to the CPU.\n",
    "\n",
    "```c++\n",
    "cudaMemcpyAsync(\n",
    "  thrust::raw_pointer_cast(h_prev.data()),\n",
    "  thrust::raw_pointer_cast(d_prev.data()), // reads d_prev\n",
    "  height * width * sizeof(float),\n",
    "  cudaMemcpyDeviceToHost,\n",
    "  copy_stream);\n",
    "\n",
    "simulate(width, height, d_prev, d_next, compute_stream); // reads d_prev, writes d_next\n",
    "simulate(width, height, d_next, d_prev, compute_stream); // reads d_next, writes d_prev\n",
    "```\n",
    "\n",
    "We can fix this with another level of indirection.\n",
    "We can allocate a staging buffer on the GPU, copy the data from `d_prev` to the staging buffer synchronously, and then copy the data from the staging buffer to the CPU.\n",
    "\n",
    "```c++\n",
    "thrust::copy(d_prev.begin(), d_prev.end(), d_buffer.begin()); // reads d_prev synchronously\n",
    "\n",
    "cudaMemcpyAsync(\n",
    "  thrust::raw_pointer_cast(h_prev.data()),\n",
    "  thrust::raw_pointer_cast(d_buffer.data()), // reads d_buffer asynchronously\n",
    "  height * width * sizeof(float),\n",
    "  cudaMemcpyDeviceToHost,\n",
    "  copy_stream);\n",
    "\n",
    "simulate(width, height, d_prev, d_next, compute_stream); // reads d_prev, writes d_next\n",
    "simulate(width, height, d_next, d_prev, compute_stream); // reads d_next, writes d_prev\n",
    "```\n",
    "\n",
    "But doesn't this defeat the purpose of overlapping computation and IO?\n",
    "We just made the copy synchronous again!\n",
    "To answer this question, let's return to our high-level overview of bandwidth provided by different subsystems:\n",
    "\n",
    "![PCIe](Images/cpu-vs-gpu-memory-pci.png \"PCIe\")\n",
    "\n",
    "Here you can see how the bandwidth of CPU-GPU interconnect is much lower than the bandwidth of GPU memory.\n",
    "This means that copying data from GPU to GPU should be significantly faster than copying data from GPU to CPU.\n",
    "So this change can still lead to a performance improvement, at the small expense of having a small temporary buffer in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Proceed to the [exercise](02.03.02-Exercise-Async-Copy.ipynb), where you'll have a chance to implement this optimization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
