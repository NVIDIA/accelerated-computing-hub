{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Compute Median Temperature\n",
    "\n",
    "In many cases, porting code from CPU to GPU is as simple as replacing `std::` with `thrust::`.\n",
    "To verify this, we'll focus on calculating the median temperature in the vector. \n",
    "You'll start with a CPU-based implementation and modify the code to run on the GPU. \n",
    "Below is the original CPU code that you'll need to adapt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/01.02-Execution-Spaces/Sources/ach.h -nv -O Sources/ach.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/port-sort-to-gpu.cpp\n",
    "#include \"ach.h\"\n",
    "\n",
    "float median(thrust::universal_vector<float> vec)\n",
    "{\n",
    "    // TODO: Make the code below execute on the GPU\n",
    "    std::sort(vec.begin(), vec.end());\n",
    "    return vec[vec.size() / 2];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float k = 0.5;\n",
    "    float ambient_temp = 20;\n",
    "    thrust::universal_vector<float> temp{ 42, 24, 50 };\n",
    "    auto transformation = [=] __host__ __device__ (float temp) { return temp + k * (ambient_temp - temp); };\n",
    "\n",
    "    std::printf(\"step  median\\n\");\n",
    "    for (int step = 0; step < 3; step++) {\n",
    "        thrust::transform(thrust::device, temp.begin(), temp.end(), temp.begin(), transformation);\n",
    "        float median_temp = median(temp);\n",
    "        std::printf(\"%d     %.2f\\n\", step, median_temp);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o /tmp/a.out --extended-lambda Sources/port-sort-to-gpu.cpp -x cu -arch=native # build executable\n",
    "!/tmp/a.out # run executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes well, the cell above should print:\n",
    "\n",
    "| step | median\n",
    "| :--- | :-----\n",
    "| 0    | 31.00\n",
    "| 1    | 25.50\n",
    "| 2    | 22.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re unsure how to proceed, consider expanding this section for guidance. Use the hint only after giving the problem a genuine attempt.\n",
    "\n",
    "<details>\n",
    "  <summary>Hints</summary>\n",
    "  \n",
    "  - Thrust provides a `thrust::sort` function that can be used to sort data on the GPU\n",
    "  - Use `thrust::device` execution policy to specify where you want `thrust::sort` to run\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this section only after you’ve made a serious attempt at solving the problem. Once you’ve completed your solution, compare it with the reference provided here to evaluate your approach and identify any potential improvements.\n",
    "\n",
    "<details>\n",
    "  <summary>Solution</summary>\n",
    "\n",
    "  Key points:\n",
    "  - change `std::sort` to `thrust::sort`\n",
    "  - add `thrust::device` execution policy parameter\n",
    "\n",
    "  Solution:\n",
    "  ```c++\n",
    "  float median(thrust::universal_vector<float> vec) {\n",
    "    thrust::sort(thrust::device, vec.begin(), vec.end());\n",
    "    return vec[vec.size() / 2];\n",
    "  }\n",
    "  ```\n",
    "\n",
    "  You can find full solution [here](Solutions/port-sort-to-gpu.cu).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now you should be comfortable with the concept of execution spaces and be able to port standard algorithms to GPU with Thrust!\n",
    "\n",
    "But what if your algorithm is not a standard one?\n",
    "\n",
    "Proceed to the [next section](../01.03-Extending-Algorithms/01.03.01-Extending-Algorithms.ipynb) to learn how to extend standard algorithms to your unique use cases."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}