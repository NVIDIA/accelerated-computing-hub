{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Execution Spaces\n",
        "\n",
        "## Content\n",
        "\n",
        "* [Heterogeneous Programming Model](#Heterogeneous-Programming-Model)\n",
        "* [Execution Policy](#Execution-Policy)\n",
        "* [Exercise: Annotate Execution Spaces](01.02.02-Exercise-Annotate-Execution-Spaces.ipynb)\n",
        "* [Exercise: Changing Execution Space](01.02.03-Exercise-Changing-Execution-Space.ipynb)\n",
        "* [Exercise: Compute Median Temperature](01.02.04-Exercise-Compute-Median-Temperature.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "By the end of this lab, you’ll have your first code running on a GPU!\n",
        "But what exactly does it mean to run code on GPU? \n",
        "For that matter, what does it mean to run code anywhere? \n",
        "Let's start by working our way through this question. \n",
        "\n",
        "To build intuition around such fundamental questions, we'll be simulating heat conduction.\n",
        "We'll start with a very simple version that simulates how objects cool down to the environment temperature.\n",
        "As we gain proficiency with necessary tools, we'll advance this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Google Colab Setup\n",
        "!mkdir -p Sources\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/01.02-Execution-Spaces/Sources/ach.h -nv -O Sources/ach.h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Sources/cpu-cooling.cpp\n",
        "\n",
        "#include <cstdio>\n",
        "#include <vector>\n",
        "\n",
        "int main() {\n",
        "    float k = 0.5;\n",
        "    float ambient_temp = 20;\n",
        "    std::vector<float> temp{ 42, 24, 50 };\n",
        "\n",
        "    std::printf(\"step  temp[0]  temp[1]  temp[2]\\n\");\n",
        "    for (int step = 0; step < 3; step++) {\n",
        "        for (int i = 0; i < temp.size(); i++) {\n",
        "            float diff = ambient_temp - temp[i];\n",
        "            temp[i] = temp[i] + k * diff;\n",
        "        }\n",
        "\n",
        "        std::printf(\"%d     %.2f    %.2f    %.2f\\n\", step, temp[0], temp[1], temp[2]);\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At the beginning of the `main` function, we construct a `std::vector` and store three elements in it:\n",
        "\n",
        "```c++\n",
        "std::vector<float> temp{ 42, 24, 50 };\n",
        "```\n",
        "\n",
        "After that, we transform each element of this vector:\n",
        "\n",
        "```c++\n",
        "for (int i = 0; i < temp.size(); i++) {\n",
        "    float diff = ambient_temp - temp[i];\n",
        "    temp[i] = temp[i] + k * diff;\n",
        "}\n",
        "```\n",
        "\n",
        "Here, we are updating each element of the vector by a constant factor times the difference between the ambient temperature and the current temperature. The result of this computation overwrites each previous element:\n",
        "\n",
        "```c++\n",
        "diff    = 20 - 42;        // -22\n",
        "temp[0] = 42 + 0.5 * -22; // 31.0\n",
        "```\n",
        "\n",
        "Finally, we print the new contents of the vector:\n",
        "\n",
        "If everything goes well and your environment is set up correctly, the cell below should print:\n",
        "\n",
        "| step | temp[0] | temp[1] | temp[2] |\n",
        "| :--- | :------ | :------ | :------ |\n",
        "| 0    | 31.00   | 22.00   | 35.00   |\n",
        "| 1    | 25.50   | 21.00   | 27.50   |\n",
        "| 2    | 22.75   | 20.50   | 23.75   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!g++ Sources/cpu-cooling.cpp -o /tmp/a.out # compile the code\n",
        "!/tmp/a.out # run the executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's revisit the steps that we've just made. \n",
        "We started by compiling our code using the `g++` compiler:\n",
        "```bash\n",
        "g++ Sources/cpu-cooling.cpp -o /tmp/a.out\n",
        "```\n",
        "\n",
        "The `g++` compiler consumed C++ code and produced an executable file, `a.out`, which contains a set of machine instructions. However, there’s a problem: different CPUs support different sets of instructions. For example, if you compile the program above for an x86 CPU, the `temp[i] + k * diff` expression will be compiled into the `vfmadd132ss` instruction on the x86 architecture. If you try running the resulting executable on an ARM CPU, it won’t work because the ARM architecture does not support this instruction. To run this code on an ARM CPU, you would need to compile it specifically for the ARM architecture. In that case, the expression would be compiled into the `vmla.f32` instruction.\n",
        "\n",
        "From this perspective, GPUs are no different.\n",
        "GPUs have their own set of instructions, therefore, we have to compile our code for GPUs somehow.\n",
        "\n",
        "![Compilation process diagram shows how a given C++ expression is turned into architecture-specific instructions](Images/compilation.svg \"Compilation\")\n",
        "\n",
        "The NVIDIA CUDA Compiler (NVCC) allows you to compile C++ code for GPUs.\n",
        "Let's try using it on the same file without changing anything:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc -x cu Sources/cpu-cooling.cpp -o /tmp/a.out # compile the code\n",
        "!/tmp/a.out # run the executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations! You just compiled your first CUDA program!\n",
        "There's one issue, though: ***none of the code above runs on the GPU***.\n",
        "That might be surprising because when we compiled our code for the CPU, the entire program could be executed on a CPU.\n",
        "But now we compile our program for the GPU, and nothing runs on the GPU. \n",
        "This confusion is an indicator that we are missing an important piece of CUDA programming model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Heterogeneous Programming Model\n",
        "\n",
        "GPUs are accelerators rather than standalone processors. \n",
        "A lot of computational work, like interactions with network and file system, is done on the CPU.\n",
        "So a CUDA program always *starts* on the CPU.\n",
        "You, the programmer, are responsible for explicitly specifying which code has to run on the GPU.\n",
        "In other words, you are responsible for specifying which code runs **where**.\n",
        "The established terminology for **where** code is executed is **execution space**.\n",
        "\n",
        "![Heterogeneous programming model](Images/heterogeneous.png \"Heterogeneous programming model\")\n",
        "\n",
        "At a high level, execution spaces are partitioned into **host** (CPU) and **device** (GPU).\n",
        "These terms are used to generalize the programming model.\n",
        "Something other than a CPU could host a GPU, and something other than a GPU could accelerate a CPU.\n",
        "\n",
        "By default, code runs on the **host** side.\n",
        "You are responsible for specifying which code should run on the **device**. \n",
        "This should explain why using `nvcc` alone was insufficient: we haven't marked any code for execution on GPU.\n",
        "\n",
        "So, let's try fixing that. \n",
        "The CUDA compiler, NVCC, is accompanied by a set of core libraries.\n",
        "These libraries allow you to explicitly specify the execution space where you want a given algorithm to run.\n",
        "To prepare our code for these libraries, let's refactor the temperature update `for` loop first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Sources/gpu-cooling.cpp\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cstdio>\n",
        "#include <vector>\n",
        "\n",
        "int main() {\n",
        "    float k = 0.5;\n",
        "    float ambient_temp = 20;\n",
        "    std::vector<float> temp{ 42, 24, 50 };\n",
        "    auto transformation = [=] (float temp) { return temp + k * (ambient_temp - temp); };\n",
        "\n",
        "    std::printf(\"step  temp[0]  temp[1]  temp[2]\\n\");\n",
        "    for (int step = 0; step < 3; step++) {\n",
        "        std::transform(temp.begin(), temp.end(), temp.begin(), transformation);\n",
        "        std::printf(\"%d     %.2f    %.2f    %.2f\\n\", step, temp[0], temp[1], temp[2]);\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc Sources/gpu-cooling.cpp -x cu -arch=native -o /tmp/a.out # compile the code\n",
        "!/tmp/a.out # run the executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of a `for` loop, we used the `std::transform` algorithm from the C++ standard library. \n",
        "One of the benefits of using algorithms instead of custom loops is reduced mental load.\n",
        "Instead of \"executing\" the loop in your mind to see that it implements a transformation pattern,\n",
        "you can quickly recognize it by the algorithm name.\n",
        "\n",
        "But above all else, using algorithms enables you to easily leverage GPUs!\n",
        "For that, we'll be using one of the CUDA Core Libraries called Thrust.\n",
        "Thrust provides standard algorithms and containers that run on the GPU. \n",
        "Let's try using those:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Sources/thrust-cooling.cpp\n",
        "\n",
        "#include <thrust/execution_policy.h>\n",
        "#include <thrust/universal_vector.h>\n",
        "#include <thrust/transform.h>\n",
        "#include <cstdio>\n",
        "\n",
        "int main() {\n",
        "    float k = 0.5;\n",
        "    float ambient_temp = 20;\n",
        "    thrust::universal_vector<float> temp{ 42, 24, 50 };\n",
        "    auto transformation = [=] __host__ __device__ (float temp) { return temp + k * (ambient_temp - temp); };\n",
        "\n",
        "    std::printf(\"step  temp[0]  temp[1]  temp[2]\\n\");\n",
        "    for (int step = 0; step < 3; step++) {\n",
        "        thrust::transform(thrust::device, temp.begin(), temp.end(), temp.begin(), transformation);\n",
        "        std::printf(\"%d     %.2f    %.2f    %.2f\\n\", step, temp[0], temp[1], temp[2]);\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc --extended-lambda Sources/thrust-cooling.cpp -x cu -arch=native -o /tmp/a.out # compile the code\n",
        "!/tmp/a.out # run the executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let's take a look at the changes that we've just made.\n",
        "We started by replacing `std::vector` with `thrust::universal_vector`.\n",
        "We'll explain why this change was necessary later in this lab.\n",
        "More importantly, we annotated the lambda with `__host__ __device__` execution specifiers.\n",
        "\n",
        "As discussed earlier, we have to compile some of the code into GPU instructions. \n",
        "Execution space specifiers tell NVCC which code can be executed on GPU. \n",
        "The `__host__` specifier denotes that a given function is executable by CPU. \n",
        "This specifier is used by default on every C++ function.\n",
        "For example, this means that `int main()` is the same as `__host__ int main()`.\n",
        "\n",
        "The `__device__` specifier, on the other hand, denotes a function that's executable by GPU.\n",
        "That's how NVCC knows which functions to compile for the GPU and which ones for the CPU.\n",
        "In the code above, we combined the `__host__ __device__` specifiers.\n",
        "This indicates that the function can be executed by both CPU and GPU.\n",
        "\n",
        "Finally, we replace `std::transform` with `thrust::transform`. \n",
        "Unlike `std::transform`, `thrust::transform` accepts the execution space as the first parameter.\n",
        "In the code above, we explicitly asked Thrust to perform the transformation on device (GPU) by passing `thrust::device`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution Policy\n",
        "\n",
        "![Execution Policy](Images/execution-policy.svg \"Execution Policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Congratulations!  You've learned some basic truths about execution spaces.  Overall, the goal of this lab is to show you that there's no magic behind CUDA:\n",
        "\n",
        "* Code that starts execution on the host stays on the host.\n",
        "* Code that runs on the device stays on the device.\n",
        "\n",
        "Proceed to your first [exercise](01.02.02-Exercise-Annotate-Execution-Spaces.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },

    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}