{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Spaces\n",
    "\n",
    "## Content\n",
    "\n",
    "* [Host and Device Memory Spaces](Host-and-Device-Memory-Spaces)\n",
    "* [Exercise: Copy](01.06.02-Exercise-Copy.ipynb)\n",
    "\n",
    "At the beginning of this section, we covered execution spaces but left one change without explanation.\n",
    "We replaced `std::vector` with `thrust::universal_vector`.\n",
    "By the end of this lab, you'll understand why this change was necessary.\n",
    "\n",
    "But before we start, let's try to figure out why GPUs are so good at massive parallelism.\n",
    "Many benefits of GPUs result focusing on high throughput.\n",
    "To support massive compute that GPUs are able of sustaining, \n",
    "we have to provide memory speed that matches these capabilities.\n",
    "This essentially means that memory also has to be throughput-oriented.\n",
    "That's why GPUs often come with built-in high-bandwidth memory rather than relying on system memory.\n",
    "Let's return to our code to see how it's affected by this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/01.06-Memory-Spaces/Sources/ach.h -nv -O Sources/ach.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/heat-2D.cpp\n",
    "#include \"ach.h\"\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int height = 4096;\n",
    "  int width  = 4096;\n",
    "\n",
    "  thrust::universal_vector<float> prev = ach::init(height, width);\n",
    "  thrust::universal_vector<float> next(height * width);\n",
    "\n",
    "  for (int write_step = 0; write_step < 3; write_step++) {\n",
    "    std::printf(\"   write step %d\\n\", write_step);\n",
    "    ach::store(write_step, height, width, prev);\n",
    "\n",
    "    for (int compute_step = 0; compute_step < 3; compute_step++) {\n",
    "      auto begin = std::chrono::high_resolution_clock::now();\n",
    "      ach::simulate(height, width, prev, next);\n",
    "      auto end = std::chrono::high_resolution_clock::now();\n",
    "      auto seconds = std::chrono::duration<double>(end - begin).count();\n",
    "      std::printf(\"computed step %d in %g s\\n\", compute_step, seconds);\n",
    "      prev.swap(next);\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we allocate data in `thrust::universal_vector`.\n",
    "Then, `ach::store` accesses content of this vector on CPU to store results on disk.\n",
    "After that, the data is repeatedly accessed by the GPU in the `ach::simulate` function.\n",
    "This is a bit suspicious. \n",
    "We just said that CPU and GPU have distinct memory spaces, \n",
    "but we are not seeing anything that'd reflect this in the code.\n",
    "Maybe performance can reveal something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --extended-lambda -o /tmp/a.out Sources/heat-2D.cpp -x cu -arch=native # build executable\n",
    "!/tmp/a.out # run executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a strange pattern in the execution times. \n",
    "Every time we write data, the next compute step takes 100 times longer to compute.\n",
    "This happens because the data is being implicitly copied between CPU and GPU memory spaces.\n",
    "\n",
    "![Implicit Memory Transfers](Images/managed.png \"Implicit Memory Transfers\")\n",
    "\n",
    "Let's say our data resides in the GPU memory.\n",
    "When `ach::store` accesses it, the data has to be copied to the CPU memory.\n",
    "Next, when we call `ach::simulate`, the data is being accessed by the GPU, so the data has to be copied back.\n",
    "So `thrust::universal_vector` works as a vector that lives in both CPU and GPU memory spaces and automatically migrates between them.\n",
    "The problem is that we know that `ach::store` is not modifying the data, so the copy back to the GPU is unnecessary.\n",
    "Fortunately, we can avoid this extra copy by using explicit memory spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host and Device Memory Spaces\n",
    "\n",
    "Presence of distinct host and device memory spaces is a fundamental concept in GPU programming.\n",
    "For you, as a software engineer, this means that in addition to thinking about where code runs, \n",
    "you also have to keep in mind where the bytes that this code accesses live.\n",
    "On a high level, we have a **host memory space** and a **device memory space**.\n",
    "Thrust provides container types that manage memory in the associated memory spaces.\n",
    "Let's take a look at a program that allocates vectors in corresponding memory spaces:\n",
    "\n",
    "```c++\n",
    "thrust::host_vector<int> h_vec{ 11, 12 };\n",
    "thrust::device_vector<int> d_vec{ 21, 22 };\n",
    "thrust::copy_n(h_vec.begin(), 1, d_vec.begin());\n",
    "```\n",
    "\n",
    "Let's take a look at this code step by step.\n",
    "We started by allocating a vector with two element in host memory.\n",
    "We initialized these two elements with `11` and `12`:\n",
    "\n",
    "```c++\n",
    "thrust::host_vector<int> h_vec{ 11, 12 };\n",
    "```\n",
    "\n",
    "Functionally, there's little difference between `std::vector` and `thrust::host_vector`.\n",
    "As you learn, we suggest using `thrust::host_vector` just to make memory space more pronounced.\n",
    "Besides host vector, we also allocated device one:\n",
    "\n",
    "```c++\n",
    "thrust::device_vector<int> d_vec{ 21, 22 };\n",
    "```\n",
    "\n",
    "We then copied one element from host memory space to device memory space using Thrust copy algorithm.\n",
    "In general, copy is one of the few algorithms that you can provide mixed memory spaces.\n",
    "\n",
    "```c++\n",
    "thrust::copy_n(h_vec.begin(), 1, d_vec.begin());\n",
    "```\n",
    "\n",
    "![Memory Spaces](Images/memory.png \"Memory Spaces\")\n",
    "\n",
    "---\n",
    "For now, it's safe to assume that:\n",
    "\n",
    "- Device memory space is accessible from device execution space\n",
    "- Host memory space is accessible from host execution space\n",
    "- Thrust data movement algorithms can copy data between memory spaces\n",
    "\n",
    "Let's try to internalize these points by practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to [the next exercise](01.06.02-Exercise-Copy.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
