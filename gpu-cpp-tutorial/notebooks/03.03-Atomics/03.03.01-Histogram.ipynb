{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [Histogram of Temperature Grid](#Histogram-of-Temperature-Grid)\n",
    "* [Data Race](#Data-Race)\n",
    "* [Exercise: Fix Histogram](03.03.02-Exercise-Fix-Histogram.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We recently fixed a bug caused by our thread hierarchy, which might prompt the question: why did we need that hierarchy in the first place? \n",
    "To illustrate its value, let’s look at a related problem: computing a histogram of our temperature grid.\n",
    "\n",
    "## Histogram of Temperature Grid\n",
    "A histogram helps visualize the distribution of temperatures by grouping values into \"bins\".\n",
    "In this example, each bin covers a 10-degree range, so the first bin represents temperatures in `[0, 10)`, the second in `[10, 20)`, and so on.\n",
    "\n",
    "![Histogram](Images/histogram.png \"Histogram\")\n",
    "\n",
    "Given a cell’s temperature, how do we determine the bin it belongs to? We can simply use integer division:\n",
    "\n",
    "```c++\n",
    "int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
    "```\n",
    "\n",
    "So, a temperature of 14 falls into bin 1, while 4 maps to bin 0. \n",
    "Next, we’ll implement this logic in a CUDA kernel, assigning one thread per cell to calculate its bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.03-Atomics/Sources/ach.cuh -nv -O Sources/ach.cuh\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.03-Atomics/Sources/__init__.py -nv -O Sources/__init__.py\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/03.03-Atomics/Sources/ach.py -nv -O Sources/ach.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/histogram-bug.cpp\n",
    "#include \"ach.cuh\"\n",
    "\n",
    "constexpr float bin_width = 10;\n",
    "\n",
    "__global__ void histogram_kernel(cuda::std::span<float> temperatures,\n",
    "                                 cuda::std::span<int> histogram)\n",
    "{\n",
    "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
    "  int old_count = histogram[bin];\n",
    "  int new_count = old_count + 1;\n",
    "  histogram[bin] = new_count;\n",
    "}\n",
    "\n",
    "void histogram(cuda::std::span<float> temperatures,\n",
    "               cuda::std::span<int> histogram,\n",
    "               cudaStream_t stream)\n",
    "{\n",
    "  int block_size = 256;\n",
    "  int grid_size = cuda::ceil_div(temperatures.size(), block_size);\n",
    "  histogram_kernel<<<grid_size, block_size, 0, stream>>>(\n",
    "    temperatures, histogram);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Sources.ach\n",
    "Sources.ach.run(\"Sources/histogram-bug.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Race\n",
    "Something went wrong. \n",
    "Despite having four million cells, our histogram comes out nearly empty. \n",
    "The culprit is in this kernel code:\n",
    "\n",
    "```c++\n",
    "int old_count = histogram[bin];\n",
    "int new_count = old_count + 1;\n",
    "histogram[bin] = new_count;\n",
    "```\n",
    "\n",
    "Because this code runs simultaneously on millions of threads while attempting to read/write a single copy of the `histogram` span, it introduces a data race.  \n",
    "For example, if two threads increment the same bin at the same time, \n",
    "both read the same initial value and overwrite one another’s updates, \n",
    "causing the bin to increment only once instead of twice. \n",
    "Multiplied by millions of cells, this leads to a nearly empty histogram.\n",
    "\n",
    "![Data Race](Images/race.png \"Data Race\")\n",
    "\n",
    "To fix this, we need to make the read, modify, and write steps a single, indivisible operation. \n",
    "CUDA provides atomic operations that handle concurrency safely, ensuring we don’t lose any increments in our histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/atomic.cpp\n",
    "#include <cuda/std/span>\n",
    "#include <cuda/std/atomic>\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/host_vector.h>\n",
    "\n",
    "__global__ void kernel(cuda::std::span<int> count)\n",
    "{\n",
    "    // Wrap data in atomic_ref\n",
    "    cuda::std::atomic_ref<int> ref(count[0]);\n",
    "\n",
    "    // Atomically increment the underlying value\n",
    "    ref.fetch_add(1);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    thrust::device_vector<int> count(1);\n",
    "\n",
    "    int threads_in_block = 256;\n",
    "    int blocks_in_grid = 42;\n",
    "\n",
    "    kernel<<<blocks_in_grid, threads_in_block>>>(\n",
    "        cuda::std::span<int>{thrust::raw_pointer_cast(count.data()), 1});\n",
    "\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    thrust::host_vector<int> count_host = count;\n",
    "    std::cout << \"expected: \" << threads_in_block * blocks_in_grid << std::endl;\n",
    "    std::cout << \"observed: \" << count_host[0] << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native  Sources/atomic.cpp -x cu -arch=native -o /tmp/a.out -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we reproduce our histogram kernel’s structure, where multiple threads attempt to increment the same memory location. \n",
    "This time, however, we wrap the memory reference in a `cuda::std::atomic_ref<int>`:\n",
    "\n",
    "```c++\n",
    "cuda::std::atomic_ref<int> ref(count[0]);\n",
    "```\n",
    "\n",
    "Here, `int` indicates the type of the underlying value, and the constructor accepts a reference to the memory we want to modify. \n",
    "The resulting atomic_ref object offers atomic operations, such as:\n",
    "\n",
    "```c++\n",
    "ref.fetch_add(1);\n",
    "```\n",
    "\n",
    "This call performs an indivisible read-modify-write operation: it reads the current value of `count[0]`, adds one, and writes the result back atomically.\n",
    "You can think of atomics as writing an instruction rather than a direct value. \n",
    "\n",
    "![Atomics](Images/atomic.png \"Atomics\")\n",
    "\n",
    "\n",
    "The \"?\" is replaced by the current value of `count[0]`, incremented by one, and stored in a single step. \n",
    "It doesn’t matter how many threads do this concurrently - the result remains correct.\n",
    "\n",
    "---\n",
    "\n",
    "In the next exercise, you will fix the histogram kernel using atomics.\n",
    "Move on to the [next exercise](03.03.02-Exercise-Fix-Histogram.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
