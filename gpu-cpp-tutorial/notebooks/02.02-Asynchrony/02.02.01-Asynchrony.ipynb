{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Asynchrony\n",
        "\n",
        "## Content\n",
        "\n",
        "* [Overlapping](#Overlapping)\n",
        "* [CUB](#CUB)\n",
        "* [Exercise: Compute / IO Overlap](02.02.02-Exercise-Compute-IO-Overlap.ipynb)\n",
        "* [Exercise: Compute-IO Overlap](02.02.02-Exercise-Compute-IO-Overlap.ipynb)\n",
        "* [Exercise: Profile Your Code with Nsight Systems](02.02.03-Exercise-Nsight.ipynb)\n",
        "* [Exercise: Use NVTX](02.02.04-Exercise-NVTX.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "In the previous sections, we learned about:\n",
        "* Execution spaces (where code runs: CPU vs. GPU)\n",
        "* Memory spaces (where data is stored: host vs. device)\n",
        "* Parallel algorithms (how to run operations in parallel using Thrust)\n",
        "\n",
        "By combining these concepts, we improved our simulator. \n",
        "Here’s what the updated simulator code looks like:\n",
        "\n",
        "```c++\n",
        "void simulate(int height, int width, \n",
        "              thrust::device_vector<float> &in, \n",
        "              thrust::device_vector<float> &out) \n",
        "{\n",
        "  cuda::std::mdspan temp_in(thrust::raw_pointer_cast(in.data()), height, width);\n",
        "  thrust::tabulate(\n",
        "    thrust::device, out.begin(), out.end(), \n",
        "    [=] __host__ __device__(int id) { /* ... */ }\n",
        "  );\n",
        "}\n",
        "\n",
        "for (int write_step = 0; write_step < 3; write_step++) \n",
        "{\n",
        "  thrust::copy(d_prev.begin(), d_prev.end(), h_prev.begin());\n",
        "  ach::store(write_step, height, width, h_prev);\n",
        "\n",
        "  for (int compute_step = 0; compute_step < 3; compute_step++) {\n",
        "    simulate(height, width, d_prev, d_next);\n",
        "    d_prev.swap(d_next);\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "In this loop we do the following:\n",
        "1. Copy data from the device (GPU) to the host (CPU).\n",
        "2. Write the host data to disk.\n",
        "3. Compute the next state on the GPU.\n",
        "\n",
        "This process can be visualized as follows:\n",
        "\n",
        "![Sync](Images/sync.png \"Sync\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overlapping\n",
        "\n",
        "We see that Thrust launches work on the GPU for each simulation step (`thrust::tabulate`).\n",
        "However, it then waits for the GPU to finish before returning control to the CPU.\n",
        "Because Thrust calls are synchronous, the CPU remains idle whenever the GPU is working.\n",
        "Writing efficient heterogeneous code means utilizing all available resources, including the CPU.\n",
        "In many real-world applications, we can keep the CPU busy at the same time the GPU is computing. \n",
        "This is called *overlapping*. \n",
        "Instead of waiting idly, the CPU could do something useful, like write data while the GPU is crunching numbers.\n",
        "\n",
        "Here’s a simple way to visualize that concept:\n",
        "\n",
        "![Compute-IO-Overlap](Images/overlap.png \"Compute/IO Overlap\")\n",
        "\n",
        "While the GPU is computing the next simulation step, the CPU can be writing out the previous results to disk.\n",
        "This overlap uses both CPU and GPU resources efficiently, reducing the total runtime.\n",
        "Unfortunately, Thrust’s interface doesn’t provide a direct way to separate launching GPU work from waiting for its completion.\n",
        "Under the hood, Thrust calls another library called [CUB (CUDA UnBound)](https://nvidia.github.io/cccl/cub/) to implement its GPU algorithms.  If you look at the software stack, you'll see CUB us underneath Thrust.  CUB is also a library in it's own right.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Google Colab Setup\n",
        "!mkdir -p Sources\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/02.02-Asynchrony/Sources/ach.h -nv -O Sources/ach.h\n",
        "!wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-cpp-tutorial/notebooks/02.02-Asynchrony/Sources/nvtx3.hpp -nv -O Sources/nvtx3.hpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUB\n",
        "If we want finer control to use the CPU while GPU kernels are still running, we need more flexible tools. \n",
        "That’s where direct libraries like CUB come into play.\n",
        "\n",
        "Let's take a closer look at CUB: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Sources/cub-perf.cpp\n",
        "#include \"ach.h\"\n",
        "\n",
        "float simulate(int width,\n",
        "               int height,\n",
        "               const thrust::device_vector<float> &in,\n",
        "                     thrust::device_vector<float> &out,\n",
        "               bool use_cub) \n",
        "{\n",
        "  cuda::std::mdspan temp_in(thrust::raw_pointer_cast(in.data()), height, width);\n",
        "  auto compute = [=] __host__ __device__(int id) {\n",
        "    const int column = id % width;\n",
        "    const int row    = id / width;\n",
        "\n",
        "    // loop over all points in domain (except boundary)\n",
        "    if (row > 0 && column > 0 && row < height - 1 && column < width - 1)\n",
        "    {\n",
        "      // evaluate derivatives\n",
        "      float d2tdx2 = temp_in(row, column - 1) - 2 * temp_in(row, column) + temp_in(row, column + 1);\n",
        "      float d2tdy2 = temp_in(row - 1, column) - 2 * temp_in(row, column) + temp_in(row + 1, column);\n",
        "\n",
        "      // update temperatures\n",
        "      return temp_in(row, column) + 0.2f * (d2tdx2 + d2tdy2);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "      return temp_in(row, column);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  auto begin = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "  if (use_cub) \n",
        "  {\n",
        "    auto cell_ids = thrust::make_counting_iterator(0);\n",
        "    cub::DeviceTransform::Transform(cell_ids, out.begin(), width * height, compute);\n",
        "  }\n",
        "  else \n",
        "  {\n",
        "    thrust::tabulate(thrust::device, out.begin(), out.end(), compute);\n",
        "  }\n",
        "  auto end = std::chrono::high_resolution_clock::now();\n",
        "  return std::chrono::duration<float>(end - begin).count();\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  std::cout << \"size, thrust, cub\\n\";\n",
        "  for (int size = 1024; size <= 16384; size *= 2)\n",
        "  {\n",
        "    int width = size;\n",
        "    int height = size;\n",
        "    thrust::device_vector<float> current_temp(height * width, 15.0f);\n",
        "    thrust::device_vector<float> next_temp(height * width);\n",
        "\n",
        "    std::cout << size << \", \"\n",
        "              << simulate(width, height, current_temp, next_temp, false) << \", \"\n",
        "              << simulate(width, height, current_temp, next_temp, true) << \"\\n\";\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc --extended-lambda -o /tmp/a.out Sources/cub-perf.cpp -x cu -arch=native # build executable\n",
        "!/tmp/a.out # run executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you run the example cell, you might see some unexpected performance results:\n",
        "\n",
        "- When using Thrust, the runtime increases as the number of cells increases. This makes sense because the GPU is doing more work.\n",
        "- When using CUB, the runtime seems almost constant, regardless of how many cells you use.\n",
        "\n",
        "Why does this happen?\n",
        "1. Thrust is synchronous, meaning it waits for the GPU to finish all work before giving control back to the CPU. Naturally, as we scale the workload, the GPU takes longer, so you see longer total run times.\n",
        "2.\tCUB, on the other hand, is asynchronous. It launches the GPU kernels and then immediately returns control to the CPU. That means your CPU timer stops quickly, and it looks like the GPU work is instantaneous, even though the GPU may still be processing in the background.\n",
        "\n",
        "In other words, CUB’s asynchronous behavior explains why the measured runtime doesn’t grow as expected with the problem size.\n",
        "The GPU is still doing the work, but the CPU measurements aren’t accounting for its actual duration.\n",
        "\n",
        "This answers the question of how Thrust launches work on the GPU, \n",
        "but what causes Thrust to wait?\n",
        "Thrust uses a function from the CUDA Runtime, `cudaDeviceSynchronize()`, to wait for the GPU to finish.  If we insert this function when using CUB, we should see the same behavior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Sources/cub-perf-sync.cpp\n",
        "#include \"ach.h\"\n",
        "\n",
        "float simulate(int width,\n",
        "               int height,\n",
        "               const thrust::device_vector<float> &in,\n",
        "                     thrust::device_vector<float> &out,\n",
        "               bool use_cub) \n",
        "{\n",
        "  cuda::std::mdspan temp_in(thrust::raw_pointer_cast(in.data()), height, width);\n",
        "  auto compute = [=] __host__ __device__(int id) {\n",
        "    const int column = id % width;\n",
        "    const int row    = id / width;\n",
        "\n",
        "    // loop over all points in domain (except boundary)\n",
        "    if (row > 0 && column > 0 && row < height - 1 && column < width - 1)\n",
        "    {\n",
        "      // evaluate derivatives\n",
        "      float d2tdx2 = temp_in(row, column - 1) - 2 * temp_in(row, column) + temp_in(row, column + 1);\n",
        "      float d2tdy2 = temp_in(row - 1, column) - 2 * temp_in(row, column) + temp_in(row + 1, column);\n",
        "\n",
        "      // update temperatures\n",
        "      return temp_in(row, column) + 0.2f * (d2tdx2 + d2tdy2);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "      return temp_in(row, column);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  auto begin = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "  if (use_cub) \n",
        "  {\n",
        "    auto cell_ids = thrust::make_counting_iterator(0);\n",
        "    cub::DeviceTransform::Transform(cell_ids, out.begin(), width * height, compute);\n",
        "    cudaDeviceSynchronize();\n",
        "  }\n",
        "  else \n",
        "  {\n",
        "    thrust::tabulate(thrust::device, out.begin(), out.end(), compute);\n",
        "  }\n",
        "  auto end = std::chrono::high_resolution_clock::now();\n",
        "  return std::chrono::duration<float>(end - begin).count();\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  std::cout << \"size, thrust, cub\\n\";\n",
        "  for (int size = 1024; size <= 16384; size *= 2)\n",
        "  {\n",
        "    int width = size;\n",
        "    int height = size;\n",
        "    thrust::device_vector<float> current_temp(height * width, 15.0f);\n",
        "    thrust::device_vector<float> next_temp(height * width);\n",
        "\n",
        "    std::cout << size << \", \"\n",
        "              << simulate(width, height, current_temp, next_temp, false) << \", \"\n",
        "              << simulate(width, height, current_temp, next_temp, true) << \"\\n\";\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc --extended-lambda -o /tmp/a.out Sources/cub-perf-sync.cpp -x cu -arch=native # build executable\n",
        "!/tmp/a.out # run executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above is similar to the previous example, but with the addition of `cudaDeviceSynchronize()` after the calls to CUB.\n",
        "`cudaDeviceSynchronize()` is a CUDA Runtime function that causes the CPU to wait until the GPU has finished all work.\n",
        "With `cudaDeviceSynchronize()`, you can see that it takes the same time for both Thrust and CUB to complete the work.\n",
        "\n",
        "We can now use CUB and `cudaDeviceSynchronize()` to control overlap computation and I/O.\n",
        "This change should result in a significant speedup, as the CPU can now write data to disk while the GPU is computing the next simulation step:\n",
        "\n",
        "![Expected Speedup](Images/sync-cub-vs-thrust.png \"Expected Speedup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Proceed to the [exercise](02.02.02-Exercise-Compute-IO-Overlap.ipynb) to make this change and see the performance improvement for yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },

    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}