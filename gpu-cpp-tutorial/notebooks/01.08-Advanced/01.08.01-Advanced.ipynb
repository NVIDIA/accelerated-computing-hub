{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Advanced Content\n",
    "\n",
    "The following exercises provide an additional challenge for those with time and interest. They require the use of more advanced techniques, and provide less scaffolding. They are difficult and excellent for your development.\n",
    "\n",
    "---\n",
    "## Exercise: Computing Run Length Encode\n",
    "\n",
    "---\n",
    "### Limitations\n",
    "\n",
    "Following the previous recipes may occasionally yield unexpected results, \n",
    "due to certain limitations imposed by CUDA on C++.\n",
    "Let’s examine one of these limitations. \n",
    "\n",
    "In C++, standard algorithms don’t require the use of lambdas. \n",
    "For example, you might want to extract a lambda into a named function for reuse. \n",
    "Let’s attempt to do that with our transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/host-function-pointers.cu\n",
    "#include <thrust/transform.h>\n",
    "#include <thrust/universal_vector.h>\n",
    "#include <cstdio>\n",
    "\n",
    "__host__ __device__ float transformation(float x) {\n",
    "  return 2 * x + 1;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  thrust::universal_vector<float> vec{ 1, 2, 3 };\n",
    "\n",
    "  thrust::transform(vec.begin(), vec.end(), vec.begin(), transformation);\n",
    "\n",
    "  std::printf(\"%g %g %g\\n\", vec[0], vec[1], vec[2]);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, if you run this code, you'll likely see an exception saying something about invalid program counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o /tmp/a.out Sources/host-function-pointers.cu\n",
    "!/tmp/a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, invoking this named function from within a lambda works just fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/host-function-pointers-fix.cu\n",
    "#include <thrust/transform.h>\n",
    "#include <thrust/universal_vector.h>\n",
    "#include <cstdio>\n",
    "\n",
    "__host__ __device__ float transformation(float x) {\n",
    "  return 2 * x + 1;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  thrust::universal_vector<float> vec{ 1, 2, 3 };\n",
    "\n",
    "  thrust::transform(vec.begin(), vec.end(), vec.begin(), [] __host__ __device__ (float x) {\n",
    "    return transformation(x);\n",
    "  });\n",
    "\n",
    "  std::printf(\"%g %g %g\\n\", vec[0], vec[1], vec[2]);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --extended-lambda -arch=native -o /tmp/a.out Sources/host-function-pointers-fix.cu\n",
    "!/tmp/a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's going on?\n",
    "This issue is related to one of the CUDA [limitations](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#function-pointers):\n",
    "\n",
    "> It is not allowed to take the address of a __device__ function in host code.\n",
    "\n",
    "We invoke `thrust::transform` on the host (CPU code).\n",
    "When we pass `transformation` function to `thrust::transform`, C++ implicitly takes its address. \n",
    "But as we just learned, taking address of `__device__` function is not allowed on the host.\n",
    "\n",
    "That should shed some light on why the version with lambda works.\n",
    "A lambda is not a function, it's a function object.\n",
    "The code below illustrates what lambda actually looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/function-objects.cu\n",
    "\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <thrust/universal_vector.h>\n",
    "#include <thrust/transform.h>\n",
    "#include <cstdio>\n",
    "\n",
    "struct transformation {\n",
    "  __host__ __device__ float operator()(float x) {\n",
    "    return 2 * x + 1;\n",
    "  }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "  thrust::universal_vector<float> vec{ 1, 2, 3 };\n",
    "\n",
    "  thrust::transform(thrust::device, vec.begin(), vec.end(), vec.begin(), transformation{});\n",
    "\n",
    "  std::printf(\"%g %g %g\\n\", vec[0], vec[1], vec[2]);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --extended-lambda -arch=native -o /tmp/a.out Sources/function-objects.cu\n",
    "!/tmp/a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code passes an object, and the `__device__` operator is not referenced on the host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
