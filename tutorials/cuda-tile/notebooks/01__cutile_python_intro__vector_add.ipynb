{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90574f83-d15b-4970-a2e0-fae792b5ab21",
   "metadata": {},
   "source": [
    "# cuTile Python Intro - Vector Add\n",
    "\n",
    "In this module, you'll be introduced to the fundamentals of the CUDA Tile programming model and write your first kernel with cuTile Python.\n",
    "\n",
    "cuTile Python is a Python DSL for writing CUDA Tile programs; it lowers to Tile IR, a new MLIR-based intermediate representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14cb93-a513-4afc-8fd5-1ab665b2b272",
   "metadata": {},
   "source": [
    "## Installing cuTile Python\n",
    "\n",
    "cuTile Python requires:\n",
    "\n",
    "    - NVIDIA Kernel Driver R580 or later.\n",
    "    - CUDA Toolkit 13.1 or later.\n",
    "    - A Blackwell GPU (this restriction will be lifted in later releases).\n",
    "\n",
    "You can install cuTile Python via the PIP package `cuda-tile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa2d6a-7cfc-43b9-a55a-0d0f31643153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"BREV_ENV_ID\") and not os.path.exists(\"/accelerated-computing-hub-installed\"): # If not running in brev\n",
    "  print(\"Installing PIP packages.\")\n",
    "  !pip uninstall \"cuda-python\" --yes > /dev/null\n",
    "  !pip install \"cuda-tile\" \"cupy-cuda13x\" > /dev/null 2>&1\n",
    "  open(\"/accelerated-computing-hub-installed\", \"a\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0b91a-13b6-4b14-beb5-519c0bd6d810",
   "metadata": {},
   "source": [
    "cuTile Python is typically used in concert with an existing GPU programming framework like CuPy, PyTorch, or JAX that provides an array type that handles GPU memory allocation, etc. For this module, we'll use CuPy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9a32b-1c9c-4f1a-9a74-ac8605cbf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuda.tile as ct\n",
    "from numba import cuda as cs\n",
    "import cupy as cp\n",
    "import cupyx as cpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255855e5-4fa0-4121-b5a3-c6a71d893548",
   "metadata": {},
   "source": [
    "## Example: Vector Add\n",
    "\n",
    "For our first example, we'll implement vector addition, e.g. `C[i] = A[i] + B[i]`.\n",
    "\n",
    "To define a kernel, we write a Python function with the `@ct.kernel` decorator. Kernel functions can take `ct.Array`s as inputs, which could be any type of global array - a CuPy array, a JAX array, a PyTorch tensor, etc. Kernels can also take scalars like ints or floats as inputs.\n",
    "\n",
    "Parameters annotated with `ct.Constant` will be embedded as literals into the compiled kernel. They behave like C++ template parameters. Changing the value of a constant parameter will require the kernel to be JIT compiled again. Some operations require constant parameters. For example, the size of a tile must be a constant.\n",
    "\n",
    "cuTile Python supports many of the common NumPy array operations, such as addition, which is highlighted here.\n",
    "\n",
    "Two operations unique to cuTile Python are `load` and `store`:\n",
    "- `ct.load(array, N, shape)` returns the `N`th tile of dimensions `shape` of `array`.\n",
    "- `ct.store(array, N, tile)` stores `tile` at the `N`th tile of dimensions `shape` in `array`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a29b0-076b-4a12-9c49-c9bdf310cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.kernel\n",
    "def vector_add_tile(A: ct.Array, B: ct.Array, C: ct.Array, t_shape: ct.Constant[int]):\n",
    "  a_tile = ct.load(A, index=(ct.bid(0),), shape=(t_shape,))\n",
    "  b_tile = ct.load(B, index=(ct.bid(0),), shape=(t_shape,))\n",
    "  c_tile = a_tile + b_tile\n",
    "  ct.store(C, index=(ct.bid(0),), tile=c_tile)\n",
    "\n",
    "@cs.jit\n",
    "def vector_add_simt(A, B, C, items_per_thread):\n",
    "  bd = cs.blockDim.x\n",
    "  bx = cs.blockIdx.x\n",
    "  tx = cs.threadIdx.x\n",
    "  items_per_block = bd * items_per_thread\n",
    "\n",
    "  base = tx + bx * items_per_block\n",
    "  for i in range(0, items_per_block, bd):\n",
    "    C[base + i] = A[base + i] + B[base + i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dce420-dc2f-425b-b199-a93090bbc589",
   "metadata": {},
   "source": [
    "Next, let's validate that our is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bd687-740a-4455-b438-8d5a47dd4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_shape = 2**24\n",
    "t_shape = 1024\n",
    "\n",
    "A = cp.random.uniform(-5, 5, a_shape)\n",
    "B = cp.random.uniform(-5, 5, a_shape)\n",
    "C = cp.zeros_like(A)\n",
    "\n",
    "grid = (ct.cdiv(a_shape, t_shape), 1, 1)\n",
    "ct.launch(cp.cuda.get_current_stream(), grid, vector_add_tile, (A, B, C, t_shape))\n",
    "\n",
    "cp.testing.assert_array_almost_equal(C, A + B) # Verify the results\n",
    "\n",
    "threads_per_block = 128\n",
    "items_per_thread = 4\n",
    "thread_blocks = int(a_shape / (threads_per_block * items_per_thread))\n",
    "\n",
    "C = cp.zeros_like(A)\n",
    "\n",
    "vector_add_simt[thread_blocks, threads_per_block](A, B, C, items_per_thread)\n",
    "\n",
    "cp.testing.assert_array_almost_equal(C, A + B) # Verify the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28c5b7-87db-403d-bfbf-47255220600b",
   "metadata": {},
   "source": [
    "Finally, let's benchmark it against CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c0085-5550-426f-b7c6-13b740046268",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy = cpx.profiler.benchmark(\n",
    "  lambda: A + B,\n",
    "  (), n_repeat=15, n_warmup=5\n",
    ").gpu_times[0]\n",
    "tile = cpx.profiler.benchmark(\n",
    "  lambda: ct.launch(cp.cuda.get_current_stream(), grid, vector_add_tile, (A, B, C, t_shape)),\n",
    "  (), n_repeat=15, n_warmup=5\n",
    ").gpu_times[0]\n",
    "simt = cpx.profiler.benchmark(\n",
    "  lambda: vector_add_simt[thread_blocks, threads_per_block](A, B, C, items_per_thread),\n",
    "  (), n_repeat=15, n_warmup=5\n",
    ").gpu_times[0]\n",
    "\n",
    "gbytes_read = (3 * a_shape * A.dtype.itemsize) / 2**30\n",
    "\n",
    "print(f\"CuPy: {cupy.mean():.3g} s ± {(cupy.std() / cupy.mean()):.2%} (mean ± relative stdev of {cupy.size} runs), {gbytes_read / cupy.mean():.1f} GB/s\")\n",
    "print(f\"Tile: {tile.mean():.3g} s ± {(tile.std() / tile.mean()):.2%} (mean ± relative stdev of {tile.size} runs), {gbytes_read / tile.mean():.1f} GB/s\")\n",
    "print(f\"SIMT: {simt.mean():.3g} s ± {(simt.std() / simt.mean()):.2%} (mean ± relative stdev of {simt.size} runs), {gbytes_read / simt.mean():.1f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b878eb-e291-467e-847d-84eff39903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for items_per_thread in [1, 2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "  for threads_per_block in [32, 64, 128, 256, 512]:\n",
    "    thread_blocks = int(a_shape / (threads_per_block * items_per_thread))\n",
    "    time = cpx.profiler.benchmark(\n",
    "      lambda: vector_add_simt[thread_blocks, threads_per_block](A, B, C, items_per_thread),\n",
    "      (), n_repeat=15, n_warmup=5\n",
    "    ).gpu_times[0].mean()\n",
    "\n",
    "    times.append({'items_per_thread': items_per_thread, 'threads_per_block': threads_per_block, 'time': time})\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "for time in times:\n",
    "  print(f\"items_per_thread={time['items_per_thread']}, threads_per_block={time['threads_per_block']}, time={time['time']:.3g} s\")\n",
    "\n",
    "print(reduce(lambda x, y: x if x['time'] < y['time'] else y, times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819250a1-553c-464f-aed0-ec8fd2ae838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for t_shape in [16, 32, 64, 128, 256, 512, 1024]:\n",
    "  grid = (ct.cdiv(a_shape, t_shape), 1, 1)\n",
    "  time = cpx.profiler.benchmark(\n",
    "    lambda: ct.launch(cp.cuda.get_current_stream(), grid, vector_add_tile, (A, B, C, t_shape)),\n",
    "    (), n_repeat=15, n_warmup=5\n",
    "  ).gpu_times[0].mean()\n",
    "\n",
    "  times.append({'t_shape': t_shape, 'time': time})\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "for time in times:\n",
    "  print(f\"t_shape={time['t_shape']}, time={time['time']:.3g} s\")\n",
    "\n",
    "print(reduce(lambda x, y: x if x['time'] < y['time'] else y, times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f2b27-c8b6-4b19-baa1-f7ba023ff8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8503b38-c453-4246-ab20-cf3ce88c593a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
