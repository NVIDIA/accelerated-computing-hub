{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90574f83-d15b-4970-a2e0-fae792b5ab21",
   "metadata": {},
   "source": [
    "# cuTile Python Intro - Vector Add\n",
    "\n",
    "In this module, you'll be introduced to the fundamentals of the CUDA Tile programming model and write your first kernel with cuTile Python.\n",
    "\n",
    "cuTile Python is a Python DSL for writing CUDA Tile programs; it lowers to Tile IR, a new MLIR-based intermediate representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14cb93-a513-4afc-8fd5-1ab665b2b272",
   "metadata": {},
   "source": [
    "## Installing cuTile Python\n",
    "\n",
    "cuTile Python requires:\n",
    "\n",
    "    - NVIDIA Kernel Driver R580 or later.\n",
    "    - CUDA Toolkit 13.1 or later.\n",
    "    - A Blackwell GPU (this restriction will be lifted in later releases).\n",
    "\n",
    "You can install cuTile Python via the PIP package `cuda-tile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa2d6a-7cfc-43b9-a55a-0d0f31643153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"BREV_ENV_ID\") and not os.path.exists(\"/accelerated-computing-hub-installed\"): # If not running in brev\n",
    "  print(\"Installing PIP packages.\")\n",
    "  !pip uninstall \"cuda-python\" --yes > /dev/null\n",
    "  !pip install \"cuda-tile\" \"cupy-cuda13x\" > /dev/null 2>&1\n",
    "  open(\"/accelerated-computing-hub-installed\", \"a\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0b91a-13b6-4b14-beb5-519c0bd6d810",
   "metadata": {},
   "source": [
    "cuTile Python is typically used in concert with an existing GPU programming framework like CuPy, PyTorch, or JAX that provides an array type that handles GPU memory allocation, etc. For this module, we'll use CuPy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9a32b-1c9c-4f1a-9a74-ac8605cbf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuda.tile as ct\n",
    "from numba import cuda as cs\n",
    "import cupy as cp\n",
    "import cupyx as cpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255855e5-4fa0-4121-b5a3-c6a71d893548",
   "metadata": {},
   "source": [
    "## Example: Vector Add\n",
    "\n",
    "For our first example, we'll implement vector addition, e.g. `C[i] = A[i] + B[i]`.\n",
    "\n",
    "To define a kernel, we write a Python function with the `@ct.kernel` decorator. Kernel functions can take `ct.Array`s as inputs, which could be any type of global array - a CuPy array, a JAX array, a PyTorch tensor, etc. Kernels can also take scalars like ints or floats as inputs.\n",
    "\n",
    "Parameters annotated with `ct.Constant` will be embedded as literals into the compiled kernel. They behave like C++ template parameters. Changing the value of a constant parameter will require the kernel to be JIT compiled again. Some operations require constant parameters. For example, the size of a tile must be a constant.\n",
    "\n",
    "cuTile Python supports many of the common NumPy array operations, such as addition, which is highlighted here.\n",
    "\n",
    "Two operations unique to cuTile Python are `load` and `store`:\n",
    "- `ct.load(array, N, shape)` returns the `N`th tile of dimensions `shape` of `array`.\n",
    "- `ct.store(array, N, tile)` stores `tile` at the `N`th tile of dimensions `shape` in `array`.\n",
    "\n",
    "Let's implement our vector addition kernel, both on CUDA Tile and CUDA SIMT (for comparison).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a29b0-076b-4a12-9c49-c9bdf310cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.kernel\n",
    "def vector_add_tile(A: ct.Array, B: ct.Array, C: ct.Array, t_shape: ct.Constant[int]):\n",
    "  a_tile = ct.load(A, index=(ct.bid(0),), shape=(t_shape,))\n",
    "  b_tile = ct.load(B, index=(ct.bid(0),), shape=(t_shape,))\n",
    "  c_tile = a_tile + b_tile\n",
    "  ct.store(C, index=(ct.bid(0),), tile=c_tile)\n",
    "\n",
    "@cs.jit\n",
    "def vector_add_simt(A, B, C, items_per_thread):\n",
    "  bd = cs.blockDim.x\n",
    "  bx = cs.blockIdx.x\n",
    "  tx = cs.threadIdx.x\n",
    "  items_per_block = bd * items_per_thread\n",
    "\n",
    "  base = tx + bx * items_per_block\n",
    "  for i in range(0, items_per_block, bd):\n",
    "    C[base + i] = A[base + i] + B[base + i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dce420-dc2f-425b-b199-a93090bbc589",
   "metadata": {},
   "source": [
    "Next, let's validate that we've implemented everything correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bd687-740a-4455-b438-8d5a47dd4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_shape = 2 ** 24\n",
    "t_shape = 1024\n",
    "\n",
    "A = cp.random.uniform(-5, 5, a_shape)\n",
    "B = cp.random.uniform(-5, 5, a_shape)\n",
    "C = cp.zeros_like(A)\n",
    "\n",
    "grid = (ct.cdiv(a_shape, t_shape), 1, 1)\n",
    "ct.launch(cp.cuda.get_current_stream(), grid, vector_add_tile, (A, B, C, t_shape))\n",
    "\n",
    "cp.testing.assert_array_almost_equal(C, A + B) # Verify the results\n",
    "\n",
    "threads_per_block = 128\n",
    "items_per_thread = 4\n",
    "thread_blocks = int(a_shape / (threads_per_block * items_per_thread))\n",
    "\n",
    "C = cp.zeros_like(A)\n",
    "\n",
    "vector_add_simt[thread_blocks, threads_per_block](A, B, C, items_per_thread)\n",
    "\n",
    "cp.testing.assert_array_almost_equal(C, A + B) # Verify the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28c5b7-87db-403d-bfbf-47255220600b",
   "metadata": {},
   "source": [
    "Finally, let's benchmark it against CuPy and SIMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9450a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device\n",
    "\n",
    "def get_peak_memory_bandwidth(device_id: int = 0):\n",
    "  dev = Device(device_id)\n",
    "  dev.set_current() # Initialize CUDA for this thread\n",
    "\n",
    "  props = dev.properties\n",
    "  mem_clock_khz = props.memory_clock_rate        # Peak memory clock in kHz.\n",
    "  bus_width_bits = props.global_memory_bus_width # DRAM bus width in bits.\n",
    "\n",
    "  bytes_per_s = (mem_clock_khz * 1_000) * (bus_width_bits / 8) * 2 # DDR factor.\n",
    "  return bytes_per_s / 2 ** 30\n",
    "\n",
    "memory_read = (3 * a_shape * A.dtype.itemsize) / 2 ** 30\n",
    "\n",
    "peak_memory_bandwidth = get_peak_memory_bandwidth(0)\n",
    "\n",
    "def print_benchmark_results(framework, results):\n",
    "  time = results.mean()\n",
    "  time_unc = results.std() / time\n",
    "  runs = results.size\n",
    "  bw = memory_read / time\n",
    "  percent_of_peak_bw = bw / peak_memory_bandwidth\n",
    "\n",
    "  print(f\"{framework}: {time:.3g} s ± {time_unc:.2%} (mean ± relative stdev of {runs} runs), {bw:.1f} GB/s, {percent_of_peak_bw:.2%} of peak bandwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c0085-5550-426f-b7c6-13b740046268",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy = cpx.profiler.benchmark(\n",
    "  lambda: A + B,\n",
    "  (), n_repeat=15, n_warmup=5\n",
    ").gpu_times[0]\n",
    "tile = cpx.profiler.benchmark(\n",
    "  lambda: ct.launch(cp.cuda.get_current_stream(), grid, vector_add_tile, (A, B, C, t_shape)),\n",
    "  (), n_repeat=15, n_warmup=5\n",
    ").gpu_times[0]\n",
    "simt = cpx.profiler.benchmark(\n",
    "  lambda: vector_add_simt[thread_blocks, threads_per_block](A, B, C, items_per_thread),\n",
    "  (), n_repeat=15, n_warmup=5\n",
    ").gpu_times[0]\n",
    "\n",
    "print_benchmark_results(\"CuPy\", cupy)\n",
    "print_benchmark_results(\"Tile\", tile)\n",
    "print_benchmark_results(\"SIMT\", simt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6eac7",
   "metadata": {},
   "source": [
    "Kernel performance often depends on tile sizes and execution parameters, so let's explore the parameter space to make sure our results are reasonable. First, let's tune our tile kernel over the tile size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b878eb-e291-467e-847d-84eff39903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for t_shape in [16, 32, 64, 128, 256, 512, 1024]:\n",
    "  grid = (ct.cdiv(a_shape, t_shape), 1, 1)\n",
    "  time = cpx.profiler.benchmark(\n",
    "    lambda: ct.launch(cp.cuda.get_current_stream(), grid, vector_add_tile, (A, B, C, t_shape)),\n",
    "    (), n_repeat=15, n_warmup=5\n",
    "  ).gpu_times[0].mean()\n",
    "\n",
    "  times.append({'t_shape': t_shape, 'time': time})\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "for time in times:\n",
    "  print(f\"t_shape={time['t_shape']}, time={time['time']:.3g} s\")\n",
    "\n",
    "print(reduce(lambda x, y: x if x['time'] < y['time'] else y, times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c907fc6",
   "metadata": {},
   "source": [
    "Next, let's tune our SIMT kernel over the number of items per thread and the number of threads per block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819250a1-553c-464f-aed0-ec8fd2ae838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for items_per_thread in [1, 2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "  for threads_per_block in [32, 64, 128, 256, 512]:\n",
    "    thread_blocks = int(a_shape / (threads_per_block * items_per_thread))\n",
    "    time = cpx.profiler.benchmark(\n",
    "      lambda: vector_add_simt[thread_blocks, threads_per_block](A, B, C, items_per_thread),\n",
    "      (), n_repeat=15, n_warmup=5\n",
    "    ).gpu_times[0].mean()\n",
    "\n",
    "    times.append({'items_per_thread': items_per_thread, 'threads_per_block': threads_per_block, 'time': time})\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "for time in times:\n",
    "  print(f\"items_per_thread={time['items_per_thread']}, threads_per_block={time['threads_per_block']}, time={time['time']:.3g} s\")\n",
    "\n",
    "print(reduce(lambda x, y: x if x['time'] < y['time'] else y, times))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
