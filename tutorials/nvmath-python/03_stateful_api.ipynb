{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04262e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0dba77",
   "metadata": {},
   "source": [
    "<img src=\"./images/nvmath_head_panel@0.5x.png\" alt=\"nvmath-python\" />\n",
    "\n",
    "# Getting Started with nvmath-python: Stateful APIs and Autotuning\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces **nvmath-python**'s stateful APIs and autotuning capabilities. Understanding the difference between stateless and stateful APIs is crucial for optimizing performance in scenarios involving repeated operations, such as batch processing.\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand the difference between stateless (function-form) and stateful (class-form) APIs\n",
    "* Identify the four phases of **nvmath-python** operations: specification, planning, execution, and resource management\n",
    "* Apply stateful APIs to amortize planning costs across multiple executions\n",
    "* Use `nvmath.linalg.advanced.Matmul` class for stateful matrix multiplication\n",
    "* Implement autotuning to find optimal kernel configurations\n",
    "* Benchmark performance improvements from using stateful APIs and autotuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd41a2",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "In many scientific computing workloads, the same operation is performed repeatedly with different data but identical parameters (e.g., matrix shapes, data types, and operation configurations). In such scenarios, performing the full operation setup (specification and planning) for each execution is wasteful. \n",
    "\n",
    "**nvmath-python** addresses this through *stateful APIs* (also called *class-form APIs*), which allow you to separate the specification and planning phases from the execution phase. This enables you to amortize the overhead of setup across multiple executions, significantly improving performance in batch processing scenarios.\n",
    "\n",
    "Additionally, **nvmath-python** provides *autotuning* capabilities that search for optimal kernel configurations beyond the default heuristics, further improving performance for specific problem sizes and configurations.\n",
    "\n",
    "This notebook demonstrates how to use stateful APIs and autotuning to achieve optimal performance in repeated operations.\n",
    "\n",
    "**Prerequisites:** To use this notebook, you will need:\n",
    "- A computer equipped with an NVIDIA GPU\n",
    "- An environment with properly installed Python libraries and (optionally) CUDA Toolkit\n",
    "- Completion of previous notebooks on kernel fusion and memory spaces (recommended)\n",
    "- Understanding of matrix multiplication and batch processing concepts\n",
    "\n",
    "For installation instructions, please refer to the [nvmath-python documentation](https://docs.nvidia.com/cuda/nvmath-python/0.2.1/installation.html#install-nvmath-python).\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "This notebook uses the same benchmarking helper function from previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33311026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a01a",
   "metadata": {},
   "source": [
    "---\n",
    "## Stateful and Stateless APIs\n",
    "\n",
    "Let us consider a scenario typical in neural networks: performing a batch of matrix-matrix multiplications combined with bias and *ReLU* activation. This example will illustrate the differences between stateless and stateful APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2812fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import nvmath\n",
    "from nvmath.linalg.advanced import MatmulEpilog\n",
    "\n",
    "m, n, k = 124, 10, 15\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "d = cp.empty((m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(m, dtype=cp.float32)\n",
    "\n",
    "d = nvmath.linalg.advanced.matmul(a, b, epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias})\n",
    "\n",
    "print(\"d type =\", type(d))\n",
    "print(\"d shape =\", d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b3808",
   "metadata": {},
   "source": [
    "### Understanding Stateless APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3c606",
   "metadata": {},
   "source": [
    "What we have used so far is the *stateless* (or *function-form*) API for `matmul` in **nvmath-python**. This is a convenience API that allows you to perform a desired operation as a single function call and get the result. Under the hood, however, a lot of machinery operates before actual computation takes place. Let us illustrate this using **nvmath-python**'s logging capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate what happens under the hood using nvmath-python's logging capabilities\n",
    "import logging\n",
    "\n",
    "# Configure the root logger to INFO and include timestamps\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\", force=True)\n",
    "logging.disable(logging.NOTSET)  # ensure logging is enabled\n",
    "\n",
    "logging.info(\"About to call matmul() â€” inspect logs for execution flow\")\n",
    "\n",
    "d = nvmath.linalg.advanced.matmul(a, b, epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias})\n",
    "\n",
    "print(\"d type =\", type(d))\n",
    "print(\"d shape =\", d.shape)\n",
    "\n",
    "logging.info(\"Completed matmul() call\")\n",
    "logging.disable(logging.CRITICAL)  # disable logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7be58",
   "metadata": {},
   "source": [
    "### The Four Phases of nvmath-python Operations\n",
    "\n",
    "As you can see from the logging output, quite a bit of action happens during the call to `matmul`:\n",
    "\n",
    "* **SPECIFICATION PHASE:** In this phase, **nvmath-python** analyzes input arguments and creates an internal `Matmul` object with all required data prepared for the underlying [cuBLASLt](https://docs.nvidia.com/cuda/cublas/#using-the-cublaslt-api) library call. Note that this phase may introduce noticeable overhead when inputs are relatively small.\n",
    "\n",
    "* **PLANNING PHASE:** This is where cuBLASLt analyzes the prepared data from the `Matmul` object and performs a search for a suitable algorithm to effectively perform the operation. It uses internal heuristics to select the most promising algorithm. The planning phase also introduces noticeable overhead before actual computation begins.\n",
    "\n",
    "* **EXECUTION PHASE:** This is the phase where actual computation occurs.\n",
    "\n",
    "* **RESOURCE MANAGEMENT PHASE:** This is the final phase where `Matmul` resources are released. Did you notice the respective INFO line `The Matmul object's resources have been released` in the log?\n",
    "\n",
    "### Motivation for Stateful APIs\n",
    "\n",
    "Now imagine that our workflow assumes a **series of matrix multiplications**. In this scenario, for each matrix multiplication we go through all phases over and over again. What if matrix shapes and layouts do not change? What if input `dtypes` do not change? In this case, it would be desirable to perform the *specification* and *planning* once and amortize their cost through *multiple executions*. \n",
    "\n",
    "Exactly for this more sophisticated scenario, **nvmath-python** offers the **stateful** (or **class-form**) API for matrix-matrix multiplication. In this API, you construct an object (an instance of the class `Matmul`, which has methods `plan()` and `execute()`, allowing them to be invoked separately. \n",
    "\n",
    "Did you notice the line in the logger stating `The Matmul operation has been created`? nvmath-python uses *stateful* APIs under-the-hood, and *stateless* APIs are just convenience wrappers around stateful API calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43eb89",
   "metadata": {},
   "source": [
    "### Stateful and stateless implementations for batched `matmul`\n",
    "\n",
    "We will be using a for loop to retrieve the new `a[i]`, `b[i]`, and `bias[i]` in the batch. For the stateful API implementation we will have to use `reset_operands()` method that prepares the new inputs for the `execute()`. Alternatively, you can perform the in-place updates for `a[i]`, `b[i]`, and `bias[i]` in each iteration. \n",
    "\n",
    "**Question:** Which method is faster, resetting operands or in-place update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351716ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import nvmath\n",
    "from nvmath.linalg.advanced import MatmulEpilog\n",
    "\n",
    "m, n, k, batch_size = 124, 10, 15, 8 # Sizes are small for demonstration purposes\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, dtype=cp.float32)\n",
    "\n",
    "\n",
    "def matmul_batched_stateless(a, b, bias):\n",
    "    for i in range(batch_size): # We use a for loop to retrieve the new `a[i]`, `b[i]`, and `bias[i]` in the batch.\n",
    "        d[i] = nvmath.linalg.advanced.matmul(\n",
    "            a[i], b[i], epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[i]}\n",
    "        )\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def matmul_batched_stateful(a, b, bias):\n",
    "    with nvmath.linalg.advanced.Matmul(a[0], b[0]) as mm: # We create a Matmul object for the first batch element.\n",
    "        mm.plan(epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[0]}) # We assume a[0] and b[0] are representative of the batch.\n",
    "        mm.execute() # The execution doesn't require resetting the operands.\n",
    "        for i in range(1, batch_size):\n",
    "            mm.reset_operands(a=a[i], b=b[i], epilog_inputs={\"bias\": bias[i]}) # Subsequent executions require resetting the operands.\n",
    "            d[i] = mm.execute() # We execute with the new operands.\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35647e",
   "metadata": {},
   "source": [
    "Now let us run the logger to see what is happening under-the-hood in stateless and stateful cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\", force=True)\n",
    "logging.disable(logging.NOTSET)\n",
    "\n",
    "logging.info(\"*************** Stateless API ***************\")\n",
    "\n",
    "matmul_batched_stateless(a, b, bias)\n",
    "\n",
    "logging.info(\"*************** Stateful API ***************\")\n",
    "\n",
    "matmul_batched_stateful(a, b, bias)\n",
    "\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d179437",
   "metadata": {},
   "source": [
    "### Comparing Stateless and Stateful APIs\n",
    "\n",
    "You can see from the logging output that in the case of the stateless API there are 8 **SPECIFICATION**, 8 **PLANNING**, and 8 **EXECUTION** phases. In the case of the stateful API implementation, we managed to reduce the number of **SPECIFICATION** and **PLANNING** phases to just 1. Let us measure the performance impact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ae4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k, batch_size = 124, 1024, 1512, 1024 # Select larger sizes for benchmarking purposes\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, dtype=cp.float32)\n",
    "\n",
    "benchmark(\n",
    "    lambda: matmul_batched_stateful(a, b, bias), \"Stateful API\", lambda: matmul_batched_stateless(a, b, bias), \"Stateless API\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6852f",
   "metadata": {},
   "source": [
    "## Exercise: Batch Dimension vs. Batch Sequence\n",
    "\n",
    "In the above example, we implemented batching as a sequence of matrices being processed one by one in a loop. This is a common technique for streaming data or when the entire batch does not fit into GPU memory. An alternative approach is to add a dedicated batching dimension and operate with the batch as a single tensor. The **nvmath-python** library supports both use cases.\n",
    "\n",
    "Implement a batching dimension approach and compare performance to the batch sequence approach. Explain the performance difference (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38995cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Batch Dimension vs. Batch Sequence\n",
    "\n",
    "import nvmath\n",
    "from nvmath.linalg.advanced import MatmulEpilog\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt\n",
    "\n",
    "\n",
    "m, n, k, batch_size = 64, 128, 256, 512 # Select larger sizes for benchmarking purposes\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, 1, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32) # Use pre-allocated resulting array to save memory\n",
    "\n",
    "\n",
    "def matmul_batched_stateless(a, b, bias):\n",
    "    # TODO: Implement stateless batching dimension approach\n",
    "    pass\n",
    "\n",
    "\n",
    "def matmul_batched_stateful_sequence(a, b, bias):\n",
    "    # TODO: Implement stateful batching sequence approach\n",
    "    pass\n",
    "\n",
    "benchmark(\n",
    "    lambda: matmul_batched_stateless(a, b, bias),\n",
    "    \"Stateless batched dimension approach\",\n",
    "    lambda: matmul_batched_stateful_sequence(a, b, bias),\n",
    "    \"Stateful with a batch sequence approach\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1049166",
   "metadata": {},
   "source": [
    "Why `matmul_batched_stateless()` is so much faster than `matmul_batched_stateful_sequence()`? What would be situation when the latter may be a preferred path to go?\n",
    "\n",
    "**Hint:** Consider amount of parallelism that can be exploited in each approach. Also take into consideration the arithmetic intensity in each approach. Last but not least, take into account the amount of consumed memory in each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f9abe",
   "metadata": {},
   "source": [
    "---\n",
    "## Autotuning with nvmath-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5d6d9",
   "metadata": {},
   "source": [
    "Using stateful APIs becomes even more essential when there is a need for autotuning. In many cases, the built-in heuristics for `matmul` kernel selection work reasonably well out-of-the-box. However, there are cases where the underlying cuBLASLt library may choose a suboptimal kernel, and additional tuning is required. \n",
    "\n",
    "Autotuning searches through multiple algorithm candidates and selects the one with the best performance for your specific problem configuration. While autotuning itself has a cost, this cost can be amortized through multiple executions using stateful APIs.\n",
    "\n",
    "Let us see how autotuning works and how its cost can be amortized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k, batch_size = 124, 1024, 1512, 1024\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, dtype=cp.float32)\n",
    "\n",
    "\n",
    "def matmul_batched_stateful_autotuned(a, b, bias):\n",
    "    with nvmath.linalg.advanced.Matmul(a[0], b[0]) as mm:\n",
    "        mm.plan(epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[0]})\n",
    "        mm.autotune(iterations=5)\n",
    "        mm.execute()\n",
    "        for i in range(1, batch_size):\n",
    "            mm.reset_operands(a=a[i], b=b[i], epilog_inputs={\"bias\": bias[i]})\n",
    "            d[i] = mm.execute()\n",
    "\n",
    "\n",
    "benchmark(\n",
    "    lambda: matmul_batched_stateful_autotuned(a, b, bias),\n",
    "    \"Stateful API with autotuning\",\n",
    "    lambda: matmul_batched_stateful(a, b, bias),\n",
    "    \"Stateful API without autotuning\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways_stateful",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Stateless API is convenient for single operations but repeats specification and planning for each call.\n",
    "- Stateful API allows specification and planning once, then multiple executions, significantly improving performance for batched operations.\n",
    "- Four phases in **nvmath-python** operations: specification, planning, execution, and resource management.\n",
    "- Autotuning finds optimal kernels when built-in heuristics are suboptimal, providing additional performance gains.\n",
    "- The cost of autotuning can be amortized across many executions, making it worthwhile for production workloads.\n",
    "\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored **nvmath-python**'s stateful APIs and autotuning capabilities. These advanced features are essential for achieving optimal performance in production workloads involving repeated operations.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **nvmath-python** operations consist of four phases: specification, planning, execution, and resource management.\n",
    "- Stateless (function-form) APIs are convenient but repeat all phases for each call.\n",
    "- Stateful (class-form) APIs allow you to perform specification and planning once, then execute multiple times with different data.\n",
    "- For batch processing, stateful APIs provide significant performance improvements by amortizing setup overhead.\n",
    "- Autotuning searches for optimal kernels beyond default heuristics, providing additional performance gains.\n",
    "- The cost of autotuning can be amortized across many executions using stateful APIs.\n",
    "- Use the **nvmath-python** logging mechanism to understand which phases consume time and optimize accordingly.\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore FFT callbacks in the next notebook: [04_callbacks.ipynb](04_callbacks.ipynb)\n",
    "- Discover device APIs: [05_device_api.ipynb](05_device_api.ipynb)\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "- NVIDIA nvmath-python documentation, \"API Reference,\" https://docs.nvidia.com/cuda/nvmath-python/, Accessed: October 23, 2025.\n",
    "- NVIDIA, \"cuBLASLt Library User Guide,\" https://docs.nvidia.com/cuda/cublas/#using-the-cublaslt-api, Accessed: October 23, 2025.\n",
    "- Williams, Samuel, et al., \"Roofline: An Insightful Visual Performance Model for Multicore Architectures,\" Communications of the ACM, 52(4), 65-76, 2009.\n",
    "- Cormen, Thomas H., et al., \"Introduction to Algorithms,\" 3rd Edition, MIT Press, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
