{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d62cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8f330",
   "metadata": {},
   "source": [
    "<img src=\"./images/nvmath_head_panel@0.5x.png\" alt=\"nvmath-python\" />\n",
    "\n",
    "# Getting Started with nvmath-python: Kernel Fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714826e",
   "metadata": {},
   "source": [
    "## Exercise: Evaluate performance of CuPy `@`-based vs. `matmul`-based implementations of GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Evaluate performance of CuPy `@`-based vs. `matmul`-based implementations of GEMM\n",
    "\n",
    "import nvmath # noqa: F401 Facilitates shared objects loading required by CuPy (Workaround for CuPy being unable to find nvrtc installed in wheels)\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "# Define GEMM parameters\n",
    "m, n, k = 10_000_000, 40, 10\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "c = cp.random.rand(m, n, dtype=cp.float32)\n",
    "\n",
    "alpha = 1.5\n",
    "beta = 0.5\n",
    "\n",
    "# Benchmarking function\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt\n",
    "\n",
    "# Write two functions that implement GEMM using `@` operator and `matmul` function.\n",
    "def gemm_operator_form(a, b, c, alpha, beta):\n",
    "    return alpha * a @ b + beta * c\n",
    "\n",
    "def gemm_matmul_form(a, b, c, alpha, beta):\n",
    "    return alpha * cp.matmul(a, b) + beta * c\n",
    "\n",
    "# Benchmark the two implementations\n",
    "benchmark(\n",
    "    lambda: gemm_operator_form(a, b, c, alpha, beta),\n",
    "    \"GEMM @ form\",\n",
    "    lambda: gemm_matmul_form(a, b, c, alpha, beta),\n",
    "    \"GEMM matmul form\",\n",
    "    n_repeat=5,\n",
    "    n_warmup=1\n",
    ")\n",
    "\n",
    "# Compute the number of flops for the two implementations\n",
    "def gemm_operator_form_flops(a, b, c):\n",
    "    matmul_flops = a.shape[0] * a.shape[1] * (2 * b.shape[1] - 1)\n",
    "    alpha_a_flops = a.shape[0] * a.shape[1]\n",
    "    beta_c_flops = c.shape[0] * c.shape[1]\n",
    "    add_flops = a.shape[0] * b.shape[1]\n",
    "    return matmul_flops + alpha_a_flops + beta_c_flops + add_flops\n",
    "\n",
    "def gemm_matmul_form_flops(a, b, c):\n",
    "    matmul_flops = a.shape[0] * a.shape[1] * (2 * b.shape[1] - 1)\n",
    "    alpha_x_flops = a.shape[0] * b.shape[1]\n",
    "    beta_c_flops = c.shape[0] * c.shape[1]\n",
    "    add_flops = a.shape[0] * b.shape[1]\n",
    "    return matmul_flops + alpha_x_flops + beta_c_flops + add_flops\n",
    "\n",
    "# Print the number of flops for the two implementations\n",
    "print(f\"GEMM operator form: {gemm_operator_form_flops(a, b, c) * 1e-9:.2f} GFLOPS\")\n",
    "print(f\"GEMM matmul form: {gemm_matmul_form_flops(a, b, c) * 1e-9:.2f} GFLOPS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
