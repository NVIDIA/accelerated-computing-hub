{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d2634",
   "metadata": {},
   "source": [
    "<img src=\"./images/nvmath_head_panel@0.5x.png\" alt=\"nvmath-python\" />\n",
    "\n",
    "# Getting Started with nvmath-python: Kernel Fusion\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a fundamental introduction to **nvmath-python** and explains how it integrates with the existing scientific computing ecosystem in Python. The focus is on understanding kernel fusion and how it enables performance gains in composite operations.\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand how **nvmath-python** coexists with array libraries like [NumPy](https://numpy.org/), [CuPy](https://cupy.dev/), and [PyTorch](https://pytorch.org/)\n",
    "* Apply `nvmath.linalg.advanced.matmul` to perform matrix multiplication operations\n",
    "* Benchmark GPU codes using `cupyx.profiler.benchmark` for accurate performance measurements\n",
    "* Explain the performance benefits of kernel fusion in composite operations\n",
    "* Use NVIDIA Nsight profiling tools to visualize kernel execution\n",
    "* Experiment with different matrix sizes to examine their effect on kernel fusion benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f205c2",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "**nvmath-python** is a powerful library designed to bridge the gap between Python's scientific computing community and [NVIDIA CUDA-X math libraries](https://developer.nvidia.com/gpu-accelerated-libraries). While the community has done excellent work enabling GPU computing through libraries like [CuPy](https://cupy.dev/) and [PyTorch](https://pytorch.org/), these frameworks are often constrained to *[NumPy](https://numpy.org/)-like* APIs and do not always exploit the full potential and advanced features of underlying NVIDIA CUDA-X libraries. This is where **nvmath-python** becomes valuable.\n",
    "\n",
    "**nvmath-python** reimagines how math library API design can be intuitive, Pythonic, and performant in sophisticated usage scenarios. Like other NumPy-like API libraries, **nvmath-python** implements core numerical algorithms useful in many scientific and engineering fields. However, **nvmath-python** does not aim to replace or duplicate them but rather co-exist with them.\n",
    "\n",
    "This notebook focuses on *kernel fusion*, a key optimization technique that allows multiple operations to be combined into a single GPU kernel, dramatically improving performance by reducing memory bandwidth requirements and kernel launch overhead.\n",
    "\n",
    "**Prerequisites:** To use this notebook, you will need:\n",
    "- A computer equipped with an NVIDIA GPU\n",
    "- Basic familiarity with NumPy-like array operations\n",
    "- Understanding of matrix multiplication concepts\n",
    "- Understanding of computational complexity and arithmetic intensity concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b36e2c",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "This notebook requires the following Python libraries:\n",
    "- `nvmath`: NVIDIA's mathematical library for Python\n",
    "- `numpy`: For CPU array operations\n",
    "- `cupy`: For GPU array operations  \n",
    "- `torch` (optional): For PyTorch tensor operations\n",
    "\n",
    "If you have not already installed these libraries, you can install them using:\n",
    "\n",
    "```bash\n",
    "pip install \"nvmath-python[cu12,cpu,dx]\" 'nvidia-cuda-nvcc-cu12==12.8.*' 'nvidia-cuda-nvrtc-cu12==12.8.*' --extra-index-url https://download.pytorch.org/whl/cu128 torch\n",
    "pip install cupy-cuda12x\n",
    "```\n",
    "Note that the above instructions will install nvmath-python with [NVIDIA CUDA Toolkit (CTK)](https://developer.nvidia.com/cuda-toolkit) 12.x. To avoid dependency conflicts CuPy and PyTorch packages must be installed for CTK 12.x, too. It also installs nvmath-python's (optional) CPU backend.\n",
    "\n",
    "For detailed installation instructions, please refer to the [nvmath-python documentation](https://docs.nvidia.com/cuda/nvmath-python/latest/installation.html#install-nvmath-python) as well as the installation instructions for CuPy and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4a6dd",
   "metadata": {},
   "source": [
    "---\n",
    "## nvmath-python is NOT an Array Library\n",
    "\n",
    "First and foremost, **nvmath-python** **is NOT** an *array library*. It does not implement traditional array library functionality such as array *indexing* and *slicing*. The example below demonstrates typical NumPy array operations that are **not** part of **nvmath-python**'s scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43072e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NumPy array creation, indexing and slicing\n",
    "import numpy as np\n",
    "\n",
    "# 1D array\n",
    "a = np.arange(10)  # create array with values from 0 to 9\n",
    "print(\"a =\", a)  # print the array\n",
    "print(\"a[2] =\", a[2])  # access the third element (index 2)\n",
    "print(\"a[2:7:2] =\", a[2:7:2])  # slice from index 2 to 6 with step 2\n",
    "\n",
    "# 2D array\n",
    "b = np.arange(12).reshape(3, 4)  # create 3x4 array (matrix)\n",
    "print(\"b =\", b)  # print the matrix\n",
    "print(\"b[0:2, 1:4] (submatrix) =\", b[0:2, 1:4])  # slice rows 0-1 and columns 1-3\n",
    "print(\"b[:,1] (second column) =\", b[:, 1])  # access all rows in second column\n",
    "print(\"b[0] (first row) =\", b[0])  # access first row\n",
    "print(\"b[1,2] =\", b[1, 2])  # access element at row 1, column 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e96dc8",
   "metadata": {},
   "source": [
    "### Interoperability with Array Libraries\n",
    "\n",
    "Instead, **nvmath-python** is designed to **co-exist with array libraries** such as NumPy, CuPy, and PyTorch. It accepts arrays from these libraries as inputs and returns results as arrays of the same type. The following examples demonstrate this interoperability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nvmath\n",
    "\n",
    "n, m, k = 2, 4, 5\n",
    "a_cpu = np.random.rand(n, k)\n",
    "b_cpu = np.random.rand(k, m)\n",
    "\n",
    "c_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)  # matrix multiplication\n",
    "print(\"c_cpu.shape =\", c_cpu.shape)  # should be (2, 4)\n",
    "print(type(c_cpu))  # should be numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd909ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvmath\n",
    "import cupy as cp\n",
    "\n",
    "n, m, k = 2, 4, 5\n",
    "a_gpu = cp.random.rand(n, k)\n",
    "b_gpu = cp.random.rand(k, m)\n",
    "\n",
    "c_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)  # matrix multiplication\n",
    "print(\"c_gpu.shape =\", c_gpu.shape)  # should be (2, 4)\n",
    "print(type(c_gpu))  # should be cupy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc81dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nvmath\n",
    "\n",
    "n, m, k = 2, 4, 5\n",
    "\n",
    "# CPU tensors\n",
    "a_cpu = torch.rand(n, k)\n",
    "b_cpu = torch.rand(k, m)\n",
    "\n",
    "# On CPU\n",
    "c_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)\n",
    "print(\"c_cpu.shape =\", c_cpu.shape)  # should be (2, 4)\n",
    "print(\"type(c_cpu) =\", type(c_cpu))  # should be torch.Tensor\n",
    "\n",
    "# Run on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    # Move the tensors to GPU\n",
    "    a_gpu = a_cpu.cuda()\n",
    "    b_gpu = b_cpu.cuda()\n",
    "    c_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)\n",
    "    print(\"c_gpu.shape =\", c_gpu.shape)\n",
    "    print(\"c_gpu device =\", getattr(c_gpu, \"device\", \"unknown\"))\n",
    "    print(\"type(c_gpu) =\", type(c_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927021c8",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **nvmath-python** accepts arrays from multiple libraries: NumPy (CPU), CuPy (GPU), and PyTorch (CPU/GPU).\n",
    "- To determine where a result resides, use simple type/device checks such as `isinstance(c, np.ndarray)`, `isinstance(c, cp.ndarray)`, or for PyTorch inspect `c.is_cuda` / `c.device`.\n",
    "- **Remember**: Array semantics (indexing/slicing) are provided by the array library (NumPy/CuPy/PyTorch). **nvmath-python** focuses on mathematical operations and instead interoperates with those libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b0df6",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmarking GPU Codes with `cupyx.profiler.benchmark`\n",
    "\n",
    "Since GPU kernels are launched asynchronously, a host call may return before the device work finishes. Naive timing methods (such as Python's `time.time()` method or Jupyter's `%%timeit` magic) measure only host-side overhead, not true device execution time. \n",
    "\n",
    "The `cupyx.profiler.benchmark` function uses CUDA events, proper synchronization, warm-ups, and repeated runs to produce stable, device-level timing measurements. It removes much of the noise introduced by Python overhead, one-time setup costs, takes into account the asynchronous execution, and reports aggregated statistics so you get reproducible, comparable numbers.\n",
    "\n",
    "Below is a helper function that benchmarks implementations and optionally compares them to report speedup/slowdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5762079",
   "metadata": {},
   "source": [
    "### Benchmarking Matrix Multiplication\n",
    "\n",
    "Now let's perform real benchmarking to compare how **nvmath-python** performs on `matmul` relative to CuPy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174e1fd",
   "metadata": {},
   "source": [
    "**Practical Benchmarking Notes:**\n",
    "\n",
    "- Make sure data is already allocated on the device before benchmarking (transfer costs should be excluded unless you intend to measure them).\n",
    "- Run several repeats and inspect distributions (median is usually more robust than min or mean).\n",
    "- For end-to-end profiling (memory transfers, kernel launches, kernel internals), use NVIDIA tools like `nsys`, `nvprof`, or Nsight Systems. The `cupyx.profiler.benchmark` function focuses on accurate kernel timing within Python workflows.\n",
    "- Watch out for GPU power/clock state and thermal throttling; for stable numbers, use consistent GPU governor/clock settings if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k = 8192, 4096, 2048  # Use large enough sizes to get measurable times. Make sure it fits into GPU memory.\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "\n",
    "# It's time to do real benchmarking to compare how nvmath-python\n",
    "# performs on `matmul` relative to `cupy`:\n",
    "benchmark(\n",
    "    lambda: nvmath.linalg.advanced.matmul(a, b),  # nvmath-python implementation\n",
    "    F_name=\"nvmath-python matmul\",\n",
    "    F_alternative=lambda: cp.matmul(a, b),  # CuPy implementation\n",
    "    F_alternative_name=\"CuPy matmul\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9a799",
   "metadata": {},
   "source": [
    "### Understanding the Performance Results\n",
    "\n",
    "Is there anything wrong with **nvmath-python**? Why does the \"advanced\" `matmul` run as good (or as poorly) as CuPy's `matmul`?\n",
    "\n",
    "The explanation is actually very simple: both CuPy and **nvmath-python** rely on the same cuBLAS library, and both perform nothing more than pure matrix-matrix multiplication. This is the reason (and the only reason) why we observe **identical** performance. To make a difference, we need a more **sophisticated use case** that does not simply translate to a NumPy-like `matmul` call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77ce58",
   "metadata": {},
   "source": [
    "---\n",
    "## Composite Operations with nvmath-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a1ebf",
   "metadata": {},
   "source": [
    "In many real-world scenarios, the `matmul` operation we considered earlier is chained with other operations. For example, in linear algebra, *Generalized Matrix Multiplication* (GEMM) is a commonly used building block in scientific and engineering applications. In its simplified form, GEMM performs the following operation:\n",
    "\n",
    "$$\n",
    "D_{m \\times n} \\leftarrow \\alpha \\cdot (A_{m \\times k} \\cdot B_{k \\times n}) + \\beta \\cdot C_{m \\times n}\n",
    "$$\n",
    "\n",
    "Using an array library, one can easily implement this chained operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n, k = 10_000_000, 40, 10  # Take tall-and-skinny matrices to illustrate kernel fusion benefits\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "c = cp.random.rand(m, n, dtype=cp.float32)\n",
    "\n",
    "alpha = 1.5\n",
    "beta = 0.5\n",
    "\n",
    "d = alpha * a @ b + beta * c\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b6cc3",
   "metadata": {},
   "source": [
    "With **nvmath-python**, you can also perform the composite operation with a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nvmath.linalg.advanced.matmul(a, b, c, alpha=alpha, beta=beta)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973c738",
   "metadata": {},
   "source": [
    "Now let's benchmark each alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\n",
    "    lambda: nvmath.linalg.advanced.matmul(a, b, c, alpha=alpha, beta=beta),  # nvmath-python implementation\n",
    "    F_name=\"nvmath-python GEMM\",\n",
    "    F_alternative=lambda: alpha * a @ b + beta * c,  # CuPy implementation\n",
    "    F_alternative_name=\"CuPy equivalent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4059f2",
   "metadata": {},
   "source": [
    "### Why is nvmath-python Significantly Faster?\n",
    "\n",
    "Both **nvmath-python** and CuPy perform the same composite operation, and both are accelerated by the cuBLAS library. So why is **nvmath-python** significantly faster?\n",
    "\n",
    "The \"magic\" behind this performance difference is called *kernel fusion*. In the case of CuPy, every basic operation is a separate function call on the host, which initiates a respective GPU kernel invocation asynchronously, returns to the host, submits the next kernel for execution, and so on. In the case of **nvmath-python**, the chained operation is performed as a whole in a single fused kernel. There are no accompanying overheads of multiple kernel invocations. More importantly, execution as a fused kernel allows optimization of memory accesses, which significantly increases the *arithmetic intensity* of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0454e0e",
   "metadata": {},
   "source": [
    "## Exercise: Evaluate performance of CuPy `@`-based vs. `matmul`-based implementations of GEMM\n",
    "\n",
    "In the above examples we implemented GEMM using `@` operator notation for matrix multiplication. Implement CuPy variant using `matmul` function. Benchmark `@` variant vs. `matmul` variant and explain performance difference (if any).\n",
    "\n",
    "**Hint**: Consider operation precedence along with computational costs of each chained operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8aa229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Evaluate performance of CuPy `@`-based vs. `matmul`-based implementations of GEMM\n",
    "\n",
    "import nvmath # Facilitates shared objects loading required by CuPy (Workaround for CuPy being unable to find nvrtc installed in wheels)\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "# Define GEMM parameters\n",
    "m, n, k = 10_000_000, 40, 10\n",
    "\n",
    "a = cp.random.rand(m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(k, n, dtype=cp.float32)\n",
    "c = cp.random.rand(m, n, dtype=cp.float32)\n",
    "\n",
    "alpha = 1.5\n",
    "beta = 0.5\n",
    "\n",
    "# Benchmarking function\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    pass # TODO: Implement this function\n",
    "\n",
    "# Write two functions that implement GEMM using `@` operator and `matmul` function.\n",
    "def gemm_operator_form(a, b, c, alpha, beta):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def gemm_matmul_form(a, b, c, alpha, beta):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Benchmark the two implementations\n",
    "benchmark(\n",
    "    # TODO: Pass the two implementations to the benchmark function\n",
    ")\n",
    "\n",
    "# Compute the number of flops for the two implementations\n",
    "def gemm_operator_form_flops(a, b, c):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def gemm_matmul_form_flops(a, b, c):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Print the number of flops for the two implementations\n",
    "print(f\"GEMM operator form: {gemm_operator_form_flops(a, b, c) * 1e-9:.2f} GFLOPS\")\n",
    "print(f\"GEMM matmul form: {gemm_matmul_form_flops(a, b, c) * 1e-9:.2f} GFLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a09170-9cfd-464e-9904-0aef36ad1002",
   "metadata": {},
   "source": [
    "---\n",
    "## NVIDIA Nsight Plugin for JupyterLab\n",
    "\n",
    "NVIDIA has created a useful JupyterLab plugin for the NVIDIA Nsight Tools, allowing performance profiling from within notebooks. The following command will install the plugin into your environment:\n",
    "\n",
    "```bash\n",
    "pip install jupyterlab-nvidia-nsight\n",
    "```\n",
    "\n",
    "After installation, you will see a new tab **NVIDIA Nsight** in JupyterLab's menu. In the menu, select **Profiling with Nsight Systems...**. This will restart the JupyterLab kernel. Select the cells you wish to profile and execute **Run and profile selected cells...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef748d-aa10-41be-a106-bac30acdd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile this cell with Nsight Systems...\n",
    "d = alpha * a @ b + beta * c\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb88ca-12ac-4bf3-9b9a-48119bbe89e4",
   "metadata": {},
   "source": [
    "After running the profiler and opening the generated report, you should see something like this:\n",
    "\n",
    "<img src=\"./images/nsys-report-cupy.png\" alt=\"Nsight Systems report showing CuPy GEMM with multiple kernels\" width=\"75%\"/>\n",
    "\n",
    "Note the number of kernels and their execution times. You should see that the CuPy multiply kernel takes the longest and consumes more time than the SGEMM kernel.\n",
    "\n",
    "Let us profile **nvmath-python**'s implementation now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile this cell with Nsight Systems...\n",
    "d = nvmath.linalg.advanced.matmul(a, b, c, alpha=alpha, beta=beta)\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d13450b",
   "metadata": {},
   "source": [
    "By opening the **Nsight Systems** UI, you should see something like this:\n",
    "\n",
    "<img src=\"./images/nsys-report-nvmath.png\" alt=\"Nsight Systems report showing nvmath-python GEMM with a single fused kernel\" width=\"75%\"/>\n",
    "\n",
    "Note that the only kernel in the timeline is SGEMM. Everything else has been fused into that single kernel. This is the true reason why **nvmath-python**'s performance is much better in a composite operation like GEMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1edb4b",
   "metadata": {},
   "source": [
    "---\n",
    "## GEMM with Fused Epilogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d60caf",
   "metadata": {},
   "source": [
    "The library goes one step further and allows fusion of the GEMM operation with an epilog function \\( f(x) \\), which is applied element-wise to the result of the GEMM operation. Specifically:\n",
    "\n",
    "$$\n",
    "D_{m \\times n} \\leftarrow f(\\alpha \\cdot (A_{m \\times k} \\cdot B_{k \\times n}) + \\beta \\cdot C_{m \\times n})\n",
    "$$\n",
    "\n",
    "For a deeper dive into GEMM epilogs and other advanced techniques of matrix-matrix multiplication in **nvmath-python**, please refer to the collection of notebooks in the `notebooks/matmul/` GitHub directory:\n",
    "* [01_introduction.ipynb](https://github.com/NVIDIA/nvmath-python/blob/main/notebooks/matmul/01_introduction.ipynb)\n",
    "* [02_epilogs.ipynb](https://github.com/NVIDIA/nvmath-python/blob/main/notebooks/matmul/02_epilogs.ipynb)\n",
    "* [03_backpropagation.ipynb](https://github.com/NVIDIA/nvmath-python/blob/main/notebooks/matmul/03_backpropagation.ipynb)\n",
    "* [04_fp8.ipynb](https://github.com/NVIDIA/nvmath-python/blob/main/notebooks/matmul/04_fp8.ipynb)\n",
    "\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the fundamentals of **nvmath-python** and its key feature: *kernel fusion*. We demonstrated how **nvmath-python** integrates seamlessly with array libraries like NumPy, CuPy, and PyTorch, accepting their array types as inputs and returning results in the same format.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **nvmath-python** is not an array library but rather a mathematical operations library that coexists with existing array libraries.\n",
    "- Kernel fusion combines multiple operations into a single GPU kernel, dramatically improving performance by reducing memory bandwidth requirements and kernel launch overhead.\n",
    "- For simple operations like matrix multiplication, **nvmath-python** and CuPy perform similarly since both use cuBLAS. However, for composite operations like GEMM, **nvmath-python** shows significant speedups through kernel fusion.\n",
    "- Proper benchmarking using `cupyx.profiler.benchmark` is essential for accurate performance measurements on GPUs.\n",
    "- NVIDIA Nsight profiling tools provide deep insights into kernel execution and help visualize the benefits of kernel fusion.\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore memory and execution spaces in the next notebook: [02_mem_exec_spaces.ipynb](02_mem_exec_spaces.ipynb)\n",
    "- Learn about stateful APIs and autotuning: [03_stateful_api.ipynb](03_stateful_api.ipynb)\n",
    "- Discover FFT callbacks: [04_callbacks.ipynb](04_callbacks.ipynb)\n",
    "- Explore device APIs: [05_device_api.ipynb](05_device_api.ipynb)\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "- NVIDIA nvmath-python documentation, \"Installation Guide\", https://docs.nvidia.com/cuda/nvmath-python/latest/installation.html. Accessed: October 23, 2025.\n",
    "- NVIDIA, \"cuBLAS Library User Guide\", https://docs.nvidia.com/cuda/cublas/. Accessed: October 23, 2025.\n",
    "- NVIDIA, \"Nsight Systems\", https://developer.nvidia.com/nsight-systems. Accessed: October 23, 2025.\n",
    "- Harris, Charles R., et al., \"Array programming with NumPy\", Nature, 585(7825), 357-362, 2020.\n",
    "- Williams, Samuel, et al., \"Roofline: An Insightful Visual Performance Model for Multicore Architectures\", Communications of the ACM, 52(4), 65-76, 2009.\n",
    "- Cormen, Thomas H., et al., \"Introduction to Algorithms\", 3rd Edition, MIT Press, 2009.\n",
    "- Asanovic, Krste, et al., \"The Landscape of Parallel Computing Research: A View from Berkeley\", Technical Report UCB/EECS-2006-183, UC Berkeley, 2006."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
