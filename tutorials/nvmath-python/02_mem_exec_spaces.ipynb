{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ab10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89dd057",
   "metadata": {},
   "source": [
    "<img src=\"./images/nvmath_head_panel@0.5x.png\" alt=\"nvmath-python\" />\n",
    "\n",
    "# Getting Started with nvmath-python: Memory and Execution Spaces\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores the concepts of *memory spaces* and *execution spaces* in **nvmath-python**. Understanding these concepts is crucial for optimizing performance and avoiding unnecessary data transfer overhead between CPU and GPU.\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand the difference between memory space and execution space\n",
    "* Identify when **nvmath-python** performs automatic data transfers between memory spaces\n",
    "* Use **nvmath-python**'s logging mechanism to diagnose implicit data transfers\n",
    "* Benchmark the performance impact of data residing in different memory spaces\n",
    "* Apply memory space concepts to generic APIs like FFT\n",
    "* Estimate and minimize CPU-GPU data transfer overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcffa0",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "In heterogeneous computing environments with both CPUs and GPUs, understanding where data resides and where computation occurs is essential for achieving optimal performance. The *memory space* refers to where data is stored (CPU RAM or GPU memory), while the *execution space* refers to where computations are performed (CPU or GPU). These two spaces are not necessarily the same, and mismatches between them can lead to implicit data transfers that significantly impact performance.\n",
    "\n",
    "**nvmath-python** is backed by both GPU libraries (such as [cuBLAS](https://developer.nvidia.com/cublas) and [cuFFT](https://developer.nvidia.com/cufft)) and CPU libraries (NVPL for NVIDIA Grace CPUs and Arm v8 and Intel MKL for x86 hosts). This allows easy code migration between CPU and GPU as well as implementation of complex hybrid workflows that combine both CPU and GPU execution.\n",
    "\n",
    "This notebook demonstrates how **nvmath-python** handles different memory and execution space configurations, and provides tools to diagnose and optimize data transfer patterns.\n",
    "\n",
    "**Prerequisites:** To use this notebook, you will need:\n",
    "- A computer equipped with an NVIDIA GPU\n",
    "- An environment with properly installed Python libraries as described in [01_kernel_fusion.ipynb](./01_kernel_fusion.ipynb)\n",
    "- Completion of the previous notebook on kernel fusion (recommended)\n",
    "- Understanding of basic GPU computing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dd080",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "This notebook requires the same libraries as the previous notebook [01_kernel_fusion.ipynb](./01_kernel_fusion.ipynb). If you already completed it, no further installation is required for this notebook.\n",
    "\n",
    "For detailed installation instructions, please refer to the [nvmath-python documentation](https://docs.nvidia.com/cuda/nvmath-python/latest/installation.html#install-nvmath-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc193e7a",
   "metadata": {},
   "source": [
    "---\n",
    "## Memory and Execution Spaces\n",
    "\n",
    "The *memory space* is the memory dedicated for storing input data and results. It is tied to a specific device (or host) and is allocated and released by means of the respective device/host API calls. The *execution space* is where the data is actually processed. \n",
    "\n",
    "**Memory and execution spaces are not necessarily the same.** This is important to remember because data transfer between memory spaces often incurs non-negligible costs. These costs may be high not only in the case of data movement between a host CPU and a GPU device, but also between two GPU devices.\n",
    "\n",
    "Let's examine an example to understand how **nvmath-python** handles different memory and execution space combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import nvmath\n",
    "\n",
    "m, n, k = 8000, 2000, 4000\n",
    "a_cpu = np.random.randn(m, k).astype(np.float32)\n",
    "b_cpu = np.random.randn(k, n).astype(np.float32)\n",
    "\n",
    "a_gpu = cp.random.randn(m, k, dtype=cp.float32)\n",
    "b_gpu = cp.random.randn(k, n, dtype=cp.float32)\n",
    "\n",
    "d_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)\n",
    "d_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)\n",
    "type(d_cpu)  # numpy.ndarray\n",
    "type(d_gpu)  # cupy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a304266",
   "metadata": {},
   "source": [
    "### Benchmarking Helper Function\n",
    "\n",
    "We will use the same benchmarking helper function from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d45e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efc198",
   "metadata": {},
   "source": [
    "### Performance Comparison: GPU vs. CPU Memory Spaces\n",
    "\n",
    "Now let's benchmark the performance difference when inputs are in GPU memory versus CPU memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ce61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\n",
    "    lambda: nvmath.linalg.advanced.matmul(a_gpu, b_gpu),\n",
    "    F_name=\"Matmul with GPU inputs\",\n",
    "    F_alternative=lambda: nvmath.linalg.advanced.matmul(a_cpu, b_cpu),\n",
    "    F_alternative_name=\"Matmul with CPU inputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b886ac",
   "metadata": {},
   "source": [
    "### Understanding the Performance Difference\n",
    "\n",
    "The difference is noticeable, but where does the cost come from? The answer lies in understanding *specialized APIs* versus *generic APIs*.\n",
    "\n",
    "`nvmath.linalg.advanced.matmul` belongs to a category of *specialized APIs*. In contrast to *generic APIs* such as `nvmath.fft.fft`, specialized APIs serve very specific needs, which comes at a cost of generality. Specifically, `nvmath.linalg.advanced.matmul` supports **GPU execution space only**. \n",
    "\n",
    "When `nvmath.linalg.advanced.matmul` receives CPU tensor inputs, it:\n",
    "1. Copies the inputs from CPU memory to GPU memory (the execution space)\n",
    "2. Performs the operation on the GPU\n",
    "3. Copies the result back from GPU memory to CPU memory (the original memory space)\n",
    "\n",
    "The next example illustrates what is happening under the hood of **nvmath-python** through the library's logging mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure root logger to show info messages from nvmath and its internals\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\", force=True)\n",
    "logging.disable(logging.NOTSET)  # ensure logging is enabled\n",
    "\n",
    "# Run matmul with GPU inputs (execution space == GPU)\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* GPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "d_gpu = nvmath.linalg.advanced.matmul(a_gpu, b_gpu)\n",
    "print(\"d_gpu type:\", type(d_gpu))\n",
    "\n",
    "# Run matmul with CPU inputs (this will cause nvmath to copy to execution space internally)\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* CPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "d_cpu = nvmath.linalg.advanced.matmul(a_cpu, b_cpu)\n",
    "print(\"d_cpu type:\", type(d_cpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb42f12",
   "metadata": {},
   "source": [
    "### Interpreting the Logging Output\n",
    "\n",
    "In the case of GPU inputs, the `= SPECIFICATION PHASE =` section reports:\n",
    "\n",
    "```\n",
    "The input operands' memory space is cuda, and the execution space is on device 0.\n",
    "```\n",
    "\n",
    "While in the case of CPU inputs, the report is different:\n",
    "\n",
    "```\n",
    "The input operands' memory space is cpu, and the execution space is on device 0.\n",
    "```\n",
    "\n",
    "This mismatch between memory space (CPU) and execution space (GPU) causes the implicit data transfers we observed. Such significant overhead cannot be ignored. The **nvmath-python** logging mechanism is a great tool to understand potential costs and refactor the code accordingly to minimize the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c145f",
   "metadata": {},
   "source": [
    "## Exercise: Estimate CPU-GPU Data Transfer Overhead\n",
    "\n",
    "We see a non-negligible performance difference between data residing in CPU memory space versus GPU memory space in the above example. Given that the execution space is always GPU, estimate the data transfer cost. Implement a dedicated benchmark for a cross-check.\n",
    "\n",
    "**Hint**: You can use CuPy to explicitly time the data transfer between CPU and GPU using `cp.asarray()` and `cp.asnumpy()` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Evaluate performance of CuPy `@`-based vs. `matmul`-based implementations of GEMM\n",
    "\n",
    "import nvmath\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "# Define matmul parameters\n",
    "m, n, k = 8000, 2000, 4000\n",
    "a_cpu = np.random.randn(m, k).astype(np.float32)\n",
    "b_cpu = np.random.randn(k, n).astype(np.float32)\n",
    "\n",
    "a_gpu = cp.random.randn(m, k, dtype=cp.float32)\n",
    "b_gpu = cp.random.randn(k, n, dtype=cp.float32)\n",
    "\n",
    "# Benchmark 1: Measure time for CPU -> GPU -> matmul -> CPU\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Benchmark 2: Measure time for GPU -> matmul -> GPU\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Based on above results print estimated time for CPU -> GPU -> CPU transfer\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Benchmark 3: Measure time for CPU -> GPU transfer\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Benchmark 4: Measure time for GPU -> CPU transfer\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Based on above results print estimated time for CPU -> GPU -> CPU transfer\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Benchmark 5: Measure time for CPU -> GPU transfer\n",
    "# TODO: Implement this benchmark and print the result\n",
    "\n",
    "# Perform cross-checks for different ways to estimate the transfer costs.\n",
    "# Are the results consistent?\n",
    "# Note how much time is spent in data transfers vs. actual computation.\n",
    "# Can data transfer overheads be ignored?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fec27",
   "metadata": {},
   "source": [
    "---\n",
    "## Generic APIs: Fast Fourier Transform (FFT)\n",
    "\n",
    "Next, let us illustrate the data flow in the case of the library's *Fast Fourier Transform* (FFT), which is a *generic API*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "e_cpu = (np.random.randn(N) + 1j * np.random.randn(N)).astype(np.complex64)\n",
    "e_gpu = cp.array(e_cpu)  # move NumPy data to GPU as CuPy array (complex64)\n",
    "\n",
    "# compute FFT with nvmath (for CPU inputs nvmath may copy to execution space internally)\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* CPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "r_cpu = nvmath.fft.fft(e_cpu)\n",
    "print(\"r_cpu type:\", type(r_cpu), getattr(r_cpu, \"dtype\", None), getattr(r_cpu, \"shape\", None))\n",
    "\n",
    "logging.info(\"******************************************************\")\n",
    "logging.info(\"********************* GPU INPUTS *********************\")\n",
    "logging.info(\"******************************************************\")\n",
    "r_gpu = nvmath.fft.fft(e_gpu)\n",
    "print(\"r_gpu type:\", type(r_gpu), getattr(r_gpu, \"dtype\", None), getattr(r_gpu, \"shape\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185f550",
   "metadata": {},
   "source": [
    "### Generic APIs Adapt to Input Location\n",
    "\n",
    "Note that when the input operand is a CPU operand, the library chooses the execution space to be CPU, thanks to the fact that FFT belongs to *generic APIs* providing consistent behavior between CPU and GPU. In the case of GPU inputs, the library selects GPU as the execution space.\n",
    "\n",
    "This is a key difference from specialized APIs: **generic APIs automatically match the execution space to the memory space of the inputs**, avoiding unnecessary data transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c0f5",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Memory space (where data is stored) and execution space (where computation happens) may differ, leading to data transfer costs.\n",
    "- Some specialized APIs, such as `nvmath.linalg.advanced.matmul`, only support GPU execution, automatically transferring CPU data to GPU with associated overhead.\n",
    "- Generic APIs like `nvmath.fft.fft` adapt to input location: CPU inputs execute on CPU, GPU inputs execute on GPU.\n",
    "- Use **nvmath-python**'s logging mechanism to understand internal operations and identify potential bottlenecks.\n",
    "\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the critical concepts of memory spaces and execution spaces in **nvmath-python**. Understanding the distinction between where data is stored and where it is processed is essential for optimizing heterogeneous computing workflows.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Memory space and execution space are distinct concepts that may not always align, potentially causing implicit data transfers.\n",
    "- Specialized APIs (like `nvmath.linalg.advanced.matmul`) only support GPU execution, requiring data transfers when CPU inputs are provided.\n",
    "- Generic APIs (like `nvmath.fft.fft`) adapt their execution space to match the input memory space, avoiding unnecessary transfers.\n",
    "- The **nvmath-python** logging mechanism provides valuable insights into internal operations and helps identify performance bottlenecks.\n",
    "- Data transfer between CPU and GPU can significantly impact performance and should be minimized in production code.\n",
    "\n",
    "**Next Steps:**\n",
    "- Learn about stateful APIs and autotuning in the next notebook: [03_stateful_api.ipynb](03_stateful_api.ipynb)\n",
    "- Explore FFT callbacks: [04_callbacks.ipynb](04_callbacks.ipynb)\n",
    "- Discover device APIs: [05_device_api.ipynb](05_device_api.ipynb)\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "- NVIDIA nvmath-python documentation, \"API Reference,\" https://docs.nvidia.com/cuda/nvmath-python/, Accessed: October 23, 2025.\n",
    "- NVIDIA, \"cuBLAS Library User Guide,\" https://docs.nvidia.com/cuda/cublas/, Accessed: October 23, 2025.\n",
    "- NVIDIA, \"cuFFT Library User Guide,\" https://docs.nvidia.com/cuda/cufft/, Accessed: October 23, 2025.\n",
    "- NVIDIA, \"CUDA C++ Programming Guide - Heterogeneous Programming,\" https://docs.nvidia.com/cuda/cuda-c-programming-guide/, Accessed: October 23, 2025.\n",
    "\n",
    "- Williams, Samuel, et al., \"Roofline: An Insightful Visual Performance Model for Multicore Architectures,\" Communications of the ACM, 52(4), 65-76, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
