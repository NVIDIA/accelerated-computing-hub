{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb65df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ede2c",
   "metadata": {},
   "source": [
    "<img src=\"./images/nvmath_head_panel@0.5x.png\" alt=\"nvmath-python\" />\n",
    "\n",
    "# Getting Started with nvmath-python: Stateful APIs and Autotuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0a67d",
   "metadata": {},
   "source": [
    "## Exercise: Batch Dimension vs. Batch Sequence\n",
    "\n",
    "In the above example, we implemented batching as a sequence of matrices being processed one by one in a loop. This is a common technique for streaming data or when the entire batch does not fit into GPU memory. An alternative approach is to add a dedicated batching dimension and operate with the batch as a single tensor. The **nvmath-python** library supports both use cases.\n",
    "\n",
    "Implement a batching dimension approach and compare performance to the batch sequence approach. Explain the performance difference (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Batch Dimension vs. Batch Sequence\n",
    "\n",
    "import nvmath\n",
    "from nvmath.linalg.advanced import MatmulEpilog\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import cupyx as cpx\n",
    "\n",
    "\n",
    "# Helper function to benchmark two implementations F and (optionally) F_alternative\n",
    "# When F_alternative is provided, in addition to raw performance numbers (seconds)\n",
    "# speedup of F relative to F_alternative is reported\n",
    "def benchmark(\n",
    "    F, F_name=\"Implementation\", F_alternative=None, F_alternative_name=\"Alternative implementation\", n_repeat=10, n_warmup=1\n",
    "):\n",
    "    timing = cpx.profiler.benchmark(F, n_repeat=n_repeat, n_warmup=n_warmup)  # warm-up + repeated runs\n",
    "    perf = np.min(timing.gpu_times)  # best time from repeated runs\n",
    "    print(f\"{F_name} performance = {perf:0.4f} sec\")\n",
    "\n",
    "    if F_alternative is not None:\n",
    "        timing_alt = cpx.profiler.benchmark(F_alternative, n_repeat=n_repeat, n_warmup=n_warmup)\n",
    "        perf_alt = np.min(timing_alt.gpu_times)\n",
    "        print(f\"{F_alternative_name} performance = {perf_alt:0.4f} sec\")\n",
    "        print(f\"Speedup = {perf_alt / perf:0.4f}x\")\n",
    "    else:\n",
    "        perf_alt = None\n",
    "\n",
    "    return perf, perf_alt\n",
    "\n",
    "\n",
    "m, n, k, batch_size = 64, 128, 256, 512 # Select larger sizes for benchmarking purposes\n",
    "\n",
    "a = cp.random.rand(batch_size, m, k, dtype=cp.float32)\n",
    "b = cp.random.rand(batch_size, k, n, dtype=cp.float32)\n",
    "d = cp.empty((batch_size, m, n), dtype=cp.float32)\n",
    "bias = cp.random.rand(batch_size, m, 1, dtype=cp.float32)\n",
    "\n",
    "\n",
    "def matmul_batched_stateless(a, b, bias):\n",
    "    global d # Use pre-allocated array to save memory\n",
    "    d[:] = nvmath.linalg.advanced.matmul( # Batch is inferred from the shape of the operands.\n",
    "        a, b, epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias}\n",
    "    )\n",
    "\n",
    "\n",
    "def matmul_batched_stateful_sequence(a, b, bias):\n",
    "    with nvmath.linalg.advanced.Matmul(a[0], b[0]) as mm: # We create a Matmul object for the first batch element.\n",
    "        mm.plan(epilog=MatmulEpilog(MatmulEpilog.RELU_BIAS), epilog_inputs={\"bias\": bias[0]}) # We assume a[0] and b[0] are representative of the batch.\n",
    "        mm.execute() # The execution doesn't require resetting the operands.\n",
    "        for i in range(1, batch_size):\n",
    "            mm.reset_operands(a=a[i], b=b[i], epilog_inputs={\"bias\": bias[i]}) # Subsequent executions require resetting the operands.\n",
    "            d[i] = mm.execute() # We execute with the new operands.\n",
    "\n",
    "\n",
    "benchmark(\n",
    "    lambda: matmul_batched_stateless(a, b, bias),\n",
    "    \"Stateless batched dimension approach\",\n",
    "    lambda: matmul_batched_stateful_sequence(a, b, bias),\n",
    "    \"Stateful with a batch sequence approach\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
