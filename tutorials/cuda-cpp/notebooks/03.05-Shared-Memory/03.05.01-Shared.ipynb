{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [Cache Memory](#Cache-Memory)\n",
    "* [Exercise: Optimize Histogram](03.05.02-Exercise-Optimize-Histogram.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With our previous optimizations, the kernel now performs significantly better. \n",
    "However, some inefficiencies remain. \n",
    "Currently, each block’s histogram is stored in global GPU memory, even though it’s never used outside the kernel. \n",
    "This approach not only consumes unnecessary bandwidth but also increases the overall memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Memory\n",
    "\n",
    "![L2](Images/L2.png \"L2\")\n",
    "\n",
    "As shown in the figure above, there’s a much closer memory resource: each Streaming Multiprocessor (SM) has its own L1 cache. \n",
    "Ideally, we want to store each block’s histogram right there in L1. \n",
    "Fortunately, CUDA makes this possible through software-controlled shared memory. \n",
    "By allocating the block histogram in shared memory, we can take full advantage of the SM’s L1 cache and reduce unnecessary memory traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/03.05-Shared-Memory/Sources/ach.cuh -nv -O Sources/ach.cuh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/simple-shmem.cpp\n",
    "#include <cstdio>\n",
    "\n",
    "__global__ void kernel()\n",
    "{\n",
    "  __shared__ int shared[4];\n",
    "  shared[threadIdx.x] = threadIdx.x;\n",
    "  __syncthreads();\n",
    "\n",
    "  if (threadIdx.x == 0)\n",
    "  {\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "      std::printf(\"shared[%d] = %d\\n\", i, shared[i]);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  kernel<<<1, 4>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o /tmp/a.out Sources/simple-shmem.cpp -x cu -arch=native && /tmp/a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allocate shared memory, simply annotate a variable with the `__shared__` keyword.\n",
    "This puts the variable into shared memory that coresides with the L1 cache.\n",
    "Since shared memory isn't automatically initialized, \n",
    "we begin our kernel by having each thread write its own index into a corresponding shared memory location:\n",
    "\n",
    "```c++\n",
    "shared[threadIdx.x] = threadIdx.x;\n",
    "__syncthreads();\n",
    "```\n",
    "\n",
    "The `__syncthreads()` call ensures that all threads have finished writing to the shared array before any thread reads from it. \n",
    "Afterwards, the first thread prints out the contents of the shared memory:\n",
    "\n",
    "![Shared Memory](Images/simply-shared.png \"Shared Memory\")\n",
    "\n",
    "As you can see, each thread successfully stored its index in the shared array, and the first thread can read back those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now lets go ahead and try an [exercise](03.05.02-Exercise-Optimize-Histogram.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
