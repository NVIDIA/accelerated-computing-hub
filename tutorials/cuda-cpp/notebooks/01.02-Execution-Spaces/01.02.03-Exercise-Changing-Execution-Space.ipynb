{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Changing Execution Space\n",
    "\n",
    "In this exercise, you have to modify the thrust algorithm which is currently running on the CPU and change it to execute on the GPU instead. \n",
    "After your changes, program should print:\n",
    "\n",
    "```\n",
    "printing 1 on GPU\n",
    "printing 2 on GPU\n",
    "printing 3 on GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/01.02-Execution-Spaces/Sources/ach.h -nv -O Sources/ach.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/port-thrust-to-gpu.cpp\n",
    "#include \"ach.h\"\n",
    "\n",
    "int main() {\n",
    "    thrust::universal_vector<int> vec{ 1, 2, 3 };\n",
    "    // TODO: Make the code below execute on the GPU\n",
    "    thrust::for_each(thrust::host, vec.begin(), vec.end(), []__host__(int val) {\n",
    "        std::printf(\"printing %d on %s\\n\", val, ach::execution_space());\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o /tmp/a.out --extended-lambda Sources/port-thrust-to-gpu.cpp -x cu -arch=native # build executable\n",
    "!/tmp/a.out # run executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re unsure how to proceed, consider expanding this section for guidance. Use the hint only after giving the problem a genuine attempt.\n",
    "\n",
    "<details>\n",
    "  <summary>Hints</summary>\n",
    "  \n",
    "  - consult [execution policy](01.02.01-Execution-Spaces.ipynb#Execution-Policy) section\n",
    "  - make sure your lambda is compiled for *device*\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this section only after you’ve made a serious attempt at solving the problem. Once you’ve completed your solution, compare it with the reference provided here to evaluate your approach and identify any potential improvements.\n",
    "\n",
    "<details>\n",
    "  <summary>Solution</summary>\n",
    "\n",
    "  Key points:\n",
    "  - change execution policy from `thrust::host` to `thrust::device`\n",
    "  - change execution space specifier from `__host__` to `__device__`\n",
    "\n",
    "  Solution:\n",
    "  ```c++\n",
    "  thrust::for_each(thrust::device, vec.begin(), vec.end(), [] __device__(int val) {\n",
    "      std::printf(\"printing %d on %s\\n\", val, ach::execution_space());\n",
    "  });\n",
    "  ```\n",
    "\n",
    "  You can find full solution [here](Solutions/port-thrust-to-gpu.cu).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Congratulations! Now you are familiar with the concept of execution policies.\n",
    "Proceed to the [next exercise](01.02.04-Exercise-Compute-Median-Temperature.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
