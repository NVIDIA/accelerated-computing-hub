{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [Memory Contention](#Memory-Contention)\n",
    "* [Private Histogram](#Private-Histogram)\n",
    "* [Add Synchronization](#Add-Synchronization)\n",
    "* [Exercise: Fix Data Race](03.04.02-Exercise-Histogram.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Memory Contention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fix from the previous exercise, our histogram kernel finally produces correct results, but performance remains suboptimal. \n",
    "Why? Because using a single shared histogram forces millions of atomic operations on the same memory location. \n",
    "This causes significant contention and implicit serialization.\n",
    "\n",
    "<img src=\"Images/contention.png\" alt=\"Contention\" width=600>\n",
    "\n",
    "In the worst case, all threads map their data to a single bin. \n",
    "With around 16 thousand blocks and 256 threads each, that’s roughly 4 million atomic operations contending for the same location.  So while we have launched a few million threads, the atomic operation serializes the write to the `histogram` span, and in effect our parallel code now runs partly in serial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private Histogram\n",
    "To reduce this overhead, we can introduce a \"private\" histogram for each thread block. \n",
    "Each block would accumulate its own local copy of histogram, then merge it into the global histogram after all local updates are complete.\n",
    "\n",
    "<img src=\"Images/private.png\" alt=\"Private\" width=800>\n",
    "\n",
    "Now, in the worst case, up to 256 atomic operations occur within a block’s private histogram, plus about 16k merges (one per block). \n",
    "That’s 256 + 16k total atomic operations, a big improvement over 4 million.\n",
    "\n",
    "Let’s see how to implement this optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/03.04-Synchronization/Sources/ach.cuh -nv -O Sources/ach.cuh\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/03.04-Synchronization/Sources/__init__.py -nv -O Sources/__init__.py\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/tutorials/cuda-cpp/notebooks/03.04-Synchronization/Sources/ach.py -nv -O Sources/ach.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/bug.cpp\n",
    "#include \"ach.cuh\"\n",
    "\n",
    "constexpr float bin_width = 10;\n",
    "\n",
    "__global__ void histogram_kernel(\n",
    "  cuda::std::span<float> temperatures,\n",
    "  cuda::std::span<int> block_histograms,\n",
    "  cuda::std::span<int> histogram)\n",
    "{\n",
    "  cuda::std::span<int> block_histogram =\n",
    "    block_histograms.subspan(blockIdx.x * histogram.size(), histogram.size());\n",
    "\n",
    "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
    "\n",
    "  cuda::std::atomic_ref<int> block_ref(block_histogram[bin]);\n",
    "  block_ref.fetch_add(1);\n",
    "\n",
    "  if (threadIdx.x < 10) {\n",
    "    cuda::std::atomic_ref<int> ref(histogram[threadIdx.x]);\n",
    "    ref.fetch_add(block_histogram[threadIdx.x]);\n",
    "  }\n",
    "}\n",
    "\n",
    "void histogram(\n",
    "  cuda::std::span<float> temperatures,\n",
    "  cuda::std::span<int> block_histograms,\n",
    "  cuda::std::span<int> histogram,\n",
    "  cudaStream_t stream)\n",
    "{\n",
    "  int block_size = 256;\n",
    "  int grid_size = cuda::ceil_div(temperatures.size(), block_size);\n",
    "  histogram_kernel<<<grid_size, block_size, 0, stream>>>(\n",
    "    temperatures, block_histograms, histogram);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our updated kernel now accepts an additional argument for storing per-block histograms. \n",
    "Its size is the number of bins times the number of thread blocks. \n",
    "Within the kernel, we use `subspan` to focus on the portion of this buffer corresponding to the current block’s histogram.\n",
    "However, if you run the code below, you’ll see that the result is still incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Sources.ach\n",
    "Sources.ach.run(\"Sources/bug.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Race\n",
    "\n",
    "The following code contains a data race:\n",
    "\n",
    "```cpp\n",
    "cuda::std::atomic_ref<int> block_ref(block_histogram[bin]);\n",
    "block_ref.fetch_add(1);\n",
    "\n",
    "if (threadIdx.x < 10) {\n",
    "  cuda::std::atomic_ref<int> ref(histogram[threadIdx.x]);\n",
    "  ref.fetch_add(block_histogram[threadIdx.x]);\n",
    "}\n",
    "```\n",
    "\n",
    "We assumed all threads in the same thread block would finish updating the block histogram before any threads started reading from it, but CUDA threads can progress independently, even within the same thread block.  To state it more clearly, there is no guarantee that threads in the same thread block are synchronized with each other.  Some threads maybe be finished executing the entire kernel before other threads even start.  This is a very important concept to internalize as you write parallel algorithms and CUDA kernels.\n",
    "\n",
    "<img src=\"Images/data-race-read-1.png\" alt=\"Expected\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, some threads may read the histogram before it’s fully updated.\n",
    "Here, we assumed that all threads in the block finished updating block histogram before other threads start reading it.\n",
    "\n",
    "<img src=\"Images/data-race-read-2.png\" alt=\"Possible\" width=800>\n",
    "\n",
    "To fix this issue, we must force all threads to complete their updates before allowing any thread to read the block histogram. \n",
    "CUDA provides `__syncthreads()` function for this exact purpose.  The `__syncthreads()` function is a barrier which all threads in the thread block *must* reach before any thread is permitted to proceed to the next part of the code.\n",
    "\n",
    "<img src=\"Images/sync.png\" alt=\"Synchronization\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise, you'll fix the issue by adding synchronization in the appropriate place.\n",
    "Besides the correctness issue, we have some performance inefficiencies in the current implementation.\n",
    "To figure out what's wrong, let's return to what's available in the `cuda::` namespace.\n",
    "We've seen `cuda::std::atomic_ref` already, but there's also `cuda::atomic_ref` type.\n",
    "These two types share the same interface except for one important difference.\n",
    "`cuda::atomic_ref` has one extra template parameter, representing a thread scope.\n",
    "\n",
    "```c++\n",
    "cuda::std::atomic_ref<int> ref(/* ... */);\n",
    "cuda::atomic_ref<int, thread_scope> ref(/* ... */);\n",
    "```\n",
    "\n",
    "Thread scope represents the set of threads that can synchronize using a given atomic. \n",
    "Thread scope can be system, device, or block.\n",
    "\n",
    "For instance, all threads of a given system are related to each other thread by `cuda::thread_scope_system`. \n",
    "This means that a thread from any GPU (in a multi-GPU system) can synchronize with any other GPU thread, or any CPU thread. \n",
    "The `cuda::std::atomic_ref` is actually the same thing as `cuda::atomic_ref<int, cuda::thread_scope_system>`.\n",
    "\n",
    "In addition to the system scope, there are also device and block scopes.\n",
    "The device scope allows threads from the same device to synchronize with each other.\n",
    "The block scope allows threads from the same block to synchronize with each other.\n",
    "\n",
    "Since our histogram kernel is limited to a single GPU, we don't need to use the system scope.\n",
    "Besides that, only threads of a single block are issuing atomics to the same block histogram.\n",
    "This means that we can leverage the block scope to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Move on to the [exercise](03.04.02-Exercise-Histogram.ipynb), where you'll reduce the scope of the atomic operations to the minimal required scope."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
