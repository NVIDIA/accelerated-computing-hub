{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinned Memory\n",
    "\n",
    "## Content\n",
    "\n",
    "* [Swap Memory](#Swap-Memory)\n",
    "* [GPU Access](#GPU-Access)\n",
    "* [Exercise: Async Copy and Pinned Memory](02.04.02-Exercise-Copy-Overlap.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Let’s take another look at our current simulator state:\n",
    "\n",
    "```c++\n",
    "cudaStream_t compute_stream;\n",
    "cudaStreamCreate(&compute_stream);\n",
    "\n",
    "cudaStream_t copy_stream;\n",
    "cudaStreamCreate(&copy_stream);\n",
    "\n",
    "for (int write_step = 0; write_step < write_steps; write_step++) \n",
    "{\n",
    "  thrust::copy(d_prev.begin(), d_prev.end(), d_buffer.begin());\n",
    "  cudaMemcpyAsync(thrust::raw_pointer_cast(h_prev.data()),\n",
    "                  thrust::raw_pointer_cast(d_buffer.data()),\n",
    "                  height * width * sizeof(float), cudaMemcpyDeviceToHost,\n",
    "                  copy_stream);\n",
    "\n",
    "  for (int compute_step = 0; compute_step < compute_steps; compute_step++) \n",
    "  {\n",
    "    simulate(width, height, d_prev, d_next, compute_stream);\n",
    "    d_prev.swap(d_next);\n",
    "  }\n",
    "\n",
    "  cudaStreamSynchronize(copy_stream);\n",
    "  ach::store(write_step, height, width, h_prev);\n",
    "\n",
    "  cudaStreamSynchronize(compute_stream);\n",
    "}\n",
    "\n",
    "cudaStreamDestroy(compute_stream);\n",
    "cudaStreamDestroy(copy_stream);\n",
    "```\n",
    "\n",
    "We use two CUDA streams to overlap the expensive device-to-host copy (`copy_stream`) with ongoing computations (`compute_stream`). \n",
    "However, if you profile this code (for instance, using Nsight Systems), you will see that the copy and compute still run sequentially. \n",
    "This indicates we’re missing a key concept about how the hardware works. \n",
    "To understand why, we need to step back and look at how memory operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swap Memory\n",
    "\n",
    "Operating systems do not provide direct access to physical memory. Instead, programs use virtual memory, which is mapped to physical memory. Virtual memory is organized into pages, enabling the operating system to manage them flexibly, such as swapping pages to disk when physical memory runs low.\n",
    "\n",
    "![Swap](Images/swap.png \"Swap\")\n",
    "\n",
    "So any given page can be in physical memory, on disk, or in some other place, and the operating system keeps track of that.\n",
    "When the page can be relocated to disk, it's called *pageable*. \n",
    "But memory can also be page-locked, or \"pinned\" to physical memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Access\n",
    "\n",
    "What does this have to do with CUDA?\n",
    "GPU can only copy data from physical memory. \n",
    "This means that when copying data between host and device, memory has to be pinned.\n",
    "\n",
    "![GPU Access](Images/pinned-staging.png \"GPU Access\")\n",
    "\n",
    "But this cannot be right. \n",
    "We just copied data between \n",
    "host and device without doing anything special like pinning memory.\n",
    "How did that work?\n",
    "Under the covers, when moving memory from host to device the CUDA Runtime utilizes a staging buffer in pinned memory.\n",
    "When you copy data from host to device, the CUDA Runtime first copies data to the staging buffer, and then copies it to device.\n",
    "\n",
    "![Read from Pageable](Images/read-from-pageable.png \"Read from Pageable\")\n",
    "\n",
    "This should explain why our copy wasn't overlapped with compute.\n",
    "It was actually synchronous, because under the covers the data was copied to a staging buffer. \n",
    "Unbeknownst to us at the time, the code first copied a chunk of data into pinned staging buffer, waited till the copy is done, and then proceeded to copy the next chunk of data that fit into the staging buffer.\n",
    "\n",
    "The good news is that we can pin memory ourselves via an explicit function call.\n",
    "In this case, there'll be no need to stream data through staging buffer, enabling asynchrony.\n",
    "\n",
    "To allocate pinned memory, it's sufficient to use another container from Thrust:\n",
    "\n",
    "```c++\n",
    "thrust::universal_host_pinned_vector<float> pinned_memory(size);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the [next exercise](02.04.02-Exercise-Copy-Overlap.ipynb), you will use pinned memory and profile your code again to observe the improvement in overlapping data transfers with computation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
