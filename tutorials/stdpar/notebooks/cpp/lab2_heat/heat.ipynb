{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ stdpar Lab 2: 2D Unsteady Heat Equation\n",
    "\n",
    "In this lab you'll parallelize a pre-existing MPI heat-equation mini-application with the C++ parallel algorithms, such that it runs on multi-node GPU and multi-node CPU HPC systems.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "We'll be visualizing the solution with `visualize()` function from [vis.py], and compiling our code with the MPI's implementation `mpicxx` compiler using the following flags for all compilers:\n",
    "\n",
    "[vis.py]: ./vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run vis.py\n",
    "mpicxx=\"mpicxx -std=c++23 -Ofast -march=native -o heat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be selecting which compiler to use with the `OMPI_CXX` environment variable (`OMPI_CXX=g++` picks the GNU g++ compiler).\n",
    "\n",
    "A working implementation parallelized using only MPI is provided in [starting_point.cpp].\n",
    "Let's compile it, run it with 2 MPI ranks, and visualize the results.\n",
    "The Command Line interface of the `heat` mini-application binary is `./heat NX NY NITER` (it takes three extra arguments):\n",
    "  * `NX` and `NY`: number of unknowns per MPI rank in `x` and `y` dimensions,\n",
    "  * `NITERS`: number of time-step to simulate.\n",
    "\n",
    "The mini-application performs a weak scaling with increasing number of ranks (NX and NY are kept constant).\n",
    "It outputs the error every few time steps, which can be used to verify the correctness of the implementation, as well as the per-rank and total throughput achieved in GB/s.\n",
    "\n",
    "**PLEASE** be mindful to not use too many MPI ranks when running this notebook on a shared HPC system during a tutorial event. Please do run these examples with more MPI ranks on your own systems :)\n",
    "\n",
    "[starting_point.cpp]: ./starting_point.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 256 256 16000\"\n",
    "!echo \"[g++]:    \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} starting_point.cpp && {mpirun}\n",
    "!echo \"[clang++]:\" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} starting_point.cpp && {mpirun}\n",
    "!echo \"[nvc++]:  \" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} starting_point.cpp && {mpirun}\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Parallelize with C++ parallel algorithms\n",
    "\n",
    "The goal of this exercise is to parallelize the `apply_stencil` and `initialize` implementations using the C++ parallel algorithms.\n",
    "\n",
    "### Parallelizing the Stencil application\n",
    "\n",
    "The serial implementation of `apply_stencil` uses two raw loops for multi-dimensional iteration over all elements of the sub-grid `[g.x_begin, g.x_end)x[g.y_begin, g.y_end)`. It applies the `stencil` operation for each element of the grid which returns the thermal energy of each element, and then adds all the energies together:\n",
    "\n",
    "```c++\n",
    "double apply_stencil(double* u_new, double* u_old, grid g, parameters p) {\n",
    "  // Reduction over the energy:\n",
    "  double energy = 0.;\n",
    "  // Multi-dimensional iteration: [g.x_begin, g.x_end)x[g.y_begin, g.y_end)\n",
    "  for (long x = g.x_begin; x < g.x_end; ++x) {\n",
    "    for (long y = g.y_begin; y < g.y_end; ++y) {\n",
    "      // Apply stencil for element (x, y) and accumulate its energy:\n",
    "      energy += stencil(u_new, u_old, x, y, p);\n",
    "    }\n",
    "  }\n",
    "  // Return energy of the grid:\n",
    "  return energy;\n",
    "}\n",
    "```\n",
    "\n",
    "The only functions that needs to be modified to achieve this are the `stencil` and `initial_condition` (see later) functions.\n",
    "\n",
    "To parallelize it with the C++ parallel algorithms, we need to solve the `TODO`s to:\n",
    "* Construct a multi-dimensional range for `[g.x_begin, g.x_end)x[g.y_begin, g.y_end)` from two [std::views::iota] ranges, one for x and one for y, using [std::views::cartesian_product], and\n",
    "* Use the [std::transform_reduce] algorithm to apply the stencil in parallel to each element and sum the energies:\n",
    "  - Using the [std::execution::par] execution policy,\n",
    "  - Iterating over the [std::views::cartesian_product] range,\n",
    "  - Initializing the result to `0.`,\n",
    "  - Using [std::plus] to sum all the energies,\n",
    "  - Using a lambda that applies the stencil to one element and returns its energy.\n",
    "\n",
    "as follows:\n",
    "\n",
    "```c++\n",
    "double apply_stencil(double* u_new, double* u_old, grid g, parameters p) {\n",
    "  // TODO: Create one iota range per dimension for [g.x_begin,g.x_end) and [g.y_begin,g.y_end).\n",
    "  auto xs = std::views::iota(g.x_begin, g.x_end);\n",
    "  auto ys = std::views::iota(g.y_begin, g.y_end);\n",
    "  // TODO: Construct a cartesian_product range from the two iota ranges: [g.x_begin,g.x_end)x[g.y_begin,g.y_end).\n",
    "  auto ids = std::views::cartesian_product(xs, ys);\n",
    "  // TODO: Use the std::transform_reduce algorithm to apply the stencil in parallel to each element and sum the energies:\n",
    "  return std::transform_reduce(\n",
    "    // TODO: Use the std::execution::par parallel execution policy\n",
    "    std::execution::par, \n",
    "    // TODO: iterate over the cartesian_product range\n",
    "    ids.begin(), ids.end(), \n",
    "    // TODO: initialize the energy to zero\n",
    "    0., \n",
    "    // TODO: use std::plus to sum the energies\n",
    "    std::plus{}, \n",
    "    // TODO: Use a lambda that applies the stencil to one element and returns its energy:\n",
    "    [u_new, u_old, p](auto idx) {\n",
    "      // TODO [within lambda]: Extract the 1D indices from the tuple of indices:\n",
    "      auto [x, y] = idx;\n",
    "      // TODO [within lambda]: Apply the stencil and return the energy.\n",
    "      return stencil(u_new, u_old, x, y, p);\n",
    "  });\n",
    "}\n",
    "```\n",
    "\n",
    "### Parallelize the initialization\n",
    "\n",
    "To keep all memory on the device when offloading the C++ parallel algorithms, also parallelize the `initial_condition` function from its raw loop version using the [std::fill_n] parallel algorithm:\n",
    "\n",
    "```c++\n",
    "void initial_condition(grid_t u_new, grid_t u_old) {\n",
    "  // TODO: parallelize using the std::fill_n parallel algorithm\n",
    "  // BEFORE:\n",
    "  // for (long i = 0; i < u_new.size(); ++i) {\n",
    "  //   u_old.data_handle()[i] = 0.;\n",
    "  //   u_new.data_handle()[i] = 0.;\n",
    "  // }\n",
    "}\n",
    "```\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "A template for working on the solution of this exercise is provided in  [exercise1.cpp]. \n",
    "The following cell compiles and runs this template, but as provided it produces incorrect results due to the incomplete `stencil` and `initialize` implementations.\n",
    "Fix the `TODO`s in the file until it compiles and run correctly:\n",
    "\n",
    "[exercise1.cpp]: ./exercise1.cpp\n",
    "[std::views::cartesian_product]: https://en.cppreference.com/w/cpp/ranges/cartesian_product_view\n",
    "[std::transform_reduce]: https://en.cppreference.com/w/cpp/algorithm/transform_reduce\n",
    "[std::fill_n]: https://en.cppreference.com/w/cpp/algorithm/fill_n\n",
    "[std::plus]: https://en.cppreference.com/w/cpp/utility/functional/plus\n",
    "[std::execution::par]: https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 1024 16384 500\"\n",
    "!echo \"[g++]:      \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} exercise1.cpp -ltbb             && {mpirun}\n",
    "!echo \"[clang++]:  \" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} exercise1.cpp -ltbb             && {mpirun}\n",
    "!echo \"[nvc++ CPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} exercise1.cpp -stdpar=multicore && {mpirun}\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} exercise1.cpp -stdpar=gpu       && {mpirun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 1\n",
    "\n",
    "The solution for this exercise is in [solutions/exercise1.cpp].\n",
    "The following cell compiles and runs the solutions for Exercise 1 using different compilers:\n",
    "\n",
    "[solutions/exercise1.cpp]: ./solutions/exercise1.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 1024 16384 500\"\n",
    "!echo \"[g++]:      \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} solutions/exercise1.cpp -ltbb             && {mpirun}\n",
    "!echo \"[clang++]:  \" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} solutions/exercise1.cpp -ltbb             && {mpirun}\n",
    "!echo \"[nvc++ CPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} solutions/exercise1.cpp -stdpar=multicore && {mpirun}\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} solutions/exercise1.cpp -stdpar=gpu       && {mpirun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the GPU version with slightly larger inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 2048 32768 2000\"\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++ {mpicxx} solutions/exercise1.cpp -stdpar=gpu && {mpirun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Overlapping Communication and Computation\n",
    "\n",
    "The goal of this exercise is to overlap communicaiton with computation using `std::thread`, `std::atomic`, and `std::barrier`.\n",
    "\n",
    "A template for the solution is provided in [exercise2.cpp]. \n",
    "[exercise2.cpp]: ./exercise2.cpp\n",
    "\n",
    "First, notice that the computation involves a data exchange with neighbors and is split into three steps:\n",
    "\n",
    "* `internal`: processes internal rows that do not depend on data from neighbors\n",
    "* `prev_boundary`: exchanges data with neighbor at `rank - 1` and processes the rows that depend on the elements received\n",
    "* `next_boundary`: exchanges data with neighbor at `rank + 1` and processes the rows that depend on the elements received\n",
    "\n",
    "\n",
    "```c++\n",
    "double internal(double* u_new, double* u_old, parameters p) {\n",
    "    grid g { .x_start = 2, .x_end = p.nx, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    energy += stencil(u_new.get(), u_old.get(), g, p);\n",
    "}\n",
    "\n",
    "double prev_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    // Send window cells, receive halo cells\n",
    "    if (p.rank > 0) {\n",
    "      // Send bottom boundary to bottom rank\n",
    "      MPI_Send(u_old + p.ny, p.ny, MPI_DOUBLE, p.rank - 1, 0, MPI_COMM_WORLD);\n",
    "      // Receive top boundary from bottom rank\n",
    "      MPI_Recv(u_old + 0, p.ny,  MPI_DOUBLE, p.rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "    grid g { .x_start = p.nx, .x_end = p.nx + 1, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "\n",
    "double next_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    if (p.rank < p.nranks - 1) {\n",
    "        // Receive bottom boundary from top rank\n",
    "        MPI_Recv(u_old + (p.nx + 1) * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        // Send top boundary to top rank, and\n",
    "        MPI_Send(u_old + p.nx * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 1, MPI_COMM_WORLD);\n",
    "    }\n",
    "    grid g { .x_start = 1, .x_end = 2, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "```\n",
    "\n",
    "In the previous exercise, these steps are performed sequentially:\n",
    "\n",
    "```c++\n",
    "for (long it = 0; it < p.nit(); ++it) {\n",
    "    double energy = 0.;\n",
    "    // Exchange and compute domain boundaries:\n",
    "    energy += prev_boundary(u_new.data(), u_old.data(), p);\n",
    "    energy += next_boundary(u_new.data(), u_old.data(), p);\n",
    "    energy += internal(u_new.data(), u_old.data(), p);\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this exercise, we need to modify the application to perform these three steps concurrently and in parallel.\n",
    "\n",
    "This will require:\n",
    "\n",
    "* using one `std::thread` per computation in such a way that we do not launch one thread on every iteration\n",
    "* using `std::atomic<double>` for the `energy`, to enable the separate threads to modify the energy concurrently\n",
    "* using a `std::barrier` to synchronize the different threads\n",
    "\n",
    "Furthermore, one of the threads will need to perform the following in a critical section:\n",
    "  * `MPI_Reduce` of the `energy`: this operation requires for all threads to have updated the `energy` for the current iteration, so it must happen after these updates have completed\n",
    "  * reset the `energy` to `0.` before the next iteration: all threads must wait for this operation to complete before starting the next iteration\n",
    "  \n",
    "The template [exercise2.cpp] provides `TODO`s to guide you through this process: \n",
    "\n",
    "```c++\n",
    "  // TODO: use an atomic variable for the energy\n",
    "  double energy = 0.;\n",
    "    \n",
    "  // TODO: use a barrier for synchronization\n",
    "  // ...bar = ...\n",
    "\n",
    "  // TODO: use threads for the different computations\n",
    "  auto thread_prev = std::thread([/*TODO: complete capture */]() {\n",
    "      for (long it = 0; it < p.nit(); ++it) {\n",
    "          // TODO: perform the prev exchange and computation\n",
    "          // TODO: update the atomic energy\n",
    "          // TODO: synchronize with the barrier\n",
    "      }\n",
    "  });\n",
    "    \n",
    "  auto thread_next = /* TODO: similar for prev */;\n",
    "      \n",
    "  auto thread_internal = /*\n",
    "    TODO: same as for next and prev\n",
    "    TODO: need to perform the reduction in one of the threads (for example this one)\n",
    "    TODO: need to reset the atomic in one of the threads (for example this one)\n",
    "  */;\n",
    "\n",
    "  // TODO: join all threads\n",
    "\n",
    "```\n",
    "\n",
    "[exercise2.cpp]: ./exercise2.cpp\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "\n",
    "The following commands compile but produce incorrect results.\n",
    "Your goal is to fix that by following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 1024 16384 500\"\n",
    "!echo \"[g++]:      \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} exercise2.cpp -ltbb             && {mpirun}\n",
    "!echo \"[clang++]:  \" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} exercise2.cpp -ltbb             && {mpirun}\n",
    "!echo \"[nvc++ CPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} exercise2.cpp -stdpar=multicore && {mpirun}\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} exercise2.cpp -stdpar=gpu       && {mpirun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercise 2\n",
    "\n",
    "The solutions for each example are available in the `solutions/exercise2.cpp` sub-directory.\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 2 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 1024 16384 500\"\n",
    "!echo \"[g++]:      \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} solutions/exercise2.cpp -ltbb             && {mpirun}\n",
    "!echo \"[clang++]:  \" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} solutions/exercise2.cpp -ltbb             && {mpirun}\n",
    "!echo \"[nvc++ CPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} solutions/exercise2.cpp -stdpar=multicore && {mpirun}\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} solutions/exercise2.cpp -stdpar=gpu       && {mpirun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Senders & Receivers\n",
    "\n",
    "The goal of this exercise is to simplify the implementation of Exercise 2 - Overlap Communication and Computation - by using Senders & Receivers with a `static_thread_pool` to manage the host threads, while combining this with the C++ parallel algorithms.\n",
    "\n",
    "The implementation of Exercise 2 is quite complex. It requires:\n",
    "\n",
    "```c++\n",
    "// A shared atomic variable to accumulate the energy:\n",
    "std::atomic<double> energy = 0.;\n",
    "\n",
    "// A shared barrier for synchronizing threads:\n",
    "std::barrier bar(3);\n",
    "\n",
    "// User must manually create and start threads:\n",
    "std::thread thread_inner(..[&] {\n",
    "      energy += computation(...);\n",
    "      bar.arrive_and_wait();\n",
    "      // User must manually create a critical section for MPI rank reduction: \n",
    "      MPI_Reduce(...);\n",
    "      // User must manually reset the shared state on each iteration:\n",
    "      energy = 0;\n",
    "      bar.arrive_and_wait();\n",
    "  });\n",
    "\n",
    "std::thread thread_prev(...);\n",
    "std::thread thread_next(...);\n",
    "\n",
    "// User must manually join all threads before doing File I/O\n",
    "thread_prev.join();\n",
    "thread_next.join();\n",
    "thread_inner.join();\n",
    "\n",
    "// File I/O\n",
    "```\n",
    "\n",
    "In this exercise, we'll use Senders & Receivers instead to create a graph representing the computation:\n",
    "\n",
    "```c++\n",
    "stde::sender iteration_step(stde::scheduler sch, parameters p, long it,\n",
    "                            std::vector<double>& u_new, std::vector<double>& u_old) {\n",
    "    // TODO: use Senders & Receivers to create a graph representing the computation of a single iteration   \n",
    "}\n",
    "```\n",
    "\n",
    "and will then dispatch it to an execution context:\n",
    "\n",
    "```c++\n",
    "stde::static_thread_pool ctx{3}; // Thread Pool with 3 threads\n",
    "stde::scheduler auto sch = ctx.get_scheduler();\n",
    "\n",
    "for (long it = 0; it < p.nit(); ++it) {\n",
    "    stde::sync_wait(iteration_step(sch));\n",
    "}\n",
    "```\n",
    "\n",
    "### Compilation and run commands\n",
    "\n",
    "[exercise3.cpp]: ./exercise3.cpp\n",
    "\n",
    "The template [exercise3.cpp] compiles and runs as provided, but produces incorrect results due to the incomplete `iteration_step` implementation.\n",
    "\n",
    "After completing it the following blocks should compile and run correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 1024 16384 500\"\n",
    "!echo \"[g++]:      \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} exercise3.cpp -ltbb             && {mpirun}\n",
    "!echo \"[clang++]:  \" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} exercise3.cpp -ltbb             && {mpirun}\n",
    "!echo \"[nvc++ CPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} exercise3.cpp -stdpar=multicore && {mpirun}\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} exercise3.cpp -stdpar=gpu       && {mpirun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 3\n",
    "\n",
    "The solutions for each example are available in the [`solutions/exercise3.cpp`] sub-directory.\n",
    "\n",
    "[`solutions/exercise3.cpp`]: ./solutions/exercise3.cpp\n",
    "\n",
    "The following blocks compiles and runs the solutions for Exercise 3 using different compilers and C++ standard versions.\n",
    "By default, the [`static_thread_pool`] scheduler is used.\n",
    "\n",
    "[`static_thread_pool`]: https://github.com/NVIDIA/stdexec/blob/main/include/exec/static_thread_pool.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun=\"mpirun -np 4 ./bind_one_gpu_per_rank.bash ./heat 1024 16384 500\"\n",
    "!echo \"[g++]:      \" && rm -f output heat && OMPI_CXX=g++     {mpicxx} solutions/exercise3.cpp -ltbb             && {mpirun}\n",
    "!echo \"[clang++]:  \" && rm -f output heat && OMPI_CXX=clang++ {mpicxx} solutions/exercise3.cpp -ltbb             && {mpirun}\n",
    "!echo \"[nvc++ CPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} solutions/exercise3.cpp -stdpar=multicore && {mpirun}\n",
    "!echo \"[nvc++ GPU]:\" && rm -f output heat && OMPI_CXX=nvc++   {mpicxx} solutions/exercise3.cpp -stdpar=gpu       && {mpirun}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
