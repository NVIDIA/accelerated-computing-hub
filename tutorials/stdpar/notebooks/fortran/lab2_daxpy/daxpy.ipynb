{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 1: DAXPY - Accelerating portable HPC Applications with ISO Fortran\n",
    "===\n",
    "\n",
    "In this tutorial we will familiarize ourselves with the Fortran `do concurrent` feature by implementing Double-precision AX Plus Y ([DAXPY](https://netlib.org/lapack/explore-html/de/da4/group__double__blas__level1_ga8f99d6a644d3396aa32db472e0cfc91c.html)): $A \\cdot X + Y$, one of the main functions in the standard Basic Linear Algebra Subroutines (BLAS) library.\n",
    "\n",
    "The operation is a combination of scalar multiplication and vector adition. It takes two vectors of 64-bit floats, `x` and `y` and a scalar value `a`.\n",
    "It multiplies each element `x(i)` by `a` and adds the result to `y(i)`.\n",
    "\n",
    "These exercise initialize `x(i) = i` and `y(i) = 2` and use the `check` subroutine once to verify the `daxpy` implementations.\n",
    "\n",
    "The `daxpy` binaries produced take the following options: `./daxpy nx niterations`\n",
    "* `nx`: number of elements in the `x` and `y` vectors.\n",
    "* `niterations`: number of benchmark iterations.\n",
    "\n",
    "Only the code around the `! TODO`s needs changing.\n",
    "\n",
    "## Exercise 1 - `do concurrent`\n",
    "\n",
    "Take a look at [exercise1.f90](./exercise1.f90). There are two `! TODO`s for parallezing with `do concurrent` the initialization and the `daxpy` subroutine. The sequential versions using raw-loops look like this:\n",
    "\n",
    "```fortran\n",
    "! Intialize vectors `x` and `y`\n",
    "! TODO: parallelize with do-concurrent\n",
    "do i = 1, n\n",
    "  x(i)  = i\n",
    "  y(i)  = 2.\n",
    "end do\n",
    "! DAXPY: Y + Y + A * X\n",
    "subroutine daxpy(x, y, n, a)\n",
    "  use, intrinsic :: iso_fortran_env\n",
    "  implicit none\n",
    "  real(kind=8), dimension(:) :: x, y\n",
    "  real(kind=8) :: a\n",
    "  integer :: n, i  \n",
    "  ! TODO: parallelize with do-concurrent\n",
    "  do i = 1, n\n",
    "    y(i) = y(i) + a * x(i)\n",
    "  end do  \n",
    "end subroutine\n",
    "```\n",
    "\n",
    "Fortran 2008 added support for `do concurrent` to express that loop iterations are independent from each other and therefore safe to parallelize. Multi-dimensiona loops of this form:\n",
    "\n",
    "```fortran\n",
    "do i = 1,ni\n",
    "  do j = 1,nj\n",
    "     if (A(i, j) >= 0) then\n",
    "       cycle\n",
    "     end if\n",
    "     ...\n",
    "  end do\n",
    "end do\n",
    "```\n",
    "\n",
    "can be written with `do concurrent` as follows, where the mask in the `(...)` describes the iteration space:\n",
    "\n",
    "```fortran\n",
    "do concurrent (i = 1:ni, j = 1:nj, A(i, j) < 0)\n",
    "  ...\n",
    "end do\n",
    "```\n",
    "\n",
    "The goal of this first exercise is to parallelize the loops indicated with `! TODO`s in [exercise1.f90](./exercise1.f90) using `do concurrent. \n",
    "\n",
    "The following cell compiles compile and run the [exercise1.f90](./exercise1.f90) template: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!gfortran -Ofast -Wall -Wextra exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `nvfortran`, the `-stdpar=multicore` and `-stdpar=gpu` options auto-parallelize do-concurrent loops on CPUs and GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercise 1\n",
    "\n",
    "The solution is available in [solutions/exercise1.f90](./solutions/exercise1.f90):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!gfortran -Ofast -Wall -Wextra solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu solutions/exercise1.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - `do concurrent` locality specifiers\n",
    "\n",
    "The following _locality specifiers_ are part of:\n",
    "\n",
    "* Fortran 2018:\n",
    "  * `default(none)`: requires every variable used in the loop to have an explicit locality specifier except for loop indices.\n",
    "  * `shared`: different iterations of the loop share the same variable memory\n",
    "  * `local`: every _iteration_ of the loop gets an uninitialized private storage for the variable\n",
    "  * `local_init`: `local` initialized with the variable's value outside the loop\n",
    "* Fortran 2023:\n",
    "  * `reduce(op:variable)` (e.g. `reduce(+:sum)`): different iterations share the same variable memory and reduce to it with the given operation.\n",
    " \n",
    "> **Note**: `gfortran` does not support Fortran 2018 locality specifiers yet.\n",
    "\n",
    "They are specified as part of the `do concurrent` loop:\n",
    "\n",
    "```fortran\n",
    "integer :: a, b, c\n",
    "\n",
    "do concurrent(i = 1:ni, j = 1:nj) default(none) shared(a) local_init(b, c)\n",
    "  ...\n",
    "end do\n",
    "```\n",
    "\n",
    "The goal of [exercise2.f90] is to modify the loops indicated with `! TODO` comments to:\n",
    "* Add the `default(none)` specifier to each `do concurrent` loop.\n",
    "* Add the remaining locality specifiers for all other variables.\n",
    "\n",
    "The following cell compiles compile and run the [exercise2.f90] template: \n",
    "\n",
    "[exercise2.f90]: ./exercise2.f90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 2\n",
    "\n",
    "The solution is available in [solutions/exercise2.f90](./solutions/exercise2.f90):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu solutions/exercise2.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - reductions\n",
    "\n",
    "In this exercise, we'll parallelize a variant of `daxpy` we'll call `daxpy_sum`: \n",
    "\n",
    "```fortran\n",
    "sum = 0.\n",
    "do i = 1, n\n",
    "  y(i) = y(i) + a * x(i)\n",
    "  sum = sum + y(i)\n",
    "end do \n",
    "```\n",
    "\n",
    "The goal of [exercise3.f90] is to modify the loops indicated with `! TODO` comments to:\n",
    "* Add the `default(none)` specifier to each `do concurrent` loop.\n",
    "* Add the remaining locality specifiers for all other variables.\n",
    "\n",
    "The following cell compiles compile and run the [exercise3.f90] template: \n",
    "\n",
    "[exercise3.f90]: ./exercise3.f90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra exercise3.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore exercise3.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu exercise3.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 3\n",
    "\n",
    "The solution is available in [solutions/exercise3.f90](./solutions/exercise3.f90):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra solutions/exercise3.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=multicore solutions/exercise3.f90 -o daxpy\n",
    "!./daxpy 10000000 100\n",
    "!rm -f daxpy\n",
    "!nvfortran -Ofast -Wall -Wextra -stdpar=gpu solutions/exercise3.f90 -o daxpy\n",
    "!./daxpy 10000000 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
