{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35320b28-7dc6-48dc-a502-96a11eaa9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01caf9-afbb-4769-a2b6-0979ed0c2b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Prerequsities\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    " - Declare variables, write loops, and use if / else statements in C++\n",
    " - Use pointer and iterators to access arrays of data\n",
    " - Use lambda expressions (unnamed functions)\n",
    " - Write basic CUDA kernels and understand core conecepts like threads, blocks, and grids\n",
    "   - If you are new to CUDA, we recommend the [CUDA C++ Tutorial](https://github.com/NVIDIA/accelerated-computing-hub/tree/main/tutorials/cuda-cpp)\n",
    "\n",
    "\n",
    "On the infrastructure side, this lab assumes that you:\n",
    " - Have the latest (13.1+) [CUDA toolkit](https://developer.nvidia.com/cuda-downloads) installed\n",
    " - Have latest MathDx package (25.12.1) downloaded into your system (preferably installed to `/opt/nvidia/mathdx/25.12`)\n",
    " - Have an NVIDIA GPU installed in your system\n",
    " - Are running on a Linux system\n",
    "\n",
    "If you are using Windows, try WSL (Windows Subsystem for Linux) or file an issue for Windows support\n",
    "\n",
    "___\n",
    "\n",
    "### Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    " - Understand key performance characteristics and parameters in GEMM kernels\n",
    " - Be able to write performant and portable CUDA kernels that leverage Tensor Cores and TMA\n",
    " - Understand kernel fusion and directly see how it can improve performance\n",
    " - Understand how to emulate FP64 matrix multiplication with the Ozaki-I Scheme \n",
    " - Understand how to simplify kernel development and achieve higher performance with a tile-based programming model'\n",
    "\n",
    "___\n",
    "\n",
    "### Content\n",
    "\n",
    " - Matrix Multiplication Fundamentals\n",
    "   - **Exercise 2.1:** A simple DGEMM Kernel\n",
    "   - **Exercise 2.2:** Improving DGEMM Performance with Shared Memory tiling\n",
    "   - **Exercise 2.3:** Improving DGEMM Performance with pipelining\n",
    " - Matrix Multiplication with cuBLASDx\n",
    "   - **Challenge Exercise 2.4:** Implementing Pipelining with cublasDx \n",
    " - Ozaki-I Emulation\n",
    "   - **Exercise 3.1**: IGEMM Based Ozaki-I Scheme\n",
    "   - **Exercise 3.2**: Optimizing Ozaki-I With Fusion\n",
    "   - **Exercise 3.3**: A Fully Fused Ozaki-I Implementation\n",
    " - Challenge Exercise:\n",
    "   - **Challenge Exercise 4.1**: DSYRK using Ozaki-I\n",
    "\n",
    "___\n",
    "\n",
    "### Why Device Extension Libraries?\n",
    "\n",
    "In short, programming with Tensor Cores is difficult.  Each Tensor Core generation changes calling conventions, requires different synchronization patterns, requires different memory layouts, and even reads and writes to different memory subsystems.  Device extension libraries provide an interface that is stable across GPU generations and provides performant access to Tensor Cores.  In this lab, we will learn how to leverage device extension libraries, specifically cuBLASDx, to implement the Ozaki-I scheme.  The kernels we write will work on Volta and newer GPU's without code modification and can reach Speed of Light (SOL) performance on all of them.\n",
    "\n",
    "The cuBLASDx library provides a tile-based programming model to which makes writing performant Tensor Core kernels as easy as it has ever been.  A tile-based programming model allows users to define the compuation at a higher level and let library developers handle low level details like thread mapping, memory hierarchies, synchronization, and TMA.\n",
    "\n",
    "In this course, you will learn how to leverage cuBLASDx to write performant kernels for complex algorithms by using FP64 Emulation with the Ozaki-I Scheme as a testbed to understand cuBLASDx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50f139-2b98-45b7-906d-045f562d49be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
