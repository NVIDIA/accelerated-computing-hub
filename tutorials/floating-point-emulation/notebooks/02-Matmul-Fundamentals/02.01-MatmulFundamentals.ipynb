{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45494e75-0bb2-4116-a92e-72bd2d5466f6",
   "metadata": {},
   "source": [
    "# Matrix Multiplication Fundamentals\n",
    "\n",
    "A GEMM (General Matrix Multiply) operation takes the form $C = \\alpha \\mathbf{A}\\mathbf{B} + \\beta\\mathbf{C}$ where $\\alpha, \\beta$ are scalars, $\\mathbf{A}$ is an $m \\times k$ matrix, $\\mathbf{B}$ is a $k \\times n$ matrix, and $\\mathbf{C}$ is a $m \\times n$ matrix.\n",
    "\n",
    "The element at row $i$ and column $j$ of matrix $\\mathbf{C}$ is calculated as the scaled and biased dot product of row $i$ of $\\mathbf{A}$ and column $j$ of $\\mathbf{B}$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{i, j} = \\alpha \\left(\\sum_{l=0}^{k} \\mathbf{A}_{i, l} \\mathbf{B}_{l, j} \\right) + \\beta \\mathbf{C}_{i, j}\n",
    "$$\n",
    "\n",
    "In implementation the above operation is usually split into 2 parts:\n",
    "1. **Matrix Multiplication** itself, computing $ \\mathbf{D}_{i, j} = \\sum_{l=0}^{k} \\mathbf{A}_{i, l} \\mathbf{B}_{l, j} $\n",
    "2. **Epilogue**, computing $ \\mathbf{C}_{i, j} = \\alpha \\cdot \\mathbf{D}_{i, j} + \\beta \\cdot \\mathbf{C}_{i, j} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd8594-9e87-406a-92c9-2386aaef7168",
   "metadata": {},
   "source": [
    "## Exercise Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df324ba7-6ef8-4939-86e5-c94da28db3e5",
   "metadata": {},
   "source": [
    "### C++ - CMake configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e8fd3-94e1-421b-9401-fd6cdf059863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "from common_cuda import setup_cmake_project\n",
    "\n",
    "# A python cmake wrapper to determine the GPU architecture and compile for only that\n",
    "setup_cmake_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3e422-bcf5-416c-aa6b-824e830a0acb",
   "metadata": {},
   "source": [
    "### Python - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc473f40-e19a-4956-b7d4-3cd0f6c6a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import nvmath\n",
    "\n",
    "from nvmath.device import Matmul\n",
    "from nvmath.device.cublasdx import DevicePipeline, SharedStorageCalc, MAX_ALIGNMENT\n",
    "from nvmath.device.cublasdx_numba import pipeline_extensions\n",
    "from nvmath.device.common import axpby, clear, copy, copy_fragment, copy_wait, make_tensor\n",
    "from numba import cuda\n",
    "\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "\n",
    "from benchmark import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5cf955-0389-427c-8f82-43680f5be3f9",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Naive DGEMM Kernel\n",
    "\n",
    "In this exercise, we will implement a naive GEMM algorithm by having each CUDA thread calculate one element in our C matrix:\n",
    "\n",
    "<img src=\"./images/naive_gemm.png\" width=\"1600\" height=\"auto\"/>\n",
    "\n",
    "This diagram shows that we compute element (0, 0) of the C matrix by calculating the dot product between row 0 of the A matrix and column 0 of the B matrix.  This can be done by iterating along the K dimension and at each step multiplying i'th element of the row of A and column of B and accumulating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035895b-38b4-4fe9-86a7-006620ac1310",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f3a97-41b2-4f09-9091-21d39e6a9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1a/parameters.hpp.inc\n",
    "\n",
    "    // (gemm_m, gemm_n, gemm_k, alpha, beta)\n",
    "    std::vector<tutorial::gemm_problem_t> problems = {\n",
    "        {2048, 2048, 2048, 0.9, 1.1}\n",
    "    };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401fb9b-bcf5-4921-a4ab-f615912bdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1a/kernel.hpp.inc\n",
    "\n",
    "template<int BlockSize, class TensorA, class TensorB, class TensorC>\n",
    "__launch_bounds__(BlockSize, 1) __global__ void kernel_1a_simple_dgemm(double        alpha,\n",
    "                                                                       TensorA const tensor_a,\n",
    "                                                                       TensorB const tensor_b,\n",
    "                                                                       double        beta,\n",
    "                                                                       TensorC       tensor_c) {\n",
    "    int const thread_row_idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int const thread_col_idx = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    auto [size_m, size_n] = tensor_c.shape();\n",
    "    auto size_k = tutorial::size<1>(tensor_a);\n",
    "\n",
    "    // EXERCISE --> What are the conditions for early exit?\n",
    "    if (...) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    double accumulator = 0.0;\n",
    "\n",
    "    // EXERCISE --> Complete the following implementation to compute the dot product between row 'thread_row_idx' of matrix A \n",
    "    // and the column of 'thread_col_idx' of matrix B.\n",
    "    for (...) {\n",
    "        accumulator += ...;\n",
    "    }\n",
    "\n",
    "    // We can use the tensor object to do 2D indexing as follows:\n",
    "    double c_elem = tensor_c(thread_row_idx, thread_col_idx);\n",
    "\n",
    "    // HINT: Remember that we are doing a GEMM operation (see above)\n",
    "    tensor_c(thread_row_idx, thread_col_idx) = ...;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a2947-b7d9-4351-bbac-7a911dee35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1a/kernel_parameters.hpp.inc\n",
    "    // If you have time, try a few different block dimensions and see how the performance changes.\n",
    "\n",
    "    // Setup kernel configuration\n",
    "    int const block_dim_x = 16;\n",
    "    int const block_dim_y = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b78dc4-0691-4aa3-b357-cb97beed15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build build/ -t 1a_simple_dgemm_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6502a-810d-4c9d-8fff-85339aab5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/1a_simple_dgemm_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad532e04-e70a-40cc-8cc7-19589db4b9bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd350ee-f00d-40f4-8731-cbc2dc73a672",
   "metadata": {},
   "source": [
    "We will rewrite kernel now and recompile the solution. If you want to restart your exercise make sure you rewrite kernel back and recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb10bd7-4e7a-40b9-b3ee-c711a1252a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1a/kernel.hpp.inc\n",
    "\n",
    "template<int BlockSize, class TensorA, class TensorB, class TensorC>\n",
    "__launch_bounds__(BlockSize, 1) __global__ void kernel_1a_simple_dgemm(double        alpha,\n",
    "                                                                       TensorA const tensor_a,\n",
    "                                                                       TensorB const tensor_b,\n",
    "                                                                       double        beta,\n",
    "                                                                       TensorC       tensor_c) {\n",
    "    int const thread_row_idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int const thread_col_idx = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    auto [size_m, size_n] = tensor_c.shape();\n",
    "    auto size_k = tutorial::size<1>(tensor_a);\n",
    "\n",
    "    if (thread_row_idx > size_m || thread_col_idx > size_n) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    double accumulator = 0.0;\n",
    "\n",
    "    // EXERCISE --> Complete the following implementation to compute the dot product between row 'thread_row_idx' of matrix A \n",
    "    // and the column of 'thread_col_idx' of matrix B.\n",
    "    for (int i = 0; i < size_k; i++) {\n",
    "        accumulator += tensor_a(thread_row_idx, i) * tensor_b(i, thread_col_idx);\n",
    "    }\n",
    "\n",
    "    // We can use the tensor object to do 2D indexing as follows:\n",
    "    double c_elem = tensor_c(thread_row_idx, thread_col_idx);\n",
    "\n",
    "    // HINT: Remember that we are doing a GEMM operation (see above)\n",
    "    tensor_c(thread_row_idx, thread_col_idx) = alpha * accumulator + beta * c_elem;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61004b-fe28-4b94-9b3f-cb39ed987351",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build build/ -t 1a_simple_dgemm_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9d43b-8644-42a4-8555-d599223369f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/1a_simple_dgemm_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf685f-c367-4655-a9c3-ef982c9cb112",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c1522-c87b-4421-9ad3-a01b49f61ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problems that we will benchmark and conduct accuracy tests on the tuple should be formed as:\n",
    "# (GEMM_M, GEMM_N, GEMM_K, ALPHA, BETA)\n",
    "problems = [\n",
    "  (2048, 2048, 2048, 0.9, 1.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9181c-8fed-4c01-9727-4859cdf1a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_1_dgemm_kernel(block_x = 16, block_y = 16):\n",
    "    block_size = block_x * block_y\n",
    "    \n",
    "    @cuda.jit(launch_bounds=(block_size, 1))\n",
    "    def dgemm_kernel(alpha, tensor_a, tensor_b, beta, tensor_c):\n",
    "        m, n = tensor_c.shape\n",
    "        _, k = tensor_a.shape\n",
    "\n",
    "        thread_row_idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        thread_col_idx = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        # EXERCISE --> What are the conditions for early exit?\n",
    "        #if :\n",
    "        #    return\n",
    "\n",
    "        accumulator = 0.0\n",
    "\n",
    "        # EXERCISE --> Complete the following implementation to compute the dot product between row 'thread_row_idx' of matrix A \n",
    "        # and the column of 'thread_col_idx' of matrix B.\n",
    "        #for ...:\n",
    "        #    accumulator += ...\n",
    "    \n",
    "        # We can use the tensor object to do 2D indexing as follows:\n",
    "        c_elem = tensor_c[thread_row_idx, thread_col_idx]\n",
    "    \n",
    "        # HINT: Remember that we are doing a GEMM operation (see above)\n",
    "        #tensor_c[thread_row_idx, thread_col_idx] = ...;\n",
    "    \n",
    "    return dgemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94253b37-b532-4c18-9832-2ccf7cdffc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_kernel_params_2_1(m, n, k, alpha, beta):\n",
    "    # EXERCISE --> Try a few different block dimensions.  A few questions to think about:\n",
    "    #              How does performance change if they are not powers of 2?\n",
    "    #              How does performance change with rectangular shapes?  What if you change the gemm problem?\n",
    "    return 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff1fab-15fc-4398-8f55-10b3d3d0a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dgemm_2_1(problems, get_2_1_dgemm_kernel, choose_kernel_params_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bd916-9f25-40fd-8855-ccd4f3f48462",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd791e-61dc-4e7c-ab02-720b7d346b2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def get_2_1_dgemm_kernel_solution(block_x = 16, block_y = 16):\n",
    "    block_size = block_x * block_y\n",
    "    \n",
    "    @cuda.jit(launch_bounds=(block_size, 1))\n",
    "    def dgemm_kernel(alpha, tensor_a, tensor_b, beta, tensor_c):\n",
    "        m, n = tensor_c.shape\n",
    "        _, k = tensor_a.shape\n",
    "\n",
    "        thread_row_idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        thread_col_idx = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        accumulator = 0.0\n",
    "\n",
    "        # EXERCISE --> Complete the following implementation to compute the dot product between row 'thread_row_idx' of matrix A \n",
    "        # and the column of 'thread_col_idx' of matrix B.\n",
    "        for i in range(k):\n",
    "            accumulator += tensor_a[thread_row_idx, i] * tensor_b[i, thread_col_idx]\n",
    "    \n",
    "        # We can use the tensor object to do 2D indexing as follows:\n",
    "        c_elem = tensor_c[thread_row_idx, thread_col_idx]\n",
    "    \n",
    "        # HINT: Remember that we are doing a GEMM operation (see above)\n",
    "        tensor_c[thread_row_idx, thread_col_idx] = alpha * accumulator + beta * c_elem;\n",
    "    \n",
    "    return dgemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3475a45-27de-4a23-a4a0-f67e830e1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dgemm_2_1(problems, get_2_1_dgemm_kernel_solution, choose_kernel_params_2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683921e9-ff62-46e6-af52-0ee9e1ff5919",
   "metadata": {},
   "source": [
    "### Analyzing Naive GEMM Performance\n",
    "\n",
    "Our kernel is expectedly performing just okay but how can we understand if this is an algorithmic or implementation limitation?  Let's breifly analyze our implementation using a [roofline model](https://en.wikipedia.org/wiki/Roofline_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d172ef-8d28-4989-854a-402f63669871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TFLOPS, MEMORY BANDWIDTH (GB/s)\n",
    "GPU_SPECS = {\n",
    "    \"L40S\": (1.43, 864),\n",
    "    \"B200\": (37, 6200)\n",
    "}\n",
    "\n",
    "def roofline_prediction_2_1(m, n, k):\n",
    "    FP64_TFLOPS, MEMORY_BANDWIDTH_GBS = GPU_SPECS[\"L40S\"]\n",
    "\n",
    "    # By design since each thread is computing one output element\n",
    "    threads = m * n\n",
    "\n",
    "    # Each dot product consists of k multiplications and k adds \n",
    "    flops_per_thread = 2 * k\n",
    "\n",
    "    fp64_size = np.dtype(np.float64).itemsize\n",
    "\n",
    "    # We load a row of matrix A, a column of matrix B, and read from / write to matrix C\n",
    "    memory_per_thread = (2 * k + 2) * fp64_size\n",
    "\n",
    "    total_memory_gb = threads * memory_per_thread * 1e-9\n",
    "    total_tflop = threads * flops_per_thread * 1e-12\n",
    "\n",
    "    return total_tflop / FP64_TFLOPS, total_memory_gb / MEMORY_BANDWIDTH_GBS\n",
    "\n",
    "time_flops, time_membw = roofline_prediction_2_1(2048, 2048, 2048)\n",
    "\n",
    "print(f\"The runtime from the math operations {time_flops * 1e3} ms and the runtime from memory is {time_membw * 1e3} ms\")\n",
    "\n",
    "# We will either be bottlenecked by FLOPS or Memory Bandwidth, so we take the maximum\n",
    "print(f\"Therefore, the estimated best case runtime is {max(time_flops, time_membw) * 1e3} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692deaa6-e830-4d64-9561-1ac32f03d451",
   "metadata": {},
   "source": [
    "We can see that our kernel is performing roughly as we'd expect with some small improvements which are probably due to the hardware caching inputs it's already seen and GPU latency hiding.  Now that we know that memory is the main bottleneck, we further optimize memory movement in the next exercise.\n",
    "\n",
    "Information regarding certain GPUs' capabilities can be found in official datasheets, you can see some examples here:\n",
    "- [NVIDIA A100](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf)\n",
    "- [NVIDIA L40](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/support-guide/NVIDIA-L40-Datasheet-January-2023.pdf)\n",
    "- [NVIDIA L40s](https://images.nvidia.com/content/Solutions/data-center/vgpu-L40-datasheet.pdf)\n",
    "- [NVIDIA Blackwell RTX Pro](https://www.nvidia.com/content/dam/en-zz/Solutions/data-center/rtx-pro-6000-blackwell-workstation-edition/workstation-blackwell-rtx-pro-6000-workstation-edition-nvidia-us-3519208-web.pdf)\n",
    "- [NVIDIA B200](https://resources.nvidia.com/en-us-blackwell-architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27106acc-03df-41f6-891e-3d2cd96d32d8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Improving DGEMM with Shared Memory Tiling\n",
    "\n",
    "In this exercise, we will discuss the memory subsystem of GPUs and leverage these hardware properties for further optimization.\n",
    "\n",
    "The memory subsystem for GPUs has many components.  Some of which are global memory, the L2 cache, the L1 cache, and registers. The relative access speed for each is very dependent on the GPU, however, a common analogy can be used to describe their relative speeds:\n",
    "\n",
    " - Accessing registers is similar to already having the part in your hand\n",
    " - Accessing L1 is like having the part in your pocket\n",
    " - Accessing L2 is similar to having the part on your workbench\n",
    " - Accessing global memory is like having the part in your toolbox in another room\n",
    "\n",
    "The higher you are in the memory hierarchy, the slower it is to access.  However, at higher memory hierarchies, you typically have more space.  Keeping with the analogy, since we can't keep that many parts in our hands at once, we need to strategically decide where to store our parts in order to increase our efficiency.\n",
    "\n",
    "In **Exercise 2.0**, what we've effectively done is read from global memory on each access, which would be like running to the other room everytime we needed a new part.  In this exercise, we will strategically store data in L1 (the pockets in our analogy) and periodically fetch new data (or parts).  CUDA allows us to read/write from the L1 cache through the use of shared memory. The exercise below will have us read from global memory in \"tiles\" and then do computations on tiles of the A/B matrices by reading from shared memory like we've shown below\n",
    "\n",
    "<img src=\"images/tiling.png\" width=\"1400\" height=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1e992-0f49-4605-9a25-23f30f58048b",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236fda1-193d-44a0-b25c-20c546c51d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1b/parameters.hpp.inc\n",
    "    // (gemm_m, gemm_n, gemm_k, alpha, beta)\n",
    "    std::vector<tutorial::gemm_problem_t> problems = {\n",
    "        {2048, 2048, 2048, 0.9, 1.1}\n",
    "    };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25d13d-11ec-45ee-a3c9-01681edb30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1b/kernel.hpp.inc\n",
    "\n",
    "template <int BlockM, int BlockN, int BlockK>\n",
    "struct tile_config {\n",
    "    static constexpr int m = BlockM;\n",
    "    static constexpr int n = BlockN;\n",
    "    static constexpr int k = BlockK;\n",
    "\n",
    "    static constexpr int num_elems_a = m * k;\n",
    "    static constexpr int num_elems_b = k * n;\n",
    "\n",
    "    static constexpr int max_threads_per_block = BlockM * BlockN;\n",
    "\n",
    "    static_assert(m == n && m == k, \"This constraint is for simplicity, feel free to challenge yourself and complicate the config\");\n",
    "};\n",
    "\n",
    "template<class TileConfig, class TensorA, class TensorB, class TensorC>\n",
    "__launch_bounds__(TileConfig::max_threads_per_block, 1) __global__\n",
    "    void kernel_1b_simple_dgemm_shared(double        alpha,\n",
    "                                       TensorA const tensor_a,\n",
    "                                       TensorB const tensor_b,\n",
    "                                       double        beta,\n",
    "                                       TensorC const tensor_c) {\n",
    "    extern __shared__ __align__(sizeof(double)) unsigned char smem[];\n",
    "\n",
    "    double* smem_a_data = reinterpret_cast<double*>(smem);\n",
    "    auto smem_a_tensor  = tutorial::make_smem_tensor<arrangement::col_major, TileConfig::m, TileConfig::k>(smem_a_data);\n",
    "\n",
    "    double* smem_b_data = tutorial::raw_pointer_cast(smem_a_tensor.data()) + TileConfig::num_elems_a;\n",
    "    auto smem_b_tensor  = tutorial::make_smem_tensor<arrangement::row_major, TileConfig::k, TileConfig::n>(smem_b_data);\n",
    "\n",
    "    // Assert that for A: mxk and B: kxn both Ks are the same size\n",
    "    auto const global_k = tutorial::size<1>(tensor_a);\n",
    "\n",
    "    // Define accumulator storage\n",
    "    double accumulator = 0.0;\n",
    "\n",
    "    int const idx_x = threadIdx.x;\n",
    "    int const idx_y = threadIdx.y;\n",
    "\n",
    "    int const thread_row_idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    int const thread_col_idx = threadIdx.y + blockDim.y * blockIdx.y;\n",
    "\n",
    "    // EXERCISE -> Iterate in tiles along the K dimension, store tiles of the A and B matrices into shored memory,\n",
    "    //             and read from shared memory buffers when accumulating.\n",
    "    // \n",
    "    // Hints:\n",
    "    //  - When do we need synchronize with (__syncthreads)?\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da81e0b-ab39-42c8-9926-533be5b5ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1b/kernel_parameters.hpp.inc\n",
    "\n",
    "    constexpr int tile_m      = 16;\n",
    "    constexpr int tile_n      = 16;\n",
    "    constexpr int tile_k      = 16;\n",
    "    using tile                = tile_config<tile_m, tile_n, tile_k>;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a039803-ec09-4020-8612-e251d0b87b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build build/ -t 1b_simple_dgemm_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923857d-72ed-48b7-9b16-cd3897881b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/1b_simple_dgemm_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e15134-6812-4dbf-9f11-34610d0696d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef2502-d0a1-4808-add9-8396b8615226",
   "metadata": {},
   "source": [
    "We will rewrite kernel now and recompile the solution. If you want to restart your exercise make sure you rewrite kernel back and recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce3886-b98c-4093-a3a1-0ed2847e4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1b/kernel.hpp.inc\n",
    "\n",
    "template <int BlockM, int BlockN, int BlockK>\n",
    "struct tile_config {\n",
    "    static constexpr int m = BlockM;\n",
    "    static constexpr int n = BlockN;\n",
    "    static constexpr int k = BlockK;\n",
    "\n",
    "    static constexpr int num_elems_a = m * k;\n",
    "    static constexpr int num_elems_b = k * n;\n",
    "\n",
    "    static constexpr int max_threads_per_block = BlockM * BlockN;\n",
    "\n",
    "    static_assert(m == n && m == k, \"This constraint is for simplicity, feel free to challenge yourself and complicate the config\");\n",
    "};\n",
    "\n",
    "template<class TileConfig, class TensorA, class TensorB, class TensorC>\n",
    "__launch_bounds__(TileConfig::max_threads_per_block, 1) __global__\n",
    "    void kernel_1b_simple_dgemm_shared(double        alpha,\n",
    "                                       TensorA const tensor_a,\n",
    "                                       TensorB const tensor_b,\n",
    "                                       double        beta,\n",
    "                                       TensorC const tensor_c) {\n",
    "    extern __shared__ __align__(sizeof(double)) unsigned char smem[];\n",
    "\n",
    "    double* smem_a_data = reinterpret_cast<double*>(smem);\n",
    "    auto smem_a_tensor  = tutorial::make_smem_tensor<arrangement::col_major, TileConfig::m, TileConfig::k>(smem_a_data);\n",
    "\n",
    "    double* smem_b_data = tutorial::raw_pointer_cast(smem_a_tensor.data()) + TileConfig::num_elems_a;\n",
    "    auto smem_b_tensor  = tutorial::make_smem_tensor<arrangement::row_major, TileConfig::k, TileConfig::n>(smem_b_data);\n",
    "\n",
    "    // Assert that for A: mxk and B: kxn both Ks are the same size\n",
    "    auto const global_k = tutorial::size<1>(tensor_a);\n",
    "\n",
    "    // Define accumulator storage\n",
    "    double accumulator = 0.0;\n",
    "\n",
    "    int const idx_x = threadIdx.x;\n",
    "    int const idx_y = threadIdx.y;\n",
    "\n",
    "    int const thread_row_idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    int const thread_col_idx = threadIdx.y + blockDim.y * blockIdx.y;\n",
    "\n",
    "    // EXERCISE -> Iterate in tiles along the K dimension, store tiles of the A and B matrices into shored memory,\n",
    "    //             and read from shared memory buffers when accumulating.\n",
    "    // \n",
    "    // Hints:\n",
    "    //  - When do we need synchronize with (__syncthreads)?\n",
    "    for (int tile_iter = 0; tile_iter < (global_k / TileConfig::k); ++tile_iter) {\n",
    "\n",
    "        // Load current tile into shared memory\n",
    "        auto current_global_tile_a = cublasdx::get_tile(tensor_a, smem_a_tensor.shape(), blockIdx.x, tile_iter);\n",
    "        auto current_global_tile_b = cublasdx::get_tile(tensor_b, smem_b_tensor.shape(), tile_iter, blockIdx.y);\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        smem_a_tensor(idx_x, idx_y) = current_global_tile_a(idx_x, idx_y);\n",
    "        smem_b_tensor(idx_x, idx_y) = current_global_tile_b(idx_x, idx_y);\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < TileConfig::k; i++) {\n",
    "            accumulator += smem_a_tensor(idx_x, i) * smem_b_tensor(i, idx_y);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    double const c_elem = tensor_c(thread_row_idx, thread_col_idx);\n",
    "    double const result = alpha * accumulator + beta * c_elem;\n",
    "\n",
    "    // Store results\n",
    "    tensor_c(thread_row_idx, thread_col_idx) = result;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601a4f2-cfc7-4e04-a12c-08d3e6e1cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build build/ -t 1b_simple_dgemm_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7234e3-0823-4711-9e7c-a17ce6807e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/1b_simple_dgemm_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898b77d-e169-45a6-9efd-e7b839034ef0",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661200e7-824b-4086-932a-3539fd1de5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problems that we will benchmark and conduct accuracy tests on the tuple should be formed as:\n",
    "# (GEMM_M, GEMM_N, GEMM_K, ALPHA, BETA)\n",
    "problems = [\n",
    "  (2048, 2048, 2048, 1.0, 1.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5193a24-1a1d-4178-a54d-afe4f63b87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_2_dgemm_kernel():\n",
    "    # For this kernel, we simplify to only 16x16x16 tile size.\n",
    "    # While it's possible to change tile sizes, it significantly complicates code\n",
    "    TILE_M = 16\n",
    "    TILE_N = 16\n",
    "    TILE_K = 16\n",
    "\n",
    "    BLOCK_SIZE = 16 * 16\n",
    "    \n",
    "    @cuda.jit(launch_bounds=(BLOCK_SIZE, 1))\n",
    "    def dgemm_kernel(alpha, tensor_a, tensor_b, beta, tensor_c):\n",
    "        m, n = tensor_c.shape\n",
    "        _, k = tensor_a.shape\n",
    "\n",
    "        smem_a_tensor = cuda.shared.array(shape=(TILE_M, TILE_K), dtype=np.float64)\n",
    "        smem_b_tensor = cuda.shared.array(shape=(TILE_K, TILE_N), dtype=np.float64)\n",
    "\n",
    "        idx_x = cuda.threadIdx.x\n",
    "        idx_y = cuda.threadIdx.y\n",
    "        \n",
    "        thread_row_idx = idx_x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        thread_col_idx = idx_y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        # EXERCISE -> Iterate in tiles along the K dimension, store tiles of the A and B matrices into shored memory,\n",
    "        #             and read from shared memory buffers when accumulating.\n",
    "        # \n",
    "        # Hints:\n",
    "        #  - When do we need synchronize with (cuda.syncthreads())?\n",
    "    \n",
    "    return dgemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929069e-d2f6-4319-a5e1-aaaf2db5f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dgemm_2_2(problems, get_2_2_dgemm_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb10441-47dd-405d-999f-50045aaeb293",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c440da-e5f6-4950-8964-835e38a9641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_2_dgemm_kernel_solution():\n",
    "    # For this kernel, we simplify to only 16x16x16 tile size.\n",
    "    # While it's possible to change tile sizes, it significantly complicates code\n",
    "    TILE_M = 16\n",
    "    TILE_N = 16\n",
    "    TILE_K = 16\n",
    "\n",
    "    BLOCK_SIZE = 16 * 16\n",
    "    \n",
    "    @cuda.jit(launch_bounds=(BLOCK_SIZE, 1))\n",
    "    def dgemm_kernel(alpha, tensor_a, tensor_b, beta, tensor_c):\n",
    "        m, n = tensor_c.shape\n",
    "        _, k = tensor_a.shape\n",
    "\n",
    "        smem_a_tensor = cuda.shared.array(shape=(TILE_M, TILE_K), dtype=np.float64)\n",
    "        smem_b_tensor = cuda.shared.array(shape=(TILE_K, TILE_N), dtype=np.float64)\n",
    "\n",
    "        idx_x = cuda.threadIdx.x\n",
    "        idx_y = cuda.threadIdx.y\n",
    "        \n",
    "        thread_row_idx = idx_x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        thread_col_idx = idx_y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        accumulator = 0.0\n",
    "\n",
    "        # EXERCISE -> Iterate in tiles along the K dimension, store tiles of the A and B matrices into shored memory,\n",
    "        #             and read from shared memory buffers when accumulating.\n",
    "        # \n",
    "        # Hints:\n",
    "        #  - When do we need synchronize with (cuda.syncthreads())?\n",
    "        for tile_k_start in range(0, k, TILE_K):\n",
    "            smem_a_tensor[idx_x, idx_y] = tensor_a[thread_row_idx, tile_k_start + idx_y]\n",
    "            smem_b_tensor[idx_x, idx_y] = tensor_b[tile_k_start + idx_x, thread_col_idx]\n",
    "    \n",
    "            cuda.syncthreads()\n",
    "    \n",
    "            for i in range(0, TILE_K):\n",
    "                accumulator += smem_a_tensor[idx_x, i] * smem_b_tensor[i, idx_y]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        c_elem = tensor_c[thread_row_idx, thread_col_idx]\n",
    "        \n",
    "        tensor_c[thread_row_idx, thread_col_idx] = alpha * accumulator + beta * c_elem\n",
    "    \n",
    "    return dgemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a534c61-ec4c-4226-8409-b4a277776254",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dgemm_2_2(problems, get_2_2_dgemm_kernel_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d7476-7b69-4a07-bbf5-b02823603a4b",
   "metadata": {},
   "source": [
    "### Analyzing Tiled GEMM Performance\n",
    "\n",
    "Let's modify our roofline model and consider the optimizations we've made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb757-b78b-465b-90b1-7bf768e74f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# TFLOPS, MEMORY BANDWIDTH (GB/s)\n",
    "GPU_SPECS = {\n",
    "    \"L40S\": (1.43, 864),\n",
    "    \"B200\": (37, 6200)\n",
    "}\n",
    "\n",
    "def roofline_prediction_2_2(m, n, k, TILE_M=16, TILE_N=16, TILE_K=16):\n",
    "    FP64_TFLOPS, MEMORY_BANDWIDTH_GBS = GPU_SPECS[\"L40S\"]\n",
    "\n",
    "    # Let's instead \n",
    "\n",
    "    # By design since each thread is computing one output element\n",
    "    tiles = math.ceil(m / TILE_M) * math.ceil(n / TILE_N)\n",
    "\n",
    "    # Each tile does TILE_M * TILE_N dot products which each have k multiplications and k additions\n",
    "    flops_per_tile = 2 * TILE_M * TILE_N * k\n",
    "\n",
    "    fp64_size = np.dtype(np.float64).itemsize\n",
    "\n",
    "    # We load a TILE_M rows of matrix A, TILE_N columns of matrix B, and write to and read from TILE_M * TILE_N elements of matrix C\n",
    "    memory_per_tile = (TILE_M * k + TILE_N * k + 2 * TILE_M * TILE_N) * fp64_size\n",
    "\n",
    "    total_memory_gb = tiles * memory_per_tile * 1e-9\n",
    "    total_tflop = tiles * flops_per_tile * 1e-12\n",
    "\n",
    "    return total_tflop / FP64_TFLOPS, total_memory_gb / MEMORY_BANDWIDTH_GBS\n",
    "\n",
    "time_flops, time_membw = roofline_prediction_2_2(2048, 2048, 2048)\n",
    "\n",
    "print(f\"The runtime from the math operations {time_flops * 1e3} ms and the runtime from memory is {time_membw * 1e3} ms\")\n",
    "\n",
    "# We will either be bottlenecked by FLOPS or Memory Bandwidth, so we take the maximum\n",
    "print(f\"Therefore, the estimated best case runtime is {max(time_flops, time_membw) * 1e3} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ad335-9a59-4489-b626-f60e7499f49a",
   "metadata": {},
   "source": [
    "Information regarding certain GPUs' capabilities can be found in official datasheets, you can see some examples here:\n",
    "- [NVIDIA A100](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf)\n",
    "- [NVIDIA H200](https://resources.nvidia.com/en-us-hopper-architecture)\n",
    "- [NVIDIA L40](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/support-guide/NVIDIA-L40-Datasheet-January-2023.pdf)\n",
    "- [NVIDIA L40s](https://images.nvidia.com/content/Solutions/data-center/vgpu-L40-datasheet.pdf)\n",
    "- [NVIDIA RTX PRO 6000 Blackwell Server Edition](https://www.nvidia.com/content/dam/en-zz/Solutions/data-center/rtx-pro-6000-blackwell-workstation-edition/workstation-blackwell-rtx-pro-6000-workstation-edition-nvidia-us-3519208-web.pdf)\n",
    "- [NVIDIA B200](https://resources.nvidia.com/en-us-blackwell-architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e400f-6079-4572-94de-0ffa35f20d58",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Improving DGEMM with the cuBLASDx Pipeline API\n",
    "\n",
    "The roofline models we used, despite being an over-simplification, provide us a lot of insight.  If we are close to the roofline, that means that we as optimized as we can be without fundamentally changing the algorithm (like when we introduced tiling).  If we are far away from the roofline, then we know that looking for further optimizations could be worth it.\n",
    "\n",
    "Going forward, further optimizations follow a similar structure, where we would apply techniques to increase the tile size and implement similar tiling schemes at lower levels of the memory hierarchy.  The algorithms grow more complicated and the source code length grows equally, if not more.  Rather than continuing with these techniques, for the remaining exercises, we will leverage the advanced optimizations without needing to implement them by calling the cublasDx and nvmath-python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa895b0-efcf-47d8-b426-eab5dd4bff1c",
   "metadata": {},
   "source": [
    "### What is GEMM pipelining\n",
    "\n",
    "MathDx operations are typically performed on the shared memory tile level just like we implemented in exercise 2.2.  The typical procedure is:\n",
    "\n",
    "1. Load data into shared memory\n",
    "2. Perform computations using shared memory\n",
    "3. Store results\n",
    "\n",
    "In this approach kernels themselves are potentially compute-bound, but when one stage of compute is being performed another could already be in flight into shared memory, this is what we describe as **pipelined approach**. Without such overlapping, CUDA hardware latency hiding may not be enough to maximize memory bandwidth. This way the computational units (Tensor Cores) can be operating at maximal occupancy all the time, without ever stalling while waiting for next stage. An advanced extension of pipelining is the producer-consumer model, where separate threads are responsible for loading data into buffers and separate for computing results of the loaded elements.\n",
    "\n",
    "**cuBLASDx pipeline API** exposes this pipelined logic with a simple interface, still allowing for fusion at all levels of hierarchy, either before computation or after.\n",
    "\n",
    "![Pipeline](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d954fe-0d0b-4592-b023-d4563dc4f3f3",
   "metadata": {},
   "source": [
    "**How to find optimal bytes-in-flight (tile size and pipeline depth)?**\n",
    "\n",
    "**Little’s law** in queuing theory states that the average number of items $L$ in a stable system equals the product of the arrival (or service) throughput $\\lambda$ and the average time $W$ that an item spends in the system, i.e., $ L = \\lambda W $\n",
    "\n",
    "In the context of GPU/CUDA memory systems, the “items” can be interpreted as data units, such as bytes or cache lines moving through the memory hierarchy. The throughput $\\lambda$ is then the sustained memory bandwidth in bytes per second, and the time $W$ corresponds to the effective memory access latency in seconds.\n",
    "\n",
    "Combining these interpretations yields the following approximation:\n",
    "$$\n",
    "\\text{bytes in flight} \\approx \\text{bandwidth} \\times \\text{latency}.\n",
    "$$\n",
    "\n",
    "This expresses that the amount of data concurrently outstanding in the memory system must be large enough to match the product of the achievable bandwidth and the latency.\n",
    "\n",
    "Practically, this means that when global-memory latency is high, a CUDA kernel must generate many independent memory requests so that enough bytes are in flight to hide latency and keep DRAM bandwidth saturated.\n",
    "\n",
    "More information about this can be found in the [CUDA Techniques to maximize memory bandwidth and hide latency](https://www.nvidia.com/en-us/on-demand/session/gtc25-s72683/) GTC presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2cef2-d6ca-4951-8a08-2ed38e6b3345",
   "metadata": {},
   "source": [
    "### MathDx and cuBLASDx\n",
    "\n",
    "The [cuBLAS Device Extension](https://docs.nvidia.com/cuda/cublasdx/) library (**cuBLASDx**) gives kernel developers the flexibility to define GEMM operations in terms of shared memory tiles and compose these operations into their kernels. cuBLASDx is a part of MathDx, a **Device Extension** library suite, also containing:\n",
    "- cuSolverDx, for numerical solvers\n",
    "- cuFFTDx, for thread and block FFTs\n",
    "- cuRANDDx, for random number generation\n",
    "- nvCOMPDx, for data compression\n",
    "\n",
    "MathDx exposes functionality on all CUDA memory levels, ranging from global memory pipelines, through shared memory tile computations to per-thread in-register algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9004db-c2f0-43fc-b91c-03acb2825b4f",
   "metadata": {},
   "source": [
    "### GEMM kernel with cuBLASDx Pipeline API\n",
    "\n",
    "Some CUDA instructions require significant orchestration around them to function properly. This strongly relates to later NVIDIA Architectures (Hopper, Blackwell) and respective capabilities (`TMA`, `WGMMA`, `UTCMMA`). \n",
    "\n",
    "Kernel patterns used underneath to allow for greater overlap also do change, including producer-consumer waprgroups, barrier based multi-stage pipelines and decoupled epilogues. \n",
    "\n",
    "With time the amount of complexity in this surrounding logic and orchestration proved to be as big as cuBLASDx's per-tile `copy` and `execute` operations. The library's core goal is to allow for fusability of external operations while still allowing for best performance math primitive execution and thus `cuBLASDx Pipeline API` was created, exposing the entire GEMM pipeline with a one line call exposing the result of GEMM in registers and allowing to fuse pre-processing operations.\n",
    "\n",
    "cuBLASDx documentation offers a [short guide on using Pipeline API](https://docs.nvidia.com/cuda/cublasdx/using_pipelines.html) for GEMM computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e03607-0487-4b9f-850e-57c95b34486e",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f4554-eb6a-4e32-97af-425ca075979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1c/parameters.hpp.inc\n",
    "\n",
    "    // (gemm_m, gemm_n, gemm_k, alpha, beta)\n",
    "    std::vector<tutorial::gemm_problem_t> problems = {\n",
    "        {2048, 2048, 2048, 0.9, 1.1}\n",
    "    };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06e039-1311-41d8-8d9d-797012ba07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1c/cublasdx_config.hpp.inc\n",
    "\n",
    "constexpr int tile_m = 64;\n",
    "constexpr int tile_n = 64;\n",
    "constexpr int tile_k = 32;\n",
    "\n",
    "constexpr int block_dim = 256;\n",
    "\n",
    "// The first step is to define the tile-level GEMM operation to be performed. \n",
    "// This is accomplished by combining cuBLASDx operators to create a GEMM description.\n",
    "\n",
    "using BLAS =\n",
    "    decltype(cublasdx::Size<tile_m, tile_n, tile_k>() +       // Description: Shared Memory GEMM Size\n",
    "        cublasdx::Precision<double, double, double>() +       // Description: Input Precisions\n",
    "        cublasdx::Type<cublasdx::type::real>() +              // Description: Input number type (real / complex)\n",
    "        cublasdx::Function<cublasdx::function::MM>() +        // Description: BLAS function (MM - Matrix Multiplication)\n",
    "        cublasdx::Arrangement<arr_a, arr_b, arr_c>() +        // Description: Global Memory arrangement (row- or column-major)\n",
    "        cublasdx::Block() +                                   // Execution: per-tile operation level (CUDA threadblock)\n",
    "        cublasdx::BlockDim<block_dim>() +                     // Execution: CUDA threadblock size (1D, 2D or 3D) \n",
    "        cublasdx::StaticBlockDim() +                          // Performance: this kernel will not use more threads than specified\n",
    "        cublasdx::MaxAlignment() +                            // Performance: global and shared memory alignment is >= 16bytes\n",
    "        cublasdx::EnableInputStreaming() +                    // Performance: no per-element preprocessing needs to be used\n",
    "        cublasdx::SM<SM_VALUE, SM_MODIFIER_VALUE>() +         // Execution: run on SM (e.g. 89) with modifier (e.g. 89a)\n",
    "        cublasdx::WithPipeline());                            // Execution: this per-tile descriptor will be only used with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba6700-8fa4-4dbc-a631-265a8e4ba990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1c/pipeline_config.hpp.inc\n",
    "\n",
    "    // IMPORTANT: The pipeline description needs to be defined on host,\n",
    "    // because possible TMA initialization must happen through a driver call\n",
    "\n",
    "    // Pipeline depth discussed in a section above\n",
    "    constexpr int pipeline_depth      = 2;\n",
    "    // cuBLASDx will return a std::optional<device_pipeline>, depending on correctness of arguments\n",
    "    auto          opt_device_pipeline = cublasdx::suggest_device_pipeline<pipeline_depth, BLAS>(tensor_a, tensor_b);\n",
    "\n",
    "    if (not opt_device_pipeline) {\n",
    "        std::cout << \"Incorrect pipeline configuration, please ensure global tensors are divisible by tile\"\n",
    "                  << std::endl;\n",
    "        exit(1);\n",
    "    }\n",
    "    // The pipeline can be retrieved now\n",
    "    auto device_pipeline = opt_device_pipeline.value();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c2b74-0ba3-4743-8a5a-807a99d59f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1c/kernel.hpp.inc\n",
    "\n",
    "template<class BLAS, class TensorC, class DevicePipeline>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void kernel_1c_simple_pipelined_dgemm(double                                 alpha,\n",
    "                                          double                                 beta,\n",
    "                                          TensorC const                          tensor_c,\n",
    "                                          // IMPORTANT --> grid constant\n",
    "                                          __grid_constant__ DevicePipeline const device_pipeline) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "\n",
    "    auto tile_pipeline = device_pipeline.get_tile(smem, blockIdx.x, blockIdx.y);\n",
    "    auto tile_gmem_c   = cublasdx::get_tile(EXERCISE);\n",
    "\n",
    "    auto epilogue_functor = [&](auto& accumulator) {\n",
    "       // EXERCISE --> implement GEMM epilogue (C = alpha * D + beta * C)\n",
    "       // Possible approaches:\n",
    "       // - manually (axpby for loop)\n",
    "       // - cublasdx::axpby(alpha, fragment, beta, fragment);\n",
    "       // - accumulator.axpby(alpha, beta, gmem_tile)\n",
    "\n",
    "       // The following calls may or may not be necessary depending on chosen implementation\n",
    "       // auto register_result_tensor = accumulator.get_results();\n",
    "       // auto c_register_fragment = accumulator.make_partition_and_copy(tile_gmem_c);\n",
    "       // accumulator.partition_and_copy(TODO, tile_gmem_c);\n",
    "    };\n",
    "\n",
    "    tile_pipeline.execute(epilogue_functor);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2a6f8-3280-4d3b-a9e4-273a9ffd3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1c/kernel_config.hpp.inc\n",
    "\n",
    "    auto kernel = kernel_1c_simple_pipelined_dgemm<BLAS, CTensor, decltype(device_pipeline)>;\n",
    "    // Pipeline exposes pre-computed shared memory requirement that includes its own cache size\n",
    "    auto shared_memory_size = device_pipeline.buffer_size();\n",
    "    CUDA_CHECK_AND_EXIT(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, shared_memory_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71d9f3-470f-49ca-892d-33534f606877",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cmake --build ./build -t 1c_simple_pipelined_dgemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3163dc7-b3ed-4626-a63d-693a8b0a9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build/1c_simple_pipelined_dgemm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cc39e-1647-4702-bef5-9c466b204a37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a730d4b-70f1-488b-b8df-74fb5b391872",
   "metadata": {},
   "source": [
    "We will rewrite kernel now and recompile the solution. If you want to restart your exercise make sure you rewrite kernel back and recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9dfb2-8f00-4cc3-a7f2-4add1bc64620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/1c/kernel.hpp.inc\n",
    "\n",
    "template<class BLAS, class TensorC, class DevicePipeline>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void kernel_1c_simple_pipelined_dgemm(double                                 alpha,\n",
    "                                          double                                 beta,\n",
    "                                          TensorC const                          tensor_c,\n",
    "                                          // IMPORTANT --> grid constant\n",
    "                                          __grid_constant__ DevicePipeline const device_pipeline) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "\n",
    "    auto tile_pipeline = device_pipeline.get_tile(smem, blockIdx.x, blockIdx.y);\n",
    "    auto tile_gmem_c   = cublasdx::get_tile(tensor_c, BLAS::c_shape, blockIdx.x, blockIdx.y);\n",
    "\n",
    "    auto epilogue_functor = [&](auto& accumulator) {\n",
    "       accumulator.axpby(alpha, beta, tile_gmem_c);\n",
    "    };\n",
    "\n",
    "    tile_pipeline.execute(epilogue_functor);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211bfa4-d93e-4a56-9379-7b0c5b7f378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cmake --build ./build -t 1c_simple_pipelined_dgemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606b327-3451-49ae-94ba-3032ff09a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build/1c_simple_pipelined_dgemm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e349e-9bc3-4a4f-8dcc-2d31fb27a39d",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0a133-c73e-4ca6-a04c-9aade04a0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problems that we will benchmark and conduct accuracy tests on the tuple should be formed as:\n",
    "# (GEMM_M, GEMM_N, GEMM_K, ALPHA, BETA)\n",
    "problems = [\n",
    "  (2048, 2048, 2048, 1.0, 1.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af23c55-d728-4a16-bdfe-75e0620a3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_kernel_params_2_3(m, n, k, alpha, beta):\n",
    "    TILE_M = 64\n",
    "    TILE_N = 64\n",
    "    TILE_K = 32\n",
    "\n",
    "    BLOCK_SIZE = 256\n",
    "    \n",
    "    # The first step is to define the tile-level GEMM operation to be performed. \n",
    "    # This is accomplished by combining cuBLASDx operators to create a GEMM description.\n",
    "\n",
    "    return Matmul(             \n",
    "        size=(TILE_M, TILE_N, TILE_K),                        # Description: Shared Memory GEMM Size\n",
    "        precision=(np.float64, np.float64, np.float64),       # Description: Input Precisions\n",
    "        data_type=\"real\",                                     # Description: Input number type (real / complex)\n",
    "        alignment=MAX_ALIGNMENT,                              # Performance: global and shared memory alignment\n",
    "        arrangement=(\"row_major\", \"col_major\", \"col_major\"),  # Description: Global Memory arrangement (row- or column-major)\n",
    "        execution=\"Block\",                                    # Execution: per-tile operation level (CUDA threadblock)\n",
    "        block_size=BLOCK_SIZE,                                # Execution: CUDA threadblock size (1D, 2D or 3D) \n",
    "        with_pipeline=True,                                   # Execution: this per-tile descriptor will be only used with pipeline\n",
    "        enable_input_streaming=True,                          # Performance: no per-element preprocessing needs to be used\n",
    "        static_block_dim=True,                                # Performance: this kernel will not use more threads than specified\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b72bcb-9265-4fc3-a54f-0bf580ac1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_args_2_3(BLAS, alpha, tensor_a, tensor_b, beta, tensor_c):\n",
    "    # IMPORTANT: The pipeline description needs to be defined on host,\n",
    "    # because possible TMA initialization must happen through a driver call\n",
    "\n",
    "    # Pipeline depth discussed in a section above\n",
    "    PIPELINE_DEPTH = 2\n",
    "\n",
    "    TILE_K = BLAS.a_dim[1]\n",
    "    _, k = tensor_a.shape\n",
    "\n",
    "    assert k >= PIPELINE_DEPTH * TILE_K, \"The user provided value for K is too small for the pipeline depth\"\n",
    "    \n",
    "    device_pipeline = BLAS.suggest_device_pipeline(PIPELINE_DEPTH, tensor_a, tensor_b)\n",
    "    return alpha, beta, tensor_c, device_pipeline\n",
    "\n",
    "def get_shared_memory_size_2_3(BLAS, kernel_args):\n",
    "    device_pipeline = kernel_args[-1]\n",
    "    # Pipeline exposes pre-computed shared memory requirement that includes its own cache size\n",
    "    return device_pipeline.buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1543db-5a59-4388-887c-bb84d27cab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgemm_kernel_2_3(BLAS):\n",
    "\n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    tile_m, tile_n = BLAS.c_dim\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLAS.block_size, 1))\n",
    "    def dgemm_kernel(alpha, beta, tensor_c, device_pipeline: DevicePipeline):\n",
    "        m, n = tensor_c.shape\n",
    "\n",
    "        ldc = max(tensor_c.strides) // tensor_c.itemsize\n",
    "\n",
    "        block_m = cuda.blockIdx.x\n",
    "        block_n = cuda.blockIdx.y\n",
    "\n",
    "        smem = cuda.shared.array(shape=(0,), dtype=BLAS.a_value_type, alignment=device_pipeline.buffer_alignment)\n",
    "\n",
    "        block_start_m = block_m * tile_m\n",
    "        block_end_m = (block_m + 1) * tile_m\n",
    "\n",
    "        block_start_n = block_n * tile_n\n",
    "        block_end_n = (block_n + 1) * tile_n\n",
    "\n",
    "        if block_start_m >= m or block_start_n >= n:\n",
    "            return\n",
    "        \n",
    "        c_view = tensor_c[\n",
    "            block_start_m : block_end_m,\n",
    "            block_start_n : block_end_n,\n",
    "        ]\n",
    "\n",
    "        gmem_c = make_tensor(c_view, BLAS.get_layout_gmem_c(ldc))\n",
    "        \n",
    "        tile_pipeline = device_pipeline.get_tile(smem, block_m, block_n)\n",
    "        \n",
    "        accumulator = BLAS.suggest_accumulator()\n",
    "        tile_pipeline.execute(accumulator)\n",
    "\n",
    "        #if accumulator.is_thread_active():\n",
    "            # EXERCISE --> implement GEMM epilogue (C = alpha * D + beta * C)\n",
    "            # Possible approaches:\n",
    "            # - manually (axpby for loop)\n",
    "            # - axpby(alpha, fragment, beta, fragment)\n",
    "\n",
    "            # The following calls may or may not be necessary depending on chosen implementation\n",
    "            # register_result_tensor = accumulator.get_results();\n",
    "            # c_register_fragment = accumulator.make_partition_and_copy(tile_gmem_c)\n",
    "            # accumulator.partition_and_copy(TODO, tile_gmem_c)\n",
    "            \n",
    "        tile_pipeline._del()\n",
    "\n",
    "    return dgemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaf638-ef91-40aa-984b-0910803f876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dgemm_2_3(problems, get_dgemm_kernel_2_3, choose_kernel_params_2_3, get_shared_memory_size_2_3, get_kernel_args_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed664f54-736e-4ed7-bfcc-1ed534aa3aae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a548188-d9e2-4c31-90ce-0344ec656075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgemm_kernel_2_3_solution(BLAS):\n",
    "\n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    tile_m, tile_n = BLAS.c_dim\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLAS.block_size, 1))\n",
    "    def dgemm_kernel(alpha, beta, tensor_c, device_pipeline: DevicePipeline):\n",
    "        m, n = tensor_c.shape\n",
    "\n",
    "        ldc = max(tensor_c.strides) // tensor_c.itemsize\n",
    "\n",
    "        block_m = cuda.blockIdx.x\n",
    "        block_n = cuda.blockIdx.y\n",
    "\n",
    "        smem = cuda.shared.array(shape=(0,), dtype=BLAS.c_value_type, alignment=device_pipeline.buffer_alignment)\n",
    "\n",
    "        block_start_m = block_m * tile_m\n",
    "        block_end_m = (block_m + 1) * tile_m\n",
    "\n",
    "        block_start_n = block_n * tile_n\n",
    "        block_end_n = (block_n + 1) * tile_n\n",
    "\n",
    "        if block_start_m >= m or block_start_n >= n:\n",
    "            return\n",
    "        \n",
    "        c_view = tensor_c[\n",
    "            block_start_m : block_end_m,\n",
    "            block_start_n : block_end_n,\n",
    "        ]\n",
    "\n",
    "        gmem_c = make_tensor(c_view, BLAS.get_layout_gmem_c(ldc))\n",
    "        \n",
    "        tile_pipeline = device_pipeline.get_tile(smem, block_m, block_n)\n",
    "        \n",
    "        accumulator = BLAS.suggest_accumulator()\n",
    "        tile_pipeline.execute(accumulator)\n",
    "\n",
    "        if accumulator.is_thread_active():\n",
    "            c_frag = accumulator.make_partition_and_copy(gmem_c)\n",
    "            axpby(alpha, accumulator.get_results(), beta, c_frag)\n",
    "            accumulator.partition_and_copy(c_frag, gmem_c)\n",
    "\n",
    "        tile_pipeline._del()\n",
    "\n",
    "    return dgemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad1d70-e9fe-4fb1-aaea-667c9ef8e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dgemm_2_3(problems, get_dgemm_kernel_2_3_solution, choose_kernel_params_2_3, get_shared_memory_size_2_3, get_kernel_args_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79e622-f14a-4043-bd30-974c06b2fe2d",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb28434-a0ab-4cb4-b448-780a516c586d",
   "metadata": {},
   "source": [
    "In this notebook, we have learned:\n",
    "\n",
    "1. What a GEMM is and how to implement a naive GEMM kernel\n",
    "2. How to analyze our implementations and make intelligent choices about what to optimize next\n",
    "3. What shared memory is and why it is critical for performance\n",
    "\n",
    "and then we have progressed to offloading all these aspects onto the cuBLASDx pipeline API, understanding:\n",
    "1. What is pipelining and when is it necessary\n",
    "2. How to define cuBLASDx per-tile description and make a pipeline based on it\n",
    "3. How to do in-kernel epilogue fusion with pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
