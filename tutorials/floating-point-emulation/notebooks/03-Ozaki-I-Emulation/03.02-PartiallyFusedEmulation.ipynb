{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24069346-2c9c-4d6b-b812-d59f2804e36c",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Optimizing the Ozaki-I Scheme with kernel fusion\n",
    "\n",
    "Now that we have a working Ozaki-I implementation, we'd like to start fusing the epilogue function into our emulated GEMM kernel. This will help reduce the global memory traffic.\n",
    "\n",
    "In this exercise, we will start this by computing all slice products and accumulate the anti-diagonals in one kernel:\n",
    "\n",
    "<img src=\"Images/Ozaki-I-Multiplications-Partial-Fusion.png\" width=\"800\" height=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407f8fb-09df-4f4e-8caa-e1c348ecf307",
   "metadata": {},
   "source": [
    "### C++ Cmake Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323a0a4-ca7d-4069-b855-328bfede2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "from common_cuda import setup_cmake_project\n",
    "setup_cmake_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c4e89-5b9e-4783-a904-f93aa5b07055",
   "metadata": {},
   "source": [
    "### Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e19123-19f9-42cb-a05c-a635497e834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import nvmath\n",
    "\n",
    "from nvmath.device import Matmul\n",
    "from nvmath.device.cublasdx import DevicePipeline, SharedStorageCalc, MAX_ALIGNMENT\n",
    "from nvmath.device.cublasdx_numba import pipeline_extensions\n",
    "from nvmath.device.common import axpby, clear, copy, copy_fragment, copy_wait, make_tensor\n",
    "from numba import cuda\n",
    "\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "\n",
    "from benchmark import *\n",
    "from emulation_utils import get_width, epilogue_ldexp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b869c3-d014-4001-9673-612dc6c35de7",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac7a49-07ab-4337-979c-aa9e121902f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/parameters.hpp.inc\n",
    "\n",
    "    // ===================================\n",
    "    // Problem configuration\n",
    "    // ===================================\n",
    "\n",
    "    // (gemm_m, gemm_n, gemm_k, alpha, beta)\n",
    "    std::vector<tutorial::gemm_problem_t> problems = {\n",
    "        {2048, 2048, 2048, 0.9, 1.1}\n",
    "    };\n",
    "    \n",
    "\n",
    "    // ===================================\n",
    "    // Global GEMM configuration\n",
    "    // ===================================\n",
    "\n",
    "    // The number of slices used in emulation algorithm\n",
    "    // More slices = higher precision but more computation\n",
    "    constexpr unsigned slices = 7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9545479-9d2f-4130-b410-173f9cf87943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/cublasdx_config.hpp.inc\n",
    "\n",
    "    using slice_value_type       = int8_t;  // Precision for individual slices\n",
    "    using accumulator_value_type = int32_t; // Precision for accumulation\n",
    "\n",
    "    // The shape of data tile processed by a single CTA block\n",
    "    constexpr int tile_m = 128;\n",
    "    constexpr int tile_n = 128;\n",
    "    constexpr int tile_k = 128;\n",
    "\n",
    "    // The shape of CTA block (number of threads)\n",
    "    constexpr int cta_shape_x = 128;\n",
    "    constexpr int cta_shape_y = 1;\n",
    "    constexpr int cta_shape_z = 1;\n",
    "\n",
    "    using BLAS = decltype(cublasdx::Size<tile_m, tile_n, tile_k>() +\n",
    "                          cublasdx::Precision<slice_value_type, slice_value_type, accumulator_value_type>() +\n",
    "                          cublasdx::Type<cublasdx::type::real>() + cublasdx::Function<cublasdx::function::MM>() +\n",
    "                          cublasdx::Arrangement<arrangement_a, arrangement_b, arrangement_c>() + cublasdx::Block() +\n",
    "                          cublasdx::BlockDim<cta_shape_x, cta_shape_y, cta_shape_z>() + cublasdx::StaticBlockDim() +\n",
    "                          cublasdx::WithPipeline() + cublasdx::MaxAlignment() + cublasdx::EnableInputStreaming() +\n",
    "                          cublasdx::SM<SM_VALUE, SM_MODIFIER_VALUE>());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a0f84-9477-4294-b59b-f4d806411cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/pipeline_config.hpp.inc\n",
    "\n",
    "        constexpr int pipeline_depth = 3;\n",
    "        auto device_pipeline = cublasdx::suggest_device_pipeline<pipeline_depth, BLAS, cublasdx::external_accumulation>(\n",
    "                                   tensor_slice_a, tensor_slice_b)\n",
    "                                   .value();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013bf66-8ad0-441d-b86e-694588047e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/fused_kernel.hpp.inc\n",
    "\n",
    "template<int Slices, class BLAS, class DevicePipeline, class SliceProductTensor>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void fused_epilogue_kernel(__grid_constant__ DevicePipeline const device_pipeline,\n",
    "                               SliceProductTensor                     slice_product_tensor) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "#ifdef __CUDA_ARCH__\n",
    "    /* \n",
    "     * EXERCISE --> Complete the kernel to compute all products and accumulate along diagonals in the same kernel\n",
    "     */\n",
    "\n",
    "    if constexpr (cublasdx::sm_of_v<BLAS> == __CUDA_ARCH__) {\n",
    "        // ================================\n",
    "        // 1. SETUP AND TILE PREPARATION\n",
    "        // ================================\n",
    "\n",
    "        // EXERCISE --> Choose your starting diagonal and term along the diagonal\n",
    "        constexpr auto initial_diag = ;\n",
    "        constexpr auto initial_term = ;\n",
    "\n",
    "        // Get pipeline tile\n",
    "        auto tile_pipeline = device_pipeline.get_tile(\n",
    "            smem, cublasdx::make_coord(blockIdx.x, initial_term), cublasdx::make_coord(blockIdx.y, initial_diag));\n",
    "\n",
    "        auto accumulator = tile_pipeline.get_accumulator();\n",
    "\n",
    "        // ============================================\n",
    "        // 2. OZAKI SCHEME DIAGONAL ITERATION\n",
    "        // ============================================\n",
    "#    pragma unroll 1\n",
    "        for (int diag = initial_diag; /* EXERCISE --> for loop over diagonals */) {\n",
    "\n",
    "            // Initialize accumulator for this diagonal\n",
    "            accumulator.clear();\n",
    "\n",
    "            // ==========================================\n",
    "            // 3. SLICE COMBINATION COMPUTATION\n",
    "            // ==========================================\n",
    "#    pragma unroll 1\n",
    "            for (int term = initial_term; /* EXERCISE --> for loop to iterate along the diagonal */) {\n",
    "                // =========================================\n",
    "                // 4. N-STAGE MEMORY PIPELINE FOR GEMM\n",
    "                // =========================================\n",
    "\n",
    "                tile_pipeline.execute(accumulator);\n",
    "\n",
    "                // EXERCISE --> Determine which slice of A and slice of B to multiply\n",
    "                const auto next_slice_row = ;\n",
    "                const auto next_slice_col = ;\n",
    "                device_pipeline.reset_tile(tile_pipeline,\n",
    "                                           cublasdx::make_coord(blockIdx.x, next_slice_row),\n",
    "                                           cublasdx::make_coord(blockIdx.y, next_slice_col));\n",
    "            }\n",
    "\n",
    "            // ========================================\n",
    "            // 5. RESULT RECONSTRUCTION AND EPILOGUE\n",
    "            // ========================================\n",
    "\n",
    "            if (accumulator.is_thread_active()) {\n",
    "                // Choose output tensor for this slice iteration\n",
    "                auto this_slice_output = slice_product_tensor(cublasdx::slice, cublasdx::slice, diag);\n",
    "                // Get output tile for this block\n",
    "                auto slice_output_tile = cublasdx::get_tile(this_slice_output, BLAS::c_shape, blockIdx.x, blockIdx.y);\n",
    "                // Store results\n",
    "                accumulator.partition_and_store(slice_output_tile);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "#endif\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67d564-a5a2-4ba7-bc7c-3f983b12183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/epilogue_config.hpp.inc\n",
    "\n",
    "        constexpr int epilogue_kernel_tile_m = 16;\n",
    "        constexpr int epilogue_kernel_tile_n = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f3a54-bab2-45e1-9459-2570b8d65b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/epilogue_kernel.hpp.inc\n",
    "\n",
    "template<int BlockSize, int Slices, class ProductTensor, class ShiftTensorA, class ShiftTensorB, class OutTensor>\n",
    "__launch_bounds__(BlockSize, 1) __global__ void epilogue_kernel(double        alpha,\n",
    "                                                                double        beta,\n",
    "                                                                ProductTensor product_tensor,\n",
    "                                                                ShiftTensorA  shift_tensor_a,\n",
    "                                                                ShiftTensorB  shift_tensor_b,\n",
    "                                                                OutTensor     out_tensor) {\n",
    "    using product_datatype = tutorial::tensor_value_type_t<ProductTensor>;\n",
    "    using shift_datatype   = tutorial::tensor_value_type_t<ShiftTensorA>;\n",
    "    using out_datatype     = tutorial::tensor_value_type_t<OutTensor>;\n",
    "\n",
    "    const auto tid_m = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    const auto tid_n = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    int shift_a = shift_tensor_a(tid_m);\n",
    "    int shift_b = shift_tensor_b(tid_n);\n",
    "\n",
    "    /*\n",
    "     * EXERCISE --> Complete the implementation of the epilogue kernel\n",
    "     */\n",
    "    #pragma unroll\n",
    "    for (/* for loop over diagonals */) {\n",
    "        product_datatype diag_acc = product_tensor(tid_m, tid_n, diag);\n",
    "        result += nth_slice_to_fp64<int32_t, int8_t>(diag, diag_acc, shift_a + shift_b);\n",
    "    }\n",
    "\n",
    "    out_tensor(tid_m, tid_n) = alpha * result + beta * out_tensor(tid_m, tid_n);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f8cd9-cc63-44e0-90fd-abd1b81abdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build ./build -t 2b_partially_fused_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdaa3e0-a583-4f99-926f-5ddc45c15a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/2b_partially_fused_emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24835933-3e6d-4cdd-b2b2-4fa9796d9a1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f67f87-240b-441a-b267-8a6c07b9a569",
   "metadata": {},
   "source": [
    "We will rewrite kernel now and recompile the solution. If you want to restart your exercise make sure you rewrite kernel back and recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa1426-9438-43ec-843d-2f086d31ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/fused_kernel.hpp.inc\n",
    "\n",
    "template<int Slices, class BLAS, class DevicePipeline, class SliceProductTensor>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void fused_epilogue_kernel(__grid_constant__ DevicePipeline const device_pipeline,\n",
    "                               SliceProductTensor                     slice_product_tensor) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "#ifdef __CUDA_ARCH__\n",
    "    if constexpr (cublasdx::sm_of_v<BLAS> == __CUDA_ARCH__) {\n",
    "        // ================================\n",
    "        // 1. SETUP AND TILE PREPARATION\n",
    "        // ================================\n",
    "\n",
    "        constexpr auto initial_diag = Slices - 1;\n",
    "        constexpr auto initial_term = 0;\n",
    "\n",
    "        // Get pipeline tile\n",
    "        auto tile_pipeline = device_pipeline.get_tile(\n",
    "            smem, cublasdx::make_coord(blockIdx.x, initial_term), cublasdx::make_coord(blockIdx.y, initial_diag));\n",
    "\n",
    "        auto accumulator = tile_pipeline.get_accumulator();\n",
    "\n",
    "        // ============================================\n",
    "        // 2. OZAKI SCHEME DIAGONAL ITERATION\n",
    "        // ============================================\n",
    "\n",
    "        // Iterate over diagonals in reverse order (highest power of 2 first)\n",
    "        // This ensures proper accumulation order for numerical stability\n",
    "#    pragma unroll 1\n",
    "        for (auto diag = initial_diag; diag >= 0; --diag) {\n",
    "\n",
    "            // Initialize accumulator for this diagonal\n",
    "            accumulator.clear();\n",
    "\n",
    "            // ==========================================\n",
    "            // 3. SLICE COMBINATION COMPUTATION\n",
    "            // ==========================================\n",
    "\n",
    "            // Compute all slice combinations that contribute to this diagonal\n",
    "            // For diagonal d, we compute: A_slice[i] * B_slice[d-i] for i = 0 to d\n",
    "#    pragma unroll 1\n",
    "            for (auto term = initial_term; term <= diag; ++term) {\n",
    "                // =========================================\n",
    "                // 4. N-STAGE MEMORY PIPELINE FOR GEMM\n",
    "                // =========================================\n",
    "\n",
    "                tile_pipeline.execute(accumulator);\n",
    "\n",
    "                const auto next_slice_row = (term == diag) ? 0 : term + 1;                         // A slice index\n",
    "                const auto next_slice_col = (term == diag) ? (diag - 1) : (diag - next_slice_row); // B slice index\n",
    "                device_pipeline.reset_tile(tile_pipeline,\n",
    "                                           cublasdx::make_coord(blockIdx.x, next_slice_row),\n",
    "                                           cublasdx::make_coord(blockIdx.y, next_slice_col));\n",
    "            } /* end of slice combination loop */\n",
    "\n",
    "            // ========================================\n",
    "            // 5. RESULT RECONSTRUCTION AND EPILOGUE\n",
    "            // ========================================\n",
    "\n",
    "            if (accumulator.is_thread_active()) {\n",
    "                // Choose output tensor for this slice iteration\n",
    "                auto this_slice_output = slice_product_tensor(cublasdx::slice, cublasdx::slice, diag);\n",
    "                // Get output tile for this block\n",
    "                auto slice_output_tile = cublasdx::get_tile(this_slice_output, BLAS::c_shape, blockIdx.x, blockIdx.y);\n",
    "                // Store results\n",
    "                accumulator.partition_and_store(slice_output_tile);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "#endif\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facdcf3-6781-4990-ab5b-74577daf9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2b_partially_fused_emulation/epilogue_kernel.hpp.inc\n",
    "\n",
    "template<int BlockSize, int Slices, class ProductTensor, class ShiftTensorA, class ShiftTensorB, class OutTensor>\n",
    "__launch_bounds__(BlockSize, 1) __global__ void epilogue_kernel(double        alpha,\n",
    "                                                                double        beta,\n",
    "                                                                ProductTensor product_tensor,\n",
    "                                                                ShiftTensorA  shift_tensor_a,\n",
    "                                                                ShiftTensorB  shift_tensor_b,\n",
    "                                                                OutTensor     out_tensor) {\n",
    "    using product_datatype = tutorial::tensor_value_type_t<ProductTensor>;\n",
    "    using shift_datatype   = tutorial::tensor_value_type_t<ShiftTensorA>;\n",
    "    using out_datatype     = tutorial::tensor_value_type_t<OutTensor>;\n",
    "\n",
    "    const auto tid_m = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    const auto tid_n = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    int shift_a = shift_tensor_a(tid_m);\n",
    "    int shift_b = shift_tensor_b(tid_n);\n",
    "\n",
    "    auto product_view = product_tensor(tid_m, tid_n, cublasdx::slice);\n",
    "\n",
    "    double result = 0.0;\n",
    "\n",
    "#pragma unroll\n",
    "    for (auto diag = Slices-1; diag >= 0; diag--) {\n",
    "        product_datatype diag_acc = product_tensor(tid_m, tid_n, diag);\n",
    "        result += nth_slice_to_fp64<int32_t, int8_t>(diag, diag_acc, shift_a + shift_b);\n",
    "    }\n",
    "\n",
    "    out_tensor(tid_m, tid_n) = alpha * result + beta * out_tensor(tid_m, tid_n);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dc237-0b70-4489-93da-50d19b77c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build ./build -t 2b_partially_fused_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f2234-8522-44da-935f-57825c4a85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/2b_partially_fused_emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797408b-db37-4676-9b5f-ae982a5f0165",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f340d7f-c138-4a09-a0f5-27b197a13641",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "  (2048, 2048, 2048, 0.9, 1.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38624e49-3518-43e7-900b-9f84ee91a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emulated_gemm_kernel(BLAS):\n",
    "\n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    tile_m, tile_n = BLAS.c_dim\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLAS.block_size, 1))\n",
    "    def gemm_kernel(tensor_c, device_pipeline: DevicePipeline):\n",
    "        _, _, slices = tensor_c.shape\n",
    "\n",
    "        block_m = cuda.blockIdx.x\n",
    "        block_n = cuda.blockIdx.y\n",
    "\n",
    "        smem = cuda.shared.array(shape=(0,), dtype=BLAS.a_value_type, alignment=device_pipeline.buffer_alignment)\n",
    "\n",
    "        block_start_m = block_m * tile_m\n",
    "        block_end_m = (block_m + 1) * tile_m\n",
    "\n",
    "        block_start_n = block_n * tile_n\n",
    "        block_end_n = (block_n + 1) * tile_n\n",
    " \n",
    "        # EXERCISE --> Complete the kernel to compute all products and accumulate along diagonals in the same kernel\n",
    "\n",
    "        # ================================\n",
    "        # 1. SETUP AND TILE PREPARATION\n",
    "        # ================================\n",
    "\n",
    "        # EXERCISE --> Choose your starting diagonal and term along the diagonal\n",
    "        initial_diag = -1\n",
    "        initial_term = -1\n",
    "\n",
    "        # Get pipeline tile\n",
    "        tile_pipeline = device_pipeline.get_tile(smem,\n",
    "                                                 (block_m, np.int32(initial_term)),\n",
    "                                                 (block_n, np.int32(initial_diag)))\n",
    "        \n",
    "        accumulator = BLAS.suggest_accumulator()\n",
    "\n",
    "        c_views = tensor_c[\n",
    "            block_start_m : block_end_m,\n",
    "            block_start_n : block_end_n,\n",
    "            :\n",
    "        ]\n",
    "        ldc = max(c_views.strides[:2]) // c_views.itemsize\n",
    "        \n",
    "        # ============================================\n",
    "        # 2. OZAKI SCHEME DIAGONAL ITERATION\n",
    "        # ============================================\n",
    "        for diag in range(-1): # EXERCISE --> for loop over diagonals\n",
    "\n",
    "            # Initialize accumulator for this diagonal\n",
    "            accumulator.clear()\n",
    "\n",
    "            # ==========================================\n",
    "            # 3. SLICE COMBINATION COMPUTATION\n",
    "            # ==========================================\n",
    "            for term in range(-1): # EXERCISE --> for loop to iterate along the diagonal\n",
    "                # =========================================\n",
    "                # 4. N-STAGE MEMORY PIPELINE FOR GEMM\n",
    "                # =========================================\n",
    "                tile_pipeline.execute(accumulator)\n",
    "\n",
    "                # EXERCISE --> Determine which slice of A and slice of B to multiply\n",
    "                next_slice_row = -1\n",
    "                next_slice_col = -1\n",
    "\n",
    "                device_pipeline.reset_tile(tile_pipeline,\n",
    "                                           (block_m, np.int32(next_slice_row)),\n",
    "                                           (block_n, np.int32(next_slice_col)))\n",
    "\n",
    "            # ========================================\n",
    "            # 5. RESULT RECONSTRUCTION AND EPILOGUE\n",
    "            # ========================================\n",
    "            if accumulator.is_thread_active():\n",
    "                gmem_c = make_tensor(c_views[:,:,diag], BLAS.get_layout_gmem_c(ldc))\n",
    "                accumulator.partition_and_copy(accumulator.get_results(), gmem_c)\n",
    "\n",
    "        # tile_pipeline._del()\n",
    "\n",
    "    return gemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc17a5c-d199-4979-8e16-309eebc31824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fused_dgemm_ozaki(tensor_slicedA_cupy, tensor_slicedB_cupy, tensor_diag_cupy, context, warmup=True):\n",
    "    BLAS = context[\"BLAS\"]\n",
    "    pipeline_depth = context[\"PIPELINE_DEPTH\"]\n",
    "    gemm_kernel = context[\"gemm_kernel\"]\n",
    "    grid = context[\"gemm_grid\"]\n",
    "    block = context[\"gemm_block\"]\n",
    "\n",
    "    tensor_slicedA = cuda.as_cuda_array(tensor_slicedA_cupy)\n",
    "    tensor_slicedB = cuda.as_cuda_array(tensor_slicedB_cupy)\n",
    "    tensor_diag = cuda.as_cuda_array(tensor_diag_cupy)\n",
    "\n",
    "    device_pipeline = BLAS.suggest_device_pipeline(pipeline_depth, tensor_slicedA, tensor_slicedB)\n",
    "\n",
    "    if warmup:\n",
    "        set_max_dynamic_shared_size_bytes(gemm_kernel, device_pipeline.buffer_size,\n",
    "                                            tensor_diag, device_pipeline)\n",
    "    gemm_kernel[grid, block, 0, device_pipeline.buffer_size](tensor_diag, device_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830fd6c-c727-47e2-870e-e94932633e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epilogue_kernel(block_size=64):\n",
    "    uint8_width = get_width(np.uint8)\n",
    "\n",
    "    @cuda.jit(device=True, forceinline=True)\n",
    "    def nth_slice_to_fp64(nth, nth_slice, exponent_shift):\n",
    "        ko = math.pow(2.0, -nth * uint8_width)\n",
    "\n",
    "        value = ko * np.float64(nth_slice)\n",
    "        return epilogue_ldexp(value, -exponent_shift)\n",
    "\n",
    "    @cuda.jit(launch_bounds=(block_size, 1))\n",
    "    def epilogue_kernel(slices, tensor_diag, tensor_shift_a, tensor_shift_b, tensor_out, alpha, beta):\n",
    "        tid_m = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        tid_n = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        if tid_m >= tensor_out.shape[0] or tid_n >= tensor_out.shape[1]:\n",
    "            return\n",
    "\n",
    "        shift_a = tensor_shift_a[tid_m]\n",
    "        shift_b = tensor_shift_b[tid_n]\n",
    "        \n",
    "        # EXERCISE --> Complete the implementation of the epilogue kernel\n",
    "        diag_view = tensor_diag[tid_m, tid_n, :]\n",
    "\n",
    "        result = 0.0\n",
    "        for diag in range(-1): # EXERCISE --> loop over diagonals\n",
    "            result += nth_slice_to_fp64(diag, diag_view[diag], shift_a + shift_b)\n",
    "\n",
    "        tensor_out[tid_m, tid_n] = alpha * result + beta * tensor_out[tid_m, tid_n]\n",
    "\n",
    "    return epilogue_kernel\n",
    "\n",
    "def epilogue(slices, tensor_products, tensor_shift_a, tensor_shift_b, tensor_c, alpha, beta, context):\n",
    "    epilogue_kernel = context[\"epilogue_kernel\"]\n",
    "    \n",
    "    grid = context[\"epilogue_grid\"]\n",
    "    block = context[\"epilogue_block\"]\n",
    "\n",
    "    epilogue_kernel[grid, block](slices, tensor_products, tensor_shift_a, tensor_shift_b, tensor_c, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242e285-16c3-48f8-b59d-486ba36e2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_func(m, n, k):\n",
    "    tile_m = 128\n",
    "    tile_n = 128\n",
    "    tile_k = 128\n",
    "    block_size = 128\n",
    "    \n",
    "    pipeline_depth = 3\n",
    "\n",
    "    epilogue_tile_m = 16\n",
    "    epilogue_tile_n = 16\n",
    "\n",
    "    assert m % tile_m == 0, \"Unsupported dimension m for TILE_M\"\n",
    "    assert n % tile_n == 0, \"Unsupported dimension n for TILE_N\"\n",
    "    assert k % tile_k == 0, \"Unsupported dimension n for TILE_N\"\n",
    "    assert k >= (tile_k * pipeline_depth), \"Unsupported pipeline depth for k\"\n",
    "\n",
    "    assert m % epilogue_tile_m == 0, \"Unsupported dimension for EPILOGUE_TILE_M\"\n",
    "    assert n % epilogue_tile_n == 0, \"Unsupported dimension for EPILOGUE_TILE_N\"\n",
    "    \n",
    "    BLAS = Matmul(size=(tile_m, tile_n, tile_k),\n",
    "                  precision=(np.int8, np.int8, np.int32),\n",
    "                  data_type=\"real\",\n",
    "                  alignment=MAX_ALIGNMENT,\n",
    "                  arrangement=(\"row_major\", \"col_major\", \"col_major\"), # Do not change\n",
    "                  execution=\"Block\",\n",
    "                  block_size=block_size,\n",
    "                  with_pipeline=True,\n",
    "                  enable_input_streaming=True,\n",
    "                  static_block_dim=True)\n",
    "\n",
    "    gemm_grid = (m // tile_m, n // tile_n)\n",
    "    gemm_block = BLAS.block_dim\n",
    "\n",
    "    epilogue_grid = (m // epilogue_tile_m, n // epilogue_tile_n)\n",
    "    epilogue_block = (epilogue_tile_m, epilogue_tile_n)\n",
    "\n",
    "    return {\n",
    "        \"BLAS\": BLAS,\n",
    "        \"PIPELINE_DEPTH\": pipeline_depth,\n",
    "        \"gemm_kernel\" : get_emulated_gemm_kernel(BLAS),\n",
    "        \"gemm_grid\": gemm_grid,\n",
    "        \"gemm_block\": gemm_block,\n",
    "        \"epilogue_kernel\": get_epilogue_kernel(math.prod(epilogue_block)),\n",
    "        \"epilogue_grid\": epilogue_grid,\n",
    "        \"epilogue_block\": epilogue_block\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e360d9-4015-4381-9e9f-c4f4ffddc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_partially_fused_emulated_dgemm(problems, setup_func, partial_fused_dgemm_ozaki, epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c366a1-54a9-4bf1-8453-952b0c59a7fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7849d-b608-40ac-a2d3-0662acf82af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emulated_gemm_kernel_solution(BLAS):\n",
    "\n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    tile_m, tile_n = BLAS.c_dim\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLAS.block_size, 1))\n",
    "    def gemm_kernel(tensor_c, device_pipeline: DevicePipeline):\n",
    "        _, _, slices = tensor_c.shape\n",
    "\n",
    "        block_m = cuda.blockIdx.x\n",
    "        block_n = cuda.blockIdx.y\n",
    "\n",
    "        smem = cuda.shared.array(shape=(0,), dtype=BLAS.a_value_type, alignment=device_pipeline.buffer_alignment)\n",
    "\n",
    "        block_start_m = block_m * tile_m\n",
    "        block_end_m = (block_m + 1) * tile_m\n",
    "\n",
    "        block_start_n = block_n * tile_n\n",
    "        block_end_n = (block_n + 1) * tile_n\n",
    "\n",
    "        initial_diag = slices - 1\n",
    "        initial_term = 0\n",
    "\n",
    "        tile_pipeline = device_pipeline.get_tile(smem,\n",
    "                                                 (block_m, np.int32(initial_term)),\n",
    "                                                 (block_n, np.int32(initial_diag)))\n",
    "\n",
    "        c_views = tensor_c[\n",
    "            block_start_m : block_end_m,\n",
    "            block_start_n : block_end_n,\n",
    "            :\n",
    "        ]\n",
    "        ldc = max(c_views.strides[:2]) // c_views.itemsize\n",
    "        \n",
    "        accumulator = BLAS.suggest_accumulator()\n",
    "        for diag in range(initial_diag, -1, -1):\n",
    "            accumulator.clear()\n",
    "\n",
    "            for term in range(initial_term, diag + 1):\n",
    "                tile_pipeline.execute(accumulator)\n",
    "\n",
    "                next_slice_row =          0 if term == diag else term + 1\n",
    "                next_slice_col = (diag - 1) if term == diag else diag - next_slice_row\n",
    "\n",
    "                device_pipeline.reset_tile(tile_pipeline,\n",
    "                                           (block_m, np.int32(next_slice_row)),\n",
    "                                           (block_n, np.int32(next_slice_col)))\n",
    "\n",
    "            if accumulator.is_thread_active():\n",
    "                gmem_c = make_tensor(c_views[:,:,diag], BLAS.get_layout_gmem_c(ldc))\n",
    "                accumulator.partition_and_copy(accumulator.get_results(), gmem_c)\n",
    "\n",
    "        tile_pipeline._del()\n",
    "\n",
    "    return gemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b39a1-6ad0-46bf-b8a0-190893ebd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epilogue_kernel_solution(block_size=64):\n",
    "    uint8_width = get_width(np.uint8)\n",
    "\n",
    "    @cuda.jit(device=True, forceinline=True)\n",
    "    def nth_slice_to_fp64(nth, nth_slice, exponent_shift):\n",
    "        ko = math.pow(2.0, -nth * uint8_width)\n",
    "\n",
    "        value = ko * np.float64(nth_slice)\n",
    "        return epilogue_ldexp(value, -exponent_shift)\n",
    "\n",
    "    @cuda.jit(launch_bounds=(block_size, 1))\n",
    "    def epilogue_kernel(slices, tensor_diag, tensor_shift_a, tensor_shift_b, tensor_out, alpha, beta):\n",
    "        tid_m = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        tid_n = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        shift_a = tensor_shift_a[tid_m]\n",
    "        shift_b = tensor_shift_b[tid_n]\n",
    "\n",
    "        diag_view = tensor_diag[tid_m, tid_n, :]\n",
    "\n",
    "        result = 0.0\n",
    "        for diag in range(slices-1, -1, -1):\n",
    "            result += nth_slice_to_fp64(diag, diag_view[diag], shift_a + shift_b)\n",
    "\n",
    "        if beta != 0:\n",
    "            result = alpha * result + beta * tensor_out[tid_m, tid_n]\n",
    "        else:\n",
    "            result = alpha * result\n",
    "\n",
    "        tensor_out[tid_m, tid_n] = result\n",
    "\n",
    "    return epilogue_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef11022-2c16-4c87-9f97-bb9701f44b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_func_solution(m, n, k):\n",
    "    ctx = setup_func(m, n, k)\n",
    "    BLAS = ctx[\"BLAS\"]\n",
    "    epilogue_block = ctx[\"epilogue_block\"]\n",
    "    ctx[\"gemm_kernel\"] = get_emulated_gemm_kernel_solution(BLAS);\n",
    "    ctx[\"epilogue_kernel\"] = get_epilogue_kernel_solution(math.prod(epilogue_block))\n",
    "    \n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dc1fa-cbb4-4182-9010-da3956cc7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_partially_fused_emulated_dgemm(problems, setup_func_solution, partial_fused_dgemm_ozaki, epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5fd0d-95b7-4133-a814-14e04c133a30",
   "metadata": {},
   "source": [
    "### Performance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ebee6-e9e2-4a86-9308-5456a39acbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# INT8 TOPS, MEMORY BANDWIDTH (GB/s)\n",
    "GPU_SPECS = {\n",
    "    \"L40S\": (733, 864),\n",
    "    \"B200\": (4500, 8000)\n",
    "}\n",
    "\n",
    "# NOTE: This model is very simplistic and does not take quantization or other overheads like slicing and FP64 operations into account\n",
    "def roofline_prediction_3_2(m, n, k, slices=7, TILE_M=128, TILE_N=128, TILE_K=128):\n",
    "    INT8_TOPS, MEMORY_BANDWIDTH_GBS = GPU_SPECS[\"L40S\"]\n",
    "\n",
    "    num_products = (slices * (slices + 1)) // 2\n",
    "\n",
    "    # By design since each thread is computing one output element\n",
    "    tiles = math.ceil(m / TILE_M) * math.ceil(n / TILE_N)\n",
    "\n",
    "    # Each tile does TILE_M * TILE_N dot products which each have k multiplications and k additions for every product\n",
    "    flops_per_tile = 2 * TILE_M * TILE_N * k * num_products\n",
    "\n",
    "    fp64_size = np.dtype(np.float64).itemsize\n",
    "    int32_size = np.dtype(np.float64).itemsize\n",
    "    int8_size = np.dtype(np.int8).itemsize\n",
    "\n",
    "    # We load a TILE_M rows of matrix A, TILE_N columns of matrix B for each product.\n",
    "    # Then, we read from and write to TILE_M * TILE_N elements of matrix C\n",
    "    # This needs to happen once for each diagonal\n",
    "    memory_per_tile = ((TILE_M * k + TILE_N * k) * int8_size + 2 * TILE_M * TILE_N * int32_size) * num_products\n",
    "\n",
    "    # In the epilogue kernel, we load the products and write the output\n",
    "    memory_per_tile += (TILE_M * TILE_N) * (num_products * int32_size + fp64_size)\n",
    "\n",
    "    total_memory_gb = tiles * memory_per_tile * 1e-9\n",
    "    total_tflop = tiles * flops_per_tile * 1e-12\n",
    "\n",
    "    return total_tflop / INT8_TOPS, total_memory_gb / MEMORY_BANDWIDTH_GBS\n",
    "\n",
    "time_flops, time_membw = roofline_prediction_3_2(2048, 2048, 2048)\n",
    "\n",
    "print(f\"The runtime from the math operations {time_flops * 1e3} ms and the runtime from memory is {time_membw * 1e3} ms\")\n",
    "\n",
    "# We will either be bottlenecked by FLOPS or Memory Bandwidth, so we take the maximum\n",
    "print(f\"Therefore, the estimated best case runtime is {max(time_flops, time_membw) * 1e3} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba46df-a842-4400-a646-3179b64e9b32",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we've learned how we can use the pipeline APIs to implement more complex routines.  Specifically, we've learned how to:\n",
    "\n",
    "1. Clear the accumulator when we are ready for a new computation\n",
    "2. Reset the device pipeline accumulator for new calculations\n",
    "3. How to iterate over 3D tensors with the pipeline API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
