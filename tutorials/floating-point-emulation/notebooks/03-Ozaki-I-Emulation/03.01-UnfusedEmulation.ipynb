{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237c0324-0fed-45bc-883b-52b975ad1927",
   "metadata": {},
   "source": [
    "# Getting Started With Emulation\n",
    "\n",
    "## Content\n",
    "\n",
    " - Introduction to the Ozaki-I Scheme\n",
    " - Exercise 1: IGEMM-based Ozaki-I Scheme \n",
    " - Exercise 2: Optimizing the Ozaki-I Scheme with kernel fusion\n",
    " - Exercise 3: Fully fused Ozaki-I Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5710fd-38ec-4e28-8393-d30528d09ab4",
   "metadata": {},
   "source": [
    "## The Ozaki-I Scheme\n",
    "\n",
    "The Ozaki Scheme was introduced as a general framework for emulating high precision data types as a combination of lower precision datatypes.  This was later applied to integer matrix multiplication in the paper \"[DGEMM on Integer Matrix Multiplication Unit](https://arxiv.org/abs/2306.11975)\" by Hiroyuki Ootomo, Katsuhisa Ozaki, and Rio Yokota.  When people refer to the Ozaki Scheme, it is usually implied that we are refering to the Ozaki-I scheme with integers.  For the remainder of the tutorial, we will mean the same.\n",
    "\n",
    "This algorithm consists of three parts:\n",
    "\n",
    "1. Slicing - A method to transform inputs into lower precision datatypes which, refered to as slices\n",
    "2. Slice Multiplication - Multiplying the corresponding A slices by the corresponding B slices\n",
    "3. Error-free transformation - This takes the output of multiplication and transforms it into the higher precision datatype.\n",
    "\n",
    "For this tutorial, we will give high level background on (1) and focus on steps (2) and (3).  If you are interested in learning more about (1), the code is present and documented within this tutorial.  We would also encourage you to review the [cubladsDx emulation sample](https://github.com/NVIDIA/CUDALibrarySamples/tree/main/MathDx/cuBLASDx/16_dgemm_emulation).\n",
    "\n",
    "A diagram of the high level workflow can be found below:\n",
    "\n",
    "<img src=\"Images/Ozaki-I-Flowchart.png\" width=\"600\" height=\"auto\"/>\n",
    "\n",
    "To perform slicing, we first need to know the maximum values for the rows of A and columns of B.  These are needed to logically align the exponents for elements in the same row (for matrix A) or column (for matrix B).  Once the exponents are aligned, we can read the mantissa bits into INT8 slices.  In practice, this is done with logical operations rather than FP64 arithmetic for a performance advantage.\n",
    "\n",
    "The next phase is slice multiplication, where each slice of \"A\" can multiply with each slice of \"B\".  As an optimization with minimal accuracy impact, we only multiply a subset of the slices as seen below.  It is important to note that in production libraries like cuBLAS, the ADP framework (see [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)) can detect if the lower order products are necessary and will leverage more integer slices to maintain FP64 level accuracy\n",
    "\n",
    "<img src=\"Images/Ozaki-I-Multiplications.png\" width=\"1000\" height=\"auto\"/>\n",
    "\n",
    "The last phase of this algorithm, shown at the bottom of the image above, is the error free transformation back into FP64.  The steps involved are:\n",
    "\n",
    "1. Accumulate products along anti-diagonals\n",
    "2. Converting the resulting accumulators into FP64\n",
    "3. Scaling the accumulators according to which anti-diagonal it represents\n",
    "4. Scaling the result once more to undo the exponent normalization in slicing\n",
    "\n",
    "## Exercise 3.1: IGEMM-based Ozaki-I Scheme\n",
    "\n",
    "The goal of this exercise will be to build a high level understanding of the Ozaki-I Scheme that we can use to further optimize in further exercise.\n",
    "\n",
    "In this exercise, we will orchestrate the right slice products and build an epilogue kernel which implements the error free transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9888bf9-ace5-44aa-9b7c-1be840a3206e",
   "metadata": {},
   "source": [
    "### C++ Cmake Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed88e1-c428-414f-82cf-a332c7186ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "from common_cuda import setup_cmake_project\n",
    "setup_cmake_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd0aa1-5c58-4cfc-8df8-d23306829fe1",
   "metadata": {},
   "source": [
    "### Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a020b4b9-025a-48df-a91c-260ca81a7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import nvmath\n",
    "\n",
    "from nvmath.device import Matmul\n",
    "from nvmath.device.cublasdx import DevicePipeline, SharedStorageCalc, MAX_ALIGNMENT\n",
    "from nvmath.device.cublasdx_numba import pipeline_extensions\n",
    "from nvmath.device.common import axpby, clear, copy, copy_fragment, copy_wait, make_tensor\n",
    "from numba import cuda\n",
    "\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "\n",
    "from benchmark import *\n",
    "from emulation_utils import get_width, epilogue_ldexp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf54e6-de9d-47d9-8ecb-6bd7901d1224",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e43f536-d2ed-4c9f-89ed-9ad608ed06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/parameters.hpp.inc\n",
    "\n",
    "    // ===================================\n",
    "    // Problem configuration\n",
    "    // ===================================\n",
    "\n",
    "    // (gemm_m, gemm_n, gemm_k, alpha, beta)\n",
    "    std::vector<tutorial::gemm_problem_t> problems = {\n",
    "        {2048, 2048, 2048, 0.9, 1.1}\n",
    "    };\n",
    "\n",
    "    // ===================================\n",
    "    // Global GEMM configuration\n",
    "    // ===================================\n",
    "\n",
    "    // The number of slices used in emulation algorithm\n",
    "    // More slices = higher precision but more computation\n",
    "    constexpr unsigned slices = 7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a00cd6-d134-4e61-9ce7-e254a44d8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/cublasdx_config.hpp.inc\n",
    "\n",
    "    using slice_value_type       = int8_t;  // Precision for individual slices\n",
    "    using accumulator_value_type = int32_t; // Precision for accumulation\n",
    "\n",
    "    // The shape of data tile processed by a single CTA block\n",
    "    constexpr int tile_m = 128;\n",
    "    constexpr int tile_n = 128;\n",
    "    constexpr int tile_k = 128;\n",
    "\n",
    "    // The shape of CTA block (number of threads)\n",
    "    constexpr int cta_shape_x = 128;\n",
    "    constexpr int cta_shape_y = 1;\n",
    "    constexpr int cta_shape_z = 1;\n",
    "\n",
    "    using BLAS = decltype(cublasdx::Size<tile_m, tile_n, tile_k>() +\n",
    "                          cublasdx::Precision<slice_value_type, slice_value_type, accumulator_value_type>() +\n",
    "                          cublasdx::Type<cublasdx::type::real>() + cublasdx::Function<cublasdx::function::MM>() +\n",
    "                          cublasdx::Arrangement<arrangement_a, arrangement_b, arrangement_c>() + cublasdx::Block() +\n",
    "                          cublasdx::BlockDim<cta_shape_x, cta_shape_y, cta_shape_z>() + cublasdx::StaticBlockDim() +\n",
    "                          cublasdx::WithPipeline() + cublasdx::MaxAlignment() + cublasdx::EnableInputStreaming() +\n",
    "                          cublasdx::SM<SM_VALUE, SM_MODIFIER_VALUE>());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4c8ba-1847-4a8a-96d3-07d426eb0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/igemm_kernel.hpp.inc\n",
    "\n",
    "template<class BLAS, class DevicePipeline, class OutTensor>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void igemm_kernel(__grid_constant__ DevicePipeline const device_pipeline,\n",
    "                      OutTensor                              out_tensor) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "#ifdef __CUDA_ARCH__\n",
    "    if constexpr (cublasdx::sm_of_v<BLAS> == __CUDA_ARCH__) {\n",
    "        auto tile_pipeline = device_pipeline.get_tile(smem, blockIdx.x, blockIdx.y);\n",
    "        auto tile_gmem_out = cublasdx::get_tile(out_tensor, BLAS::c_shape, blockIdx.x, blockIdx.y);\n",
    "\n",
    "        auto accumulator = tile_pipeline.get_accumulator();\n",
    "\n",
    "        tile_pipeline.execute(accumulator); \n",
    "        if (accumulator.is_thread_active()) {\n",
    "            accumulator.partition_and_store(tile_gmem_out);\n",
    "        }\n",
    "    }\n",
    "#endif\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b360f-bd0c-473c-bf8c-3155df5b5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/slice_coordination.hpp.inc\n",
    "\n",
    "            int product_idx = 0;\n",
    "            int num_products = (Slices * (Slices + 1)) / 2;\n",
    "\n",
    "            /*\n",
    "             * EXERCISE --> Coordinate the products between slices of matrix A and slices of matrix B\n",
    "             *              NOTE that tensor_slice_a is shaped like (m, k, nslices) and\n",
    "             *                        tensor_slice_b is shaped like (k, n, nslices)\n",
    "             *              with strides being (k, 1, m * k) and (1, k, k * n) respectively\n",
    "             *  \n",
    "             * Compute only the most significant products shown in the diagram above and store them into the\n",
    "             * tensor_products which has shape (m, n, num_products)\n",
    "             * with shape (m, n, num_products)\n",
    "             */\n",
    "            constexpr auto initial_diag = ;\n",
    "            constexpr auto initial_term = ;\n",
    "                 \n",
    "            for (auto diag = initial_diag; /* EXERCISE --> loop over diagonals */) {\n",
    "                for (auto term = initial_term; /* EXERCISE --> loop along the diagonal */) {\n",
    "                    // EXERCISE --> Determine which slice of A and slice of B to multiply\n",
    "                    int slice_a_index = ;\n",
    "                    int slice_b_index = ;\n",
    "\n",
    "                    // Prepare our view of the tensors, in this case we are getting the int8 submatrix for slice 'slice_a_index'.\n",
    "                    auto slice_a_view = tensor_slice_a(cublasdx::slice, cublasdx::slice, slice_a_index);\n",
    "                    auto slice_b_view = tensor_slice_b(cublasdx::slice, cublasdx::slice, slice_b_index);\n",
    "                    auto product_view = tensor_products(cublasdx::slice, cublasdx::slice, product_idx++);\n",
    "\n",
    "                    // Configure the device pipelines\n",
    "                    constexpr int pipeline_depth = 3;\n",
    "                    auto const device_pipeline =\n",
    "                        cublasdx::suggest_device_pipeline<pipeline_depth, BLAS, cublasdx::external_accumulation>(slice_a_view, slice_b_view).value();\n",
    "                    auto const shared_memory_size = device_pipeline.buffer_size();\n",
    "                    dim3 const grid_dim(shape_a_rows / static_tile_m(), shape_b_cols / static_tile_n());\n",
    "\n",
    "                    // Get the kernel and allow the kernel to use the shared memory required for the device pipeline\n",
    "                    auto kernel = igemm_kernel<BLAS, decltype(device_pipeline), decltype(product_view)>;\n",
    "                    CUDA_CHECK_AND_EXIT(\n",
    "                        cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, shared_memory_size));\n",
    "\n",
    "                    kernel<<<grid_dim, device_pipeline.get_block_dim(), shared_memory_size, str>>>(device_pipeline,\n",
    "                                                                                                   product_view);\n",
    "                    CUDA_CHECK_AND_EXIT(cudaGetLastError());\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e762f82-d2ea-4506-812a-a877747ae187",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/epilogue_config.hpp.inc\n",
    "        // Sets the block dimensions for the epilogue kernel\n",
    "        constexpr int epilogue_kernel_tile_m = 16;\n",
    "        constexpr int epilogue_kernel_tile_n = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cedf29-050d-481f-b2fb-0d53a7af15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/epilogue_kernel.hpp.inc\n",
    "\n",
    "template<int BlockSize, int Slices, class ProductTensor, class ShiftTensorA, class ShiftTensorB, class OutTensor>\n",
    "__launch_bounds__(BlockSize, 1) __global__ void epilogue_kernel(double        alpha,\n",
    "                                                                double        beta,\n",
    "                                                                ProductTensor product_tensor,\n",
    "                                                                ShiftTensorA  shift_tensor_a,\n",
    "                                                                ShiftTensorB  shift_tensor_b,\n",
    "                                                                OutTensor     out_tensor) {\n",
    "    using product_datatype = tutorial::tensor_value_type_t<ProductTensor>;\n",
    "    using shift_datatype   = tutorial::tensor_value_type_t<ShiftTensorA>;\n",
    "    using out_datatype     = tutorial::tensor_value_type_t<OutTensor>;\n",
    "\n",
    "    const auto tid_m = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    const auto tid_n = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    int shift_a = shift_tensor_a(tid_m);\n",
    "    int shift_b = shift_tensor_b(tid_n);\n",
    "\n",
    "    auto product_view = product_tensor(tid_m, tid_n, cublasdx::slice);\n",
    "\n",
    "    int    product_id = 0;\n",
    "    double accumulator = 0.0;\n",
    "\n",
    "    /*\n",
    "     * EXERCISE --> Complete the implementation of the epilogue kernel.  This kernel:\n",
    "     *   1. Accumulates along the anti-diagonals into diag_acc\n",
    "     *   2. Calls nth_slice_to_fp64 to convert back to fp64 and scale the exponent\n",
    "     *   3. Implements a typical GEMM epilogue (alpha * accumulator + beta * C)\n",
    "     */\n",
    "\n",
    "    constexpr auto initial_diag = ;\n",
    "    constexpr auto initial_term = ;\n",
    "                 \n",
    "    for (auto diag = initial_diag; /* EXERCISE --> loop over diagonals */) {\n",
    "        product_datatype diag_acc = 0;\n",
    "        for (auto term = initial_term; /* EXERCISE --> loop along the diagonal */) {\n",
    "            diag_acc += product_view(product_id++);\n",
    "        }\n",
    "\n",
    "        // HINT: Be careful here, the most significant diagonal is the 0th diagonal\n",
    "        accumulator += nth_slice_to_fp64<int32_t, int8_t>(diag, diag_acc, shift_a + shift_b);\n",
    "    }\n",
    "\n",
    "    out_tensor(tid_m, tid_n) = alpha * result + beta * out_tensor(tid_m, tid_n);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021ad97-4a44-4340-8b14-61888237dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build ./build -t 2a_unfused_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7bca6-d9dc-4c9a-b705-2fc412c0fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/2a_unfused_emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da3767-beae-4f4e-b18c-ae7d715227db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68e7b4-5f4f-4655-a5ba-5a91c726194f",
   "metadata": {},
   "source": [
    "We will rewrite kernel now and recompile the solution. If you want to restart your exercise make sure you rewrite kernel back and recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6416b-8d92-4ea6-8211-0e679d30af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/slice_coordination.hpp.inc\n",
    "\n",
    "            int product = 0;\n",
    "\n",
    "            /*\n",
    "             * EXERCISE --> Coordinate the products between slices of matrix A and slices of matrix B\n",
    "             *              NOTE that tensor_slice_a is shaped like (m, k, nslices) and\n",
    "             *                        tensor_slice_b is shaped like (k, n, nslices)\n",
    "             *              with strides being (k, 1, m * k) and (1, k, k * n) respectively\n",
    "             *  \n",
    "             * Compute only the most significant products shown in the diagram above and store them into the\n",
    "             * tensor_products which has shape (m, n, num_products)\n",
    "             * with shape (m, n, num_products)\n",
    "             */\n",
    "            constexpr auto initial_diag = Slices - 1;\n",
    "            constexpr auto initial_term = 0;\n",
    "\n",
    "            for (auto diag = initial_diag; diag >= 0; --diag) {\n",
    "                for (auto term = initial_term; term <= diag; ++term) {\n",
    "                    auto slice_a_view = tensor_slice_a(cublasdx::slice, cublasdx::slice, term);\n",
    "                    auto slice_b_view = tensor_slice_b(cublasdx::slice, cublasdx::slice, diag - term);\n",
    "                    auto product_view = tensor_products(cublasdx::slice, cublasdx::slice, product++);\n",
    "\n",
    "                    constexpr int pipeline_depth = 3;\n",
    "                    auto const device_pipeline =\n",
    "                        cublasdx::suggest_device_pipeline<pipeline_depth, BLAS, cublasdx::external_accumulation>(slice_a_view, slice_b_view).value();\n",
    "                    auto const shared_memory_size = device_pipeline.buffer_size();\n",
    "                    dim3 const grid_dim(shape_a_rows / static_tile_m(), shape_b_cols / static_tile_n());\n",
    "\n",
    "                    auto kernel = igemm_kernel<BLAS, decltype(device_pipeline), decltype(product_view)>;\n",
    "                    CUDA_CHECK_AND_EXIT(\n",
    "                        cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, shared_memory_size));\n",
    "\n",
    "                    kernel<<<grid_dim, device_pipeline.get_block_dim(), shared_memory_size, str>>>(device_pipeline,\n",
    "                                                                                                   product_view);\n",
    "                    CUDA_CHECK_AND_EXIT(cudaGetLastError());\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecadf8-0e30-486d-b25f-5ea4732fda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/2a_unfused_emulation/epilogue_kernel.hpp.inc\n",
    "\n",
    "template<int BlockSize, int Slices, class ProductTensor, class ShiftTensorA, class ShiftTensorB, class OutTensor>\n",
    "__launch_bounds__(BlockSize, 1) __global__ void epilogue_kernel(double        alpha,\n",
    "                                                                double        beta,\n",
    "                                                                ProductTensor product_tensor,\n",
    "                                                                ShiftTensorA  shift_tensor_a,\n",
    "                                                                ShiftTensorB  shift_tensor_b,\n",
    "                                                                OutTensor     out_tensor) {\n",
    "    using product_datatype = tutorial::tensor_value_type_t<ProductTensor>;\n",
    "    using shift_datatype   = tutorial::tensor_value_type_t<ShiftTensorA>;\n",
    "    using out_datatype     = tutorial::tensor_value_type_t<OutTensor>;\n",
    "\n",
    "    const auto tid_m = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    const auto tid_n = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    int shift_a = shift_tensor_a(tid_m);\n",
    "    int shift_b = shift_tensor_b(tid_n);\n",
    "\n",
    "    auto product_view = product_tensor(tid_m, tid_n, cublasdx::slice);\n",
    "\n",
    "    int    product_id = 0;\n",
    "    double result     = 0.0;\n",
    "\n",
    "    constexpr auto initial_diag = Slices - 1;\n",
    "    constexpr auto initial_term = 0;\n",
    "\n",
    "    for (auto diag = initial_diag; diag >= 0; --diag) {\n",
    "        product_datatype diag_acc = 0;\n",
    "        for (auto term = initial_term; term <= diag; ++term) {\n",
    "            diag_acc += product_view(product_id++);\n",
    "        }\n",
    "\n",
    "        result += nth_slice_to_fp64<int32_t, int8_t>(diag, diag_acc, shift_a + shift_b);\n",
    "    }\n",
    "\n",
    "    out_tensor(tid_m, tid_n) = alpha * result + beta * out_tensor(tid_m, tid_n);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff19100-4bee-4008-988e-1f57d4a885ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build ./build -t 2a_unfused_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a3521-4169-483f-a931-4a3516069955",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/2a_unfused_emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd0bd3-0591-4dca-9803-94f5a7aa84a7",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452cfba-e563-47fb-b210-f2e765b28867",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "  (2048, 2048, 2048, 0.9, 1.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271065b-2582-4d1a-805e-4e857f1f108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_igemm_kernel(BLAS):\n",
    "\n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    A_SIZE = BLAS.suggest_layout_smem_a().cosize\n",
    "    B_SIZE = BLAS.suggest_layout_smem_b().cosize\n",
    "    C_SIZE = BLAS.suggest_layout_rmem_c().cosize\n",
    "\n",
    "    TILE_M, TILE_N = BLAS.c_dim\n",
    "    TILE_K = BLAS.a_dim[1]\n",
    "    BLOCK_SIZE = BLAS.block_size\n",
    "    ALIGNMENT = min(BLAS.alignment.a, min(BLAS.alignment.b, BLAS.alignment.c))\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLOCK_SIZE, 1))\n",
    "    def igemm_kernel(tensor_c, device_pipeline: DevicePipeline):\n",
    "        m, n = tensor_c.shape\n",
    "\n",
    "        ldc = max(tensor_c.strides) // tensor_c.itemsize\n",
    "\n",
    "        block_m = cuda.blockIdx.x\n",
    "        block_n = cuda.blockIdx.y\n",
    "\n",
    "        smem = cuda.shared.array(shape=(0,), dtype=BLAS.a_value_type, alignment=ALIGNMENT)\n",
    "\n",
    "        block_start_m = block_m * TILE_M\n",
    "        block_end_m = (block_m + 1) * TILE_M\n",
    "\n",
    "        block_start_n = block_n * TILE_N\n",
    "        block_end_n = (block_n + 1) * TILE_N\n",
    "\n",
    "        if block_start_m >= m or block_start_n >= n:\n",
    "            return\n",
    "        \n",
    "        c_view = tensor_c[\n",
    "            block_start_m : block_end_m,\n",
    "            block_start_n : block_end_n,\n",
    "        ]\n",
    "\n",
    "        gmem_c = make_tensor(c_view, BLAS.get_layout_gmem_c(ldc))\n",
    "        \n",
    "        tile_pipeline = device_pipeline.get_tile(smem, block_m, block_n)\n",
    "        \n",
    "        accumulator = BLAS.suggest_accumulator()\n",
    "        tile_pipeline.execute(accumulator)\n",
    "\n",
    "        if accumulator.is_thread_active():\n",
    "            accumulator.partition_and_copy(accumulator.get_results(), gmem_c)\n",
    "\n",
    "        tile_pipeline._del()\n",
    "\n",
    "    return igemm_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313ead1-b6ec-4068-94a4-847e9f747cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfused_igemm_ozaki(tensor_slicedA, tensor_slicedB, tensor_product, context, warmup=True):\n",
    "    BLAS = context[\"BLAS\"]\n",
    "    pipeline_depth = context[\"PIPELINE_DEPTH\"]\n",
    "    igemm_kernel = context[\"gemm_kernel\"]\n",
    "    grid = context[\"gemm_grid\"]\n",
    "    block = context[\"gemm_block\"]\n",
    "\n",
    "    _, _, slices = tensor_slicedA.shape\n",
    "\n",
    "    product_index = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    EXERCISE --> Coordinate the products between slices of matrix A and slices of matrix B\n",
    "                 NOTE that tensor_slice_a is shaped like (m, k, nslices) and\n",
    "                           tensor_slice_b is shaped like (k, n, nslices)\n",
    "                 with strides being (k, 1, m * k) and (1, k, k * n) respectively  \n",
    "    Compute only the most significant products and store them into tensor_products\n",
    "    with shape (m, n, num_products)\n",
    "    \"\"\"\n",
    "\n",
    "    initial_diag = -1\n",
    "    initial_term = -1\n",
    "    \n",
    "    for diag in range(-1): # EXERCISE --> loop over diagonals\n",
    "        for term in range(-1): # EXERCISE --> loop along the diagonal\n",
    "            # EXERCISE --> Determine which slice of A and slice of B to multiply\n",
    "            slice_a_index = -1\n",
    "            slice_b_index = -1\n",
    "            \n",
    "            # Convert from a cupy array to numba arrays\n",
    "            #  - cupy arrays are needed to setup 3D strides for the pipeline API\n",
    "            slice_a_view = cuda.as_cuda_array(tensor_slicedA[:, :, slice_a_index])\n",
    "            slice_b_view = cuda.as_cuda_array(tensor_slicedB[:, :, slice_a_index])\n",
    "            product_view = cuda.as_cuda_array(tensor_product[:, :, product_index])\n",
    "\n",
    "            product_index += 1\n",
    "\n",
    "            device_pipeline = BLAS.suggest_device_pipeline(pipeline_depth, slice_a_view, slice_b_view)\n",
    "\n",
    "            if warmup and diag == initial_diag and term == initial_term:\n",
    "                set_max_dynamic_shared_size_bytes(igemm_kernel, device_pipeline.buffer_size,\n",
    "                                                    product_view, device_pipeline)\n",
    "\n",
    "            igemm_kernel[grid, block, 0, device_pipeline.buffer_size](product_view, device_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f092b50-00d8-4a50-880e-fb935d74a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epilogue_kernel(block_size=64):\n",
    "    uint8_width = get_width(np.uint8)\n",
    "\n",
    "    @cuda.jit(device=True, forceinline=True)\n",
    "    def nth_slice_to_fp64(nth, nth_slice, exponent_shift):\n",
    "        ko = math.pow(2.0, -nth * uint8_width)\n",
    "\n",
    "        value = ko * np.float64(nth_slice)\n",
    "        return epilogue_ldexp(value, -exponent_shift)\n",
    "\n",
    "    @cuda.jit(launch_bounds=(block_size, 1))\n",
    "    def epilogue_kernel(slices, tensor_product, tensor_shift_a, tensor_shift_b, tensor_out, alpha, beta):\n",
    "        tid_m = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        tid_n = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        if tid_m >= tensor_out.shape[0] or tid_n >= tensor_out.shape[1]:\n",
    "            return\n",
    "\n",
    "        shift_a = tensor_shift_a[tid_m]\n",
    "        shift_b = tensor_shift_b[tid_n]\n",
    "\n",
    "        product_view = tensor_product[tid_m, tid_n, :]\n",
    "\n",
    "        product_id = 0\n",
    "        accumulator = 0.0\n",
    "\n",
    "        \"\"\"\n",
    "        EXERCISE --> Complete the implementation of the epilogue kernel.  This kernel:\n",
    "          1. Accumulates along the anti-diagonals into diag_acc\n",
    "          2. Calls nth_slice_to_fp64 to convert back to fp64 and scale the exponent\n",
    "          3. Implements a typical GEMM epilogue (alpha * accumulator + beta * C)\n",
    "        \"\"\"\n",
    "\n",
    "        initial_diag = -1\n",
    "        initial_term = -1\n",
    "    \n",
    "        for diag in range(-1): # EXERCISE --> loop over diagonals\n",
    "            diag_acc = 0\n",
    "            for term in range(-1): # EXERCISE --> loop along the diagonal\n",
    "                diag_acc += product_view[product_id]\n",
    "                product_id += 1\n",
    "\n",
    "            # HINT: Be careful here, the most significant diagonal is the 0th diagonal\n",
    "            accumulator += nth_slice_to_fp64(diag, diag_acc, shift_a + shift_b)\n",
    "    \n",
    "        tensor_out[tid_m, tid_n] = alpha * accumulator + beta * tensor_out[tid_m, tid_n]\n",
    "\n",
    "    return epilogue_kernel\n",
    "\n",
    "def epilogue(slices, tensor_products, tensor_shift_a, tensor_shift_b, tensor_c, alpha, beta, context):\n",
    "    epilogue_kernel = context[\"epilogue_kernel\"]\n",
    "    grid = context[\"epilogue_grid\"]\n",
    "    block = context[\"epilogue_block\"]\n",
    "    \n",
    "    epilogue_kernel[grid, block](slices, tensor_products, tensor_shift_a, tensor_shift_b, tensor_c, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba20117-19ab-4bf2-a158-4dcf2af9f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_func(m, n, k):\n",
    "    tile_m = 128\n",
    "    tile_n = 128\n",
    "    tile_k = 128\n",
    "    pipeline_depth = 3\n",
    "    block_size = 128\n",
    "\n",
    "    epilogue_tile_m = 16\n",
    "    epilogue_tile_n = 16\n",
    "\n",
    "    assert m % tile_m == 0, \"Unsupported dimension m for TILE_M\"\n",
    "    assert n % tile_n == 0, \"Unsupported dimension n for TILE_N\"\n",
    "    assert k % tile_k == 0, \"Unsupported dimension n for TILE_N\"\n",
    "    assert k >= (tile_k * pipeline_depth), \"Unsupported pipeline depth for k\"\n",
    "\n",
    "    assert m % epilogue_tile_m == 0, \"Unsupported dimension for EPILOGUE_TILE_M\"\n",
    "    assert n % epilogue_tile_n == 0, \"Unsupported dimension for EPILOGUE_TILE_N\"\n",
    "    \n",
    "    BLAS = Matmul(size=(tile_m, tile_n, tile_k),\n",
    "                  precision=(np.int8, np.int8, np.int32),\n",
    "                  data_type=\"real\",\n",
    "                  alignment=MAX_ALIGNMENT,\n",
    "                  arrangement=(\"row_major\", \"col_major\", \"col_major\"), # Do not change\n",
    "                  execution=\"Block\",\n",
    "                  block_size=block_size,\n",
    "                  with_pipeline=True,\n",
    "                  enable_input_streaming=True,\n",
    "                  static_block_dim=True)\n",
    "\n",
    "    gemm_grid = (m // tile_m, n // tile_n)\n",
    "    gemm_block = BLAS.block_dim\n",
    "\n",
    "    epilogue_grid = (m // epilogue_tile_m, n // epilogue_tile_n)\n",
    "    epilogue_block = (epilogue_tile_m, epilogue_tile_n)\n",
    "\n",
    "    return {\n",
    "        \"BLAS\": BLAS,\n",
    "        \"PIPELINE_DEPTH\": pipeline_depth,\n",
    "        \"gemm_kernel\" : get_igemm_kernel(BLAS),\n",
    "        \"gemm_grid\": gemm_grid,\n",
    "        \"gemm_block\": gemm_block,\n",
    "        \"epilogue_kernel\": get_epilogue_kernel(math.prod(epilogue_block)),\n",
    "        \"epilogue_grid\": epilogue_grid,\n",
    "        \"epilogue_block\": epilogue_block\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556d45c-3a73-4f04-ae97-cd7d2dd4661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_unfused_emulated_dgemm(problems, setup_func, unfused_igemm_ozaki, epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf99b3-1603-4170-8828-f37dfbb0353a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc339107-5ef9-44bc-8dfc-ef262f59a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfused_igemm_ozaki_solution(tensor_slicedA, tensor_slicedB, tensor_product, context, warmup=True):\n",
    "    BLAS = context[\"BLAS\"]\n",
    "    pipeline_depth = context[\"PIPELINE_DEPTH\"]\n",
    "    igemm_kernel = context[\"gemm_kernel\"]\n",
    "    grid = context[\"gemm_grid\"]\n",
    "    block = context[\"gemm_block\"]\n",
    "\n",
    "    _, _, slices = tensor_slicedA.shape\n",
    "\n",
    "    product_id = 0\n",
    "\n",
    "    \"\"\"\n",
    "     EXERCISE --> Coordinate the products between slices of matrix A and slices of matrix B\n",
    "                  NOTE that tensor_slice_a is shaped like (m, k, nslices) and\n",
    "                            tensor_slice_b is shaped like (k, n, nslices)\n",
    "                  with strides being (k, 1, m * k) and (1, k, k * n) respectively\n",
    "      \n",
    "     Compute only the most significant products shown in the diagram above and store them into the\n",
    "     tensor_products which has shape (m, n, num_products)\n",
    "     with shape (m, n, num_products)\n",
    "    \"\"\"\n",
    "    initial_diag = slices - 1\n",
    "    initial_term = 0\n",
    "    \n",
    "    for diag in range(initial_diag, -1, -1):\n",
    "        for term in range(diag + 1):\n",
    "            slice_a = term\n",
    "            slice_b = diag - term\n",
    "\n",
    "            # Convert from a cupy array to numba arrays\n",
    "            #  - cupy arrays are needed to setup 3D strides for the pipeline API\n",
    "            slice_a_view = cuda.as_cuda_array(tensor_slicedA[:, :, slice_a])\n",
    "            slice_b_view = cuda.as_cuda_array(tensor_slicedB[:, :, slice_b])\n",
    "            product_view = cuda.as_cuda_array(tensor_product[:, :, product_id])\n",
    "\n",
    "            device_pipeline = BLAS.suggest_device_pipeline(pipeline_depth, slice_a_view, slice_b_view)\n",
    "\n",
    "            if warmup and diag == initial_diag and term == initial_term:\n",
    "                set_max_dynamic_shared_size_bytes(igemm_kernel, device_pipeline.buffer_size,\n",
    "                                                    product_view, device_pipeline)\n",
    "\n",
    "            igemm_kernel[grid, block, 0, device_pipeline.buffer_size](product_view, device_pipeline)\n",
    "             \n",
    "            product_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77669dc6-000d-446b-b009-9aebb349a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epilogue_kernel_solution(block_size=64):\n",
    "    uint8_width = get_width(np.uint8)\n",
    "\n",
    "    @cuda.jit(device=True, forceinline=True)\n",
    "    def nth_slice_to_fp64(nth, nth_slice, exponent_shift):\n",
    "        ko = math.pow(2.0, -nth * uint8_width)\n",
    "\n",
    "        value = ko * np.float64(nth_slice)\n",
    "        return epilogue_ldexp(value, -exponent_shift)\n",
    "\n",
    "    @cuda.jit(launch_bounds=(block_size, 1))\n",
    "    def epilogue_kernel(slices, tensor_product, tensor_shift_a, tensor_shift_b, tensor_out, alpha, beta):\n",
    "        tid_m = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        tid_n = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "\n",
    "        if tid_m >= tensor_out.shape[0] or tid_n >= tensor_out.shape[1]:\n",
    "            return\n",
    "\n",
    "        shift_a = tensor_shift_a[tid_m]\n",
    "        shift_b = tensor_shift_b[tid_n]\n",
    "\n",
    "        product_view = tensor_product[tid_m, tid_n, :]\n",
    "\n",
    "        product_id = 0\n",
    "        result = 0.0\n",
    "\n",
    "        initial_diag = slices - 1\n",
    "        initial_term = 0\n",
    "    \n",
    "        for diag in range(initial_diag, -1, -1):\n",
    "            diag_acc = 0\n",
    "            for term in range(diag + 1):\n",
    "                diag_acc += product_view[product_id]\n",
    "                product_id += 1\n",
    "\n",
    "            result += nth_slice_to_fp64(diag, diag_acc, shift_a + shift_b)\n",
    "\n",
    "        if beta != 0:\n",
    "            result = alpha * result + beta * tensor_out[tid_m, tid_n]\n",
    "        else:\n",
    "            result = alpha * result\n",
    "\n",
    "        tensor_out[tid_m, tid_n] = result\n",
    "\n",
    "    return epilogue_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5af2b7-c662-440d-abc9-e7a351efbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_func_solution(m, n, k):\n",
    "    ctx = setup_func(m, n, k)\n",
    "    epilogue_block = ctx[\"epilogue_block\"]\n",
    "    ctx[\"epilogue_kernel\"] = get_epilogue_kernel_solution(math.prod(epilogue_block))\n",
    "    \n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97126e3-103b-4b10-a44a-3c8e7875c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_unfused_emulated_dgemm(problems, setup_func_solution, unfused_igemm_ozaki_solution, epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76d1a6-d28c-4664-a937-010bc663766a",
   "metadata": {},
   "source": [
    "### Performance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb09f77-4bd5-4b92-bad6-dbc9664a5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# INT8 TOPS, MEMORY BANDWIDTH (GB/s)\n",
    "GPU_SPECS = {\n",
    "    \"L40S\": (733, 864),\n",
    "    \"B200\": (4500, 8000)\n",
    "}\n",
    "\n",
    "# NOTE: This model is very simplistic and does not take quantization or other overheads like slicing and FP64 operations into account\n",
    "def roofline_prediction_3_1(m, n, k, slices=7, TILE_M=128, TILE_N=128, TILE_K=128):\n",
    "    INT8_TOPS, MEMORY_BANDWIDTH_GBS = GPU_SPECS[\"L40S\"]\n",
    "\n",
    "    num_products = (slices * (slices + 1)) // 2\n",
    "\n",
    "    # By design since each thread is computing one output element\n",
    "    tiles = math.ceil(m / TILE_M) * math.ceil(n / TILE_N)\n",
    "\n",
    "    # Each tile does TILE_M * TILE_N dot products which each have k multiplications and k additions for every product\n",
    "    flops_per_tile = 2 * TILE_M * TILE_N * k * num_products\n",
    "\n",
    "    fp64_size = np.dtype(np.float64).itemsize\n",
    "    int32_size = np.dtype(np.float64).itemsize\n",
    "    int8_size = np.dtype(np.int8).itemsize\n",
    "\n",
    "    # We load a TILE_M rows of matrix A, TILE_N columns of matrix B, and write to TILE_M * TILE_N elements of matrix C\n",
    "    # This needs to happen for each product\n",
    "    memory_per_tile = ((TILE_M * k + TILE_N * k) * int8_size + TILE_M * TILE_N * int32_size) * num_products\n",
    "\n",
    "    # In the epilogue kernel, we load the products and read from and write to the output\n",
    "    memory_per_tile += (TILE_M * TILE_N) * (num_products * int32_size + 2 * fp64_size)\n",
    "\n",
    "    total_memory_gb = tiles * memory_per_tile * 1e-9\n",
    "    total_tflop = tiles * flops_per_tile * 1e-12\n",
    "\n",
    "    return total_tflop / INT8_TOPS, total_memory_gb / MEMORY_BANDWIDTH_GBS\n",
    "\n",
    "time_flops, time_membw = roofline_prediction_3_1(2048, 2048, 2048)\n",
    "\n",
    "print(f\"The runtime from the math operations {time_flops * 1e3} ms and the runtime from memory is {time_membw * 1e3} ms\")\n",
    "\n",
    "# We will either be bottlenecked by FLOPS or Memory Bandwidth, so we take the maximum\n",
    "print(f\"Therefore, the estimated best case runtime is {max(time_flops, time_membw) * 1e3} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc68c7-c9d9-4bab-9665-6ade4fc8402d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we've learned the fundamentals of the Ozaki-I Scheme and built an implemenation that we will optimize in the next exercises.\n",
    "\n",
    "We then analyzed why the Ozaki scheme makes sense by building a simplistic model for the product gemms and epilogue kernel.  In the next exercise, we will implement kernel fusion to reduce memory overhead and speedup the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
