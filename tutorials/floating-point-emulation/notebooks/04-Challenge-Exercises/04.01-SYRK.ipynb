{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24069346-2c9c-4d6b-b812-d59f2804e36c",
   "metadata": {},
   "source": [
    "## Challenge Exercise 4.1: SYRK Emulation\n",
    "\n",
    "Now that we understand how to build a performant emulated GEMM kernel, we can start to think about how we can apply the same algorithm and techniques to other routines.\n",
    "\n",
    "A very closely related routine is the symmetric rank-k update, often referred to as SYRK.  For a given $n \\times k$ matrix $\\mathbf{A}$, we can compute an $n \\times n$ matrix $\\mathbf{C}$ at row $i$, column $j$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{i, j} = \\alpha \\sum_{l=0}^{k}\\left( \\mathbf{A}_{i, l} \\mathbf{A}^{T}_{l, j} \\right) + \\beta \\mathbf{C}_{i, j}\n",
    "$$\n",
    "\n",
    "You may notice that this follows the same definition as GEMM, except that we are multiplying $\\mathbf{A}$ with itself.  This allows the output matrix to be symmetric (i.e. $\\mathbf{C} = \\mathbf{C}^{T}$)\n",
    "\n",
    "That matrix property along with the knowledge that we are multiply $A$ by itself allows us to make more problem specific optimizations to reduce the amount of math operations and access memory more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db64c8-86ef-4756-930d-bcd78b776b93",
   "metadata": {},
   "source": [
    "### C++ CMake Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4d4b5-ec12-4a3c-a0b2-30b3bf786314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "from common_cuda import setup_cmake_project\n",
    "setup_cmake_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94855e83-341a-4b13-83ea-273c6e7af279",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23406d00-44c5-4f62-b9d3-82dd1ba25817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import nvmath\n",
    "\n",
    "from nvmath.device import Matmul\n",
    "from nvmath.device.cublasdx import DevicePipeline, SharedStorageCalc\n",
    "from nvmath.device.cublasdx_numba import pipeline_extensions\n",
    "from nvmath.device.common import axpby, clear, copy, copy_fragment, copy_wait, make_tensor\n",
    "from numba import cuda\n",
    "\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "\n",
    "from benchmark import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf99cb9-bfa3-4c5e-bfcf-cbae48061443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac7a49-07ab-4337-979c-aa9e121902f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/parameters.hpp.inc\n",
    "\n",
    "    int const warm_up_runs = 10;\n",
    "    int const kernel_runs = 100;\n",
    "\n",
    "    // ===================================\n",
    "    // Problem configuration\n",
    "    // ===================================\n",
    "\n",
    "    // (syrk_n, syrk_k, alpha, beta, uplo)\n",
    "    std::vector<tutorial::syrk_problem_t> problems = {\n",
    "        {8192, 8192, 0.9, 1.1, tutorial::matrix_half::upper},\n",
    "        {8192, 8192, 0.9, 1.1, tutorial::matrix_half::lower}\n",
    "    };\n",
    "    \n",
    "\n",
    "    // ===================================\n",
    "    // Global SYRK configuration\n",
    "    // ===================================\n",
    "\n",
    "    // The number of slices used in emulation algorithm\n",
    "    // More slices = higher precision but more computation\n",
    "    constexpr unsigned slices = 7;\n",
    "\n",
    "    bool const   debug        = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9545479-9d2f-4130-b410-173f9cf87943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/cublasdx_config.hpp.inc\n",
    "\n",
    "    using slice_value_type       = int8_t;  // Precision for individual slices\n",
    "    using accumulator_value_type = int32_t; // Precision for accumulation\n",
    "\n",
    "    // The shape of data tile processed by a single CTA block\n",
    "    constexpr int tile_m = 128;\n",
    "    constexpr int tile_n = 128;\n",
    "    constexpr int tile_k = 128;\n",
    "\n",
    "    // The shape of CTA block (number of threads)\n",
    "    constexpr int cta_shape_x = 128;\n",
    "    constexpr int cta_shape_y = 1;\n",
    "    constexpr int cta_shape_z = 1;\n",
    "\n",
    "    using BLAS = decltype(cublasdx::Size<tile_m, tile_n, tile_k>() +\n",
    "                          cublasdx::Precision<slice_value_type, slice_value_type, accumulator_value_type>() +\n",
    "                          cublasdx::Type<cublasdx::type::real>() + cublasdx::Function<cublasdx::function::MM>() +\n",
    "                          cublasdx::Arrangement<arrangement_a, arrangement_a_t, arrangement_c>() + cublasdx::Block() +\n",
    "                          cublasdx::BlockDim<cta_shape_x, cta_shape_y, cta_shape_z>() + cublasdx::StaticBlockDim() +\n",
    "                          cublasdx::MaxAlignment() + cublasdx::EnableInputStreaming() + cublasdx::WithPipeline() +\n",
    "                          cublasdx::SM<SM_VALUE, SM_MODIFIER_VALUE>());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a0f84-9477-4294-b59b-f4d806411cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/pipeline_config.hpp.inc\n",
    "\n",
    "        constexpr unsigned pipeline_depth = 3;\n",
    "\n",
    "        auto device_pipeline = cublasdx::suggest_device_pipeline<pipeline_depth, BLAS, cublasdx::external_accumulation>(\n",
    "                                   tensor_slice_a, tensor_slice_at)\n",
    "                                   .value();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d63bf-c79e-4f0b-ab54-75e574518fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/kernel_config.hpp.inc\n",
    "\n",
    "        dim3      grid(shape_a_rows / static_tile_m(), shape_a_rows / static_tile_n());\n",
    "        auto kernel = fused_epilogue_kernel<BLAS,\n",
    "                                            decltype(device_pipeline),\n",
    "                                            double,\n",
    "                                            double,\n",
    "                                            CTensor,\n",
    "                                            decltype(tensor_shift_a),\n",
    "                                            decltype(tensor_shift_at),\n",
    "                                            Slices>;\n",
    "\n",
    "        auto shared_memory_size = cublasdx::make_shared_storage_calculator()\n",
    "                                      .add(device_pipeline.buffer_alignment(), device_pipeline.buffer_size())\n",
    "                                      .add(16, sizeof(int32_t), static_tile_m()) // shift_a\n",
    "                                      .add(16, sizeof(int32_t), static_tile_n()) // shift_b\n",
    "                                      .get();\n",
    "\n",
    "        CUDA_CHECK_AND_EXIT(\n",
    "            cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, shared_memory_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013bf66-8ad0-441d-b86e-694588047e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/fused_kernel.hpp.inc\n",
    "\n",
    "template<class BLAS,\n",
    "         class DevicePipeline,\n",
    "         class Alpha,\n",
    "         class Beta,\n",
    "         class CTensor,\n",
    "         class AShiftTensor,\n",
    "         class AtShiftTensor,\n",
    "         int32_t Slices>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void fused_epilogue_kernel(__grid_constant__ DevicePipeline const device_pipeline,\n",
    "                               Alpha                                  alpha,\n",
    "                               Beta                                   beta,\n",
    "                               CTensor                                gmem_c_fp64,\n",
    "                               tutorial::matrix_half                  output_half,\n",
    "                               AShiftTensor const                     gmem_shift_a,\n",
    "                               AtShiftTensor const                    gmem_shift_at) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "#ifdef __CUDA_ARCH__\n",
    "    if constexpr (cublasdx::sm_of_v<BLAS> == __CUDA_ARCH__) {\n",
    "        // CHALLENGE EXERCISE --> Implement a fused SYRK\n",
    "        // HINT --> Start with emulated GEMM kernel from 3.3\n",
    "    }\n",
    "#endif\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aabebb9-0b1f-49f1-9675-5649b7219ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/kernel_launch.hpp.inc\n",
    "\n",
    "        kernel<<<grid, device_pipeline.get_block_dim(), shared_memory_size, stream>>>(\n",
    "            device_pipeline, alpha, beta, tensor_c, output_half, tensor_shift_a, tensor_shift_at);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94a5e0-4f37-4a41-9524-31bf6f593cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build ./build -t 3a_fused_syrk_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29399cdc-c197-4e78-9e05-f2b9833dffc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./build/3a_fused_syrk_emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd736c-ed7b-4319-acd5-4d5c6d10ea37",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853a1a3-c227-4811-b23d-b36f4cc3969f",
   "metadata": {},
   "source": [
    "We will rewrite kernel now and recompile the solution. If you want to restart your exercise make sure you rewrite kernel back and recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59880a32-4f56-4b34-9cf5-5289e9d85cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpp/3a_fused_syrk_emulation/fused_kernel.hpp.inc\n",
    "\n",
    "template<class BLAS,\n",
    "         class DevicePipeline,\n",
    "         class Alpha,\n",
    "         class Beta,\n",
    "         class CTensor,\n",
    "         class AShiftTensor,\n",
    "         class AtShiftTensor,\n",
    "         int32_t Slices>\n",
    "__launch_bounds__(DevicePipeline::max_threads_per_block, 1) __global__\n",
    "    void fused_epilogue_kernel(__grid_constant__ DevicePipeline const device_pipeline,\n",
    "                               Alpha                                  alpha,\n",
    "                               Beta                                   beta,\n",
    "                               CTensor                                gmem_c_fp64,\n",
    "                               tutorial::matrix_half                  output_half,\n",
    "                               AShiftTensor const                     gmem_shift_a,\n",
    "                               AtShiftTensor const                    gmem_shift_at) {\n",
    "    extern __shared__ __align__(device_pipeline.buffer_alignment()) char smem[];\n",
    "#ifdef __CUDA_ARCH__\n",
    "    if constexpr (cublasdx::sm_of_v<BLAS> == __CUDA_ARCH__) {\n",
    "        // ================================\n",
    "        // 1. SETUP AND TILE PREPARATION\n",
    "        // ================================\n",
    "\n",
    "        constexpr int tile_m = cublasdx::size_of_v_m<BLAS>;\n",
    "        constexpr int tile_n = cublasdx::size_of_v_n<BLAS>;\n",
    "\n",
    "        auto const block_offset_m = blockIdx.x * tile_m;\n",
    "        auto const block_offset_n = blockIdx.y * tile_n;\n",
    "\n",
    "        if ((block_offset_n > (block_offset_m + tile_m) and output_half == tutorial::matrix_half::lower) or\n",
    "            (block_offset_m > (block_offset_n + tile_n) and output_half == tutorial::matrix_half::upper)) {\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        constexpr auto initial_diag = Slices - 1;\n",
    "        constexpr auto initial_term = 0;\n",
    "\n",
    "        auto [pipeline_smem, smem_shift_a, smem_shift_at] =\n",
    "            cublasdx::shared_memory::slice<char, int32_t, int32_t>(smem,\n",
    "                                                                   device_pipeline.buffer_alignment(),\n",
    "                                                                   device_pipeline.buffer_size(),\n",
    "                                                                   cublasdx::alignment_of_v_a<BLAS>,\n",
    "                                                                   cute::make_layout(cute::Int<tile_m>()),\n",
    "                                                                   cublasdx::alignment_of_v_b<BLAS>,\n",
    "                                                                   cute::make_layout(cute::Int<tile_n>()));\n",
    "\n",
    "        // Copy general purpose data\n",
    "        cublasdx::copy<BLAS, 16>(gmem_shift_a(cute::_, blockIdx.x), smem_shift_a);\n",
    "        cublasdx::copy<BLAS, 16>(gmem_shift_at(cute::_, blockIdx.y), smem_shift_at);\n",
    "        cublasdx::copy_wait();\n",
    "\n",
    "\n",
    "        // Get pipeline tile\n",
    "        auto tile_pipeline = device_pipeline.get_tile(pipeline_smem,\n",
    "                                                      cublasdx::make_coord(blockIdx.x, initial_term),\n",
    "                                                      cublasdx::make_coord(blockIdx.y, initial_diag));\n",
    "\n",
    "        auto accumulator = tile_pipeline.get_accumulator();\n",
    "\n",
    "        // ================================\n",
    "        // 2. FP64 C INPUT / OUTPUT TILE SETUP\n",
    "        // ================================\n",
    "\n",
    "        auto tile_c_fp64_gmem = cublasdx::get_tile(gmem_c_fp64, BLAS::c_shape, blockIdx.x, blockIdx.y);\n",
    "\n",
    "        // ============================================\n",
    "        // 3. OZAKI SCHEME DIAGONAL ITERATION\n",
    "        // ============================================\n",
    "\n",
    "        // Iterate over diagonals in reverse order (highest power of 2 first)\n",
    "        // This ensures proper accumulation order for numerical stability\n",
    "#    pragma unroll 1\n",
    "        for (auto diag = initial_diag; diag >= 0; --diag) {\n",
    "\n",
    "            // Initialize accumulator for this diagonal\n",
    "            accumulator.clear();\n",
    "\n",
    "            // ==========================================\n",
    "            // 4. SLICE COMBINATION COMPUTATION\n",
    "            // ==========================================\n",
    "\n",
    "            // Compute all slice combinations that contribute to this diagonal\n",
    "            // For diagonal d, we compute: A_slice[i] * B_slice[d-i] for i = 0 to d\n",
    "#    pragma unroll 1\n",
    "            for (auto term = initial_term; term <= diag; ++term) {\n",
    "                // =========================================\n",
    "                // 5. N-STAGE MEMORY PIPELINE FOR GEMM\n",
    "                // =========================================\n",
    "\n",
    "                tile_pipeline.execute(accumulator);\n",
    "\n",
    "                const auto next_slice_row = (term == diag) ? 0 : term + 1;                         // A slice index\n",
    "                const auto next_slice_col = (term == diag) ? (diag - 1) : (diag - next_slice_row); // B slice index\n",
    "                device_pipeline.reset_tile(tile_pipeline,\n",
    "                                           cublasdx::make_coord(blockIdx.x, next_slice_row),\n",
    "                                           cublasdx::make_coord(blockIdx.y, next_slice_col));\n",
    "            } /* end of slice combination loop */\n",
    "\n",
    "            // ========================================\n",
    "            // 6. RESULT RECONSTRUCTION AND EPILOGUE\n",
    "            // ========================================\n",
    "\n",
    "\n",
    "            // Load existing C values\n",
    "            auto d_fp64_frag = accumulator.make_partition_and_copy(tile_c_fp64_gmem);\n",
    "\n",
    "            if(accumulator.is_thread_active()) {\n",
    "                // Convert accumulated int32_t results back to double precision\n",
    "                // and apply appropriate scaling based on slice positions\n",
    "                auto gemm_results = accumulator.get_results();\n",
    "\n",
    "                // Process each element in the register fragment\n",
    "#    pragma unroll\n",
    "                for (int i = 0; i < cublasdx::size(d_fp64_frag); ++i) {\n",
    "                    auto const [global_x, global_y] = accumulator.map_fragment_index(i);\n",
    "                    auto const shift_a_elem         = smem_shift_a(global_x);\n",
    "                    auto const shift_at_elem        = smem_shift_at(global_y);\n",
    "\n",
    "                    int const total_x = block_offset_m + global_x;\n",
    "                    int const total_y = block_offset_n + global_y;\n",
    "                    bool const is_in_bounds = (output_half == tutorial::matrix_half::lower and (total_x >= total_y)) or\n",
    "                                              (output_half == tutorial::matrix_half::upper and (total_y >= total_x));\n",
    "\n",
    "                    // Convert int32_t slice result back to double precision\n",
    "                    // with appropriate scaling for this diagonal and element\n",
    "                    double const val = nth_slice_to_fp64<int32_t, int8_t>(diag, gemm_results(i), shift_a_elem + shift_at_elem);\n",
    "                    d_fp64_frag(i) = is_in_bounds ? (alpha * val + beta * d_fp64_frag(i)) : d_fp64_frag(i);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            accumulator.partition_and_copy(d_fp64_frag, tile_c_fp64_gmem);\n",
    "            beta = 1.0;\n",
    "        }\n",
    "    }\n",
    "#endif\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca293b12-c518-462a-91a4-0bd1965800e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build ./build -t 3a_fused_syrk_emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fc7cf-8368-4fb7-8553-e0a0adb11dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build/3a_fused_syrk_emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79313d8-adca-4056-8f8c-f9612f08a104",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf954468-ad39-45ea-985d-63cb990ae3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import nvmath\n",
    "import math\n",
    "\n",
    "from nvmath.device import Matmul\n",
    "from nvmath.device.cublasdx import DevicePipeline, SharedStorageCalc\n",
    "from nvmath.device.cublasdx_numba import pipeline_extensions\n",
    "from nvmath.device.common import axpby, clear, copy, copy_fragment, copy_wait, make_tensor, make_fragment_like\n",
    "from numba import cuda\n",
    "\n",
    "sys.path.append(os.sep.join([\"..\", \"utilities\", \"python\"]))\n",
    "\n",
    "from benchmark import *\n",
    "from emulation_utils import get_width, epilogue_ldexp, MatrixHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37722a1c-a7b1-4a3b-b191-8158644b2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "  (8192, 8192, 0.9, 1.1, 'L'),\n",
    "  (8192, 8192, 0.9, 1.1, 'U'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb5d86-e55b-4f2d-a423-4abe53e38095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emulated_dsyrk_kernel(BLAS, matrix_half):\n",
    "    \n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    TILE_M, TILE_N = BLAS.c_dim\n",
    "    TILE_K = BLAS.a_dim[1]\n",
    "    BLOCK_SIZE = BLAS.block_size\n",
    "    ALIGNMENT = min(BLAS.alignment.a, min(BLAS.alignment.b, BLAS.alignment.c))\n",
    "\n",
    "    uint8_width = get_width(np.uint8)\n",
    "\n",
    "    assert TILE_M == TILE_N, \"Invalid SYRK configuration\"\n",
    "    is_lower = (matrix_half == MatrixHalf.lower)\n",
    "    \n",
    "    @cuda.jit(device=True, forceinline=True)\n",
    "    def nth_slice_to_fp64(nth, nth_slice, exponent_shift):\n",
    "        ko = math.pow(2.0, -nth * uint8_width)\n",
    "\n",
    "        value = ko * np.float64(nth_slice)\n",
    "        return epilogue_ldexp(value, -exponent_shift)\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLOCK_SIZE, 1))\n",
    "    def dsyrk_kernel(slices, shift_a_tensor, alpha, beta, tensor_c, device_pipeline: DevicePipeline):\n",
    "        # CHALLENGE EXERCISE --> Implement a fused SYRK kernel\n",
    "        pass\n",
    "\n",
    "    return dsyrk_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522600b-2efa-446e-8dad-17e3fe57ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_dsyrk_ozaki(tensor_slicedA_cupy, tensor_c_cupy, tensor_shift_a_cupy, alpha, beta, context, warmup=True):\n",
    "    m, n           = tensor_c_cupy.shape\n",
    "    _, k, slices   = tensor_slicedA_cupy.shape\n",
    "\n",
    "    BLAS = context[\"BLAS\"]\n",
    "    PIPELINE_DEPTH = context[\"PIPELINE_DEPTH\"]\n",
    "    syrk_kernel = context[\"syrk_kernel\"]\n",
    "    grid = context[\"syrk_grid\"]\n",
    "    block = context[\"syrk_block\"]\n",
    "\n",
    "    _, TILE_N = BLAS.c_dim\n",
    "\n",
    "    # Create transposed view of A for A^T\n",
    "    # Swap the shape and strides for the first two dimensions\n",
    "    stride_n, stride_k, stride_slices = tensor_slicedA_cupy.strides\n",
    "    tensor_slicedAT_cupy = cp.ndarray(\n",
    "        shape=(k, m, slices),\n",
    "        dtype=np.int8,\n",
    "        memptr=tensor_slicedA_cupy.data,\n",
    "        strides=(stride_k, stride_n, stride_slices)\n",
    "    )\n",
    "\n",
    "    tensor_slicedA = cuda.as_cuda_array(tensor_slicedA_cupy)\n",
    "    tensor_slicedAT = cuda.as_cuda_array(tensor_slicedAT_cupy)\n",
    "    tensor_shift_a = cuda.as_cuda_array(tensor_shift_a_cupy)\n",
    "    tensor_c = cuda.as_cuda_array(tensor_c_cupy)\n",
    "\n",
    "    device_pipeline = BLAS.suggest_device_pipeline(PIPELINE_DEPTH, tensor_slicedA, tensor_slicedAT)\n",
    "\n",
    "    smem_size = device_pipeline.buffer_size + (TILE_N + TILE_N) * np.dtype(np.int32).itemsize\n",
    "    \n",
    "    if warmup:\n",
    "        set_max_dynamic_shared_size_bytes(syrk_kernel, smem_size,\n",
    "                                            slices, tensor_shift_a, alpha, beta, tensor_c, device_pipeline)\n",
    "    syrk_kernel[grid, block, 0, smem_size](slices, tensor_shift_a, alpha, beta, tensor_c, device_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258a169-e9bd-4f22-8017-ae643ff70aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_func(n, k, matrix_half):\n",
    "    TILE_N = 128\n",
    "    TILE_K = 128\n",
    "    PIPELINE_DEPTH = 3\n",
    "    BLOCK_SIZE = 128\n",
    "    ALIGNMENT = 16\n",
    "    DATA_TYPE = \"real\"\n",
    "\n",
    "    assert n % TILE_N == 0, \"Unsupported dimension n for TILE_N\"\n",
    "    assert k % TILE_K == 0, \"Unsupported dimension n for TILE_N\"\n",
    "    assert k >= (TILE_K * PIPELINE_DEPTH), \"Unsupported pipeline depth for k\"\n",
    "    \n",
    "    BLAS = Matmul(size=(TILE_N, TILE_N, TILE_K),\n",
    "                  precision=(np.int8, np.int8, np.int32),\n",
    "                  data_type=DATA_TYPE,\n",
    "                  alignment=(ALIGNMENT, ALIGNMENT, ALIGNMENT),\n",
    "                  arrangement=(\"row_major\", \"col_major\", \"col_major\"), # Do not change\n",
    "                  execution=\"Block\",\n",
    "                  block_size=BLOCK_SIZE,\n",
    "                  with_pipeline=True,\n",
    "                  enable_input_streaming=True,\n",
    "                  static_block_dim=True)\n",
    "\n",
    "    syrk_grid = (n // TILE_N, n // TILE_N)\n",
    "    syrk_block = BLAS.block_dim\n",
    "\n",
    "    return {\n",
    "        \"BLAS\": BLAS,\n",
    "        \"PIPELINE_DEPTH\": PIPELINE_DEPTH,\n",
    "        \"syrk_kernel\" : get_emulated_dsyrk_kernel(BLAS, matrix_half),\n",
    "        \"syrk_grid\": syrk_grid,\n",
    "        \"syrk_block\": syrk_block,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538a57d-b194-4404-a560-6e6ea4f9f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_fused_emulated_dsyrk(problems, setup_func, fused_dsyrk_ozaki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c44ea6-70f5-43ba-92a1-88eaf4af5211",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ed4a2-07ed-4423-b91b-6be2ec80f725",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_emulated_dsyrk_kernel_solution(BLAS, matrix_half):\n",
    "\n",
    "    assert BLAS.a_value_type == BLAS.b_value_type, \"Invalid BLAS configuration\"\n",
    "\n",
    "    A_SIZE = BLAS.suggest_layout_smem_a().cosize\n",
    "    B_SIZE = BLAS.suggest_layout_smem_b().cosize\n",
    "    C_SIZE = BLAS.suggest_layout_rmem_c().cosize\n",
    "\n",
    "    TILE_M, TILE_N = BLAS.c_dim\n",
    "    TILE_K = BLAS.a_dim[1]\n",
    "    BLOCK_SIZE = BLAS.block_size\n",
    "    ALIGNMENT = min(BLAS.alignment.a, min(BLAS.alignment.b, BLAS.alignment.c))\n",
    "\n",
    "    uint8_width = get_width(np.uint8)\n",
    "\n",
    "    assert TILE_M == TILE_N, \"Invalid SYRK configuration\"\n",
    "    is_lower = (matrix_half == MatrixHalf.lower)\n",
    "    \n",
    "    @cuda.jit(device=True, forceinline=True)\n",
    "    def nth_slice_to_fp64(nth, nth_slice, exponent_shift):\n",
    "        ko = math.pow(2.0, -nth * uint8_width)\n",
    "\n",
    "        value = ko * np.float64(nth_slice)\n",
    "        return epilogue_ldexp(value, -exponent_shift)\n",
    "    \n",
    "    @cuda.jit(extensions=pipeline_extensions, launch_bounds=(BLOCK_SIZE, 1))\n",
    "    def dsyrk_kernel(slices, shift_a_tensor, alpha, beta, tensor_c, device_pipeline: DevicePipeline):\n",
    "        m, n = tensor_c.shape\n",
    "\n",
    "        block_m = cuda.blockIdx.x\n",
    "        block_n = cuda.blockIdx.y\n",
    "\n",
    "        smem_pipeline = cuda.shared.array(shape=(0,), dtype=BLAS.a_value_type, alignment=ALIGNMENT)\n",
    "        \n",
    "        smem_shift_a  = cuda.shared.array(shape=(TILE_M), dtype=np.int32)\n",
    "        smem_shift_at = cuda.shared.array(shape=(TILE_N), dtype=np.int32)\n",
    "\n",
    "        block_start_m = block_m * TILE_M\n",
    "        block_end_m = (block_m + 1) * TILE_M\n",
    "\n",
    "        block_start_n = block_n * TILE_N\n",
    "        block_end_n = (block_n + 1) * TILE_N\n",
    "\n",
    "        # Skip blocks outside the triangular region\n",
    "        if is_lower:\n",
    "            # Lower triangular: skip if block_n > block_m\n",
    "            if block_n > block_m:\n",
    "                return\n",
    "        else:\n",
    "            # Upper triangular: skip if block_m > block_n\n",
    "            if block_m > block_n:\n",
    "                return\n",
    "        \n",
    "        if block_start_m >= m or block_start_n >= n:\n",
    "            return\n",
    "\n",
    "        shift_a_view = shift_a_tensor[block_start_m : block_end_m]\n",
    "        shift_at_view = shift_a_tensor[block_start_n : block_end_n]\n",
    "\n",
    "        tid = cuda.threadIdx.x\n",
    "        if tid < TILE_M:\n",
    "            smem_shift_a[tid] = shift_a_view[tid]\n",
    "        if tid < TILE_N:\n",
    "            smem_shift_at[tid] = shift_at_view[tid]\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        c_view = tensor_c[\n",
    "            block_start_m : block_end_m,\n",
    "            block_start_n : block_end_n,\n",
    "        ]\n",
    "\n",
    "        ldc = max(c_view.strides) // c_view.itemsize\n",
    "        gmem_c = make_tensor(c_view, BLAS.get_layout_gmem_c(ldc))\n",
    "        \n",
    "        initial_diag = slices - 1\n",
    "        initial_term = 0\n",
    "\n",
    "        tile_pipeline = device_pipeline.get_tile(smem_pipeline,\n",
    "                                                 (block_m, np.int32(initial_term)),\n",
    "                                                 (block_n, np.int32(initial_diag)))\n",
    "        \n",
    "        accumulator = BLAS.suggest_accumulator()\n",
    "        beta_used = beta\n",
    "        for diag in range(initial_diag, -1, -1):\n",
    "            \n",
    "            accumulator.clear()\n",
    "            for term in range(initial_term, diag + 1):\n",
    "                tile_pipeline.execute(accumulator)\n",
    "\n",
    "                next_slice_row =          0 if term == diag else term + 1\n",
    "                next_slice_col = (diag - 1) if term == diag else diag - next_slice_row\n",
    "\n",
    "                device_pipeline.reset_tile(tile_pipeline,\n",
    "                                           (block_m, np.int32(next_slice_row)),\n",
    "                                           (block_n, np.int32(next_slice_col)))\n",
    "\n",
    "            if accumulator.is_thread_active():\n",
    "                gemm_results = accumulator.get_results()\n",
    "                c_fp64_frag = make_fragment_like(gemm_results, np.float64)\n",
    "                copy_fragment(gmem_c, c_fp64_frag)\n",
    "\n",
    "                for i in range(c_fp64_frag.layout.size):\n",
    "                    (global_x, global_y) = accumulator.map_fragment_index(i)\n",
    "                    shift_a = smem_shift_a[global_x]\n",
    "                    shift_at = smem_shift_at[global_y]\n",
    "\n",
    "                    syrk_m = block_start_m + global_x\n",
    "                    syrk_n = block_start_n + global_y\n",
    "                    if is_lower:\n",
    "                        in_bounds = (syrk_m >= syrk_n)\n",
    "                    else:\n",
    "                        in_bounds = (syrk_m <= syrk_n)\n",
    "\n",
    "                    value = alpha * nth_slice_to_fp64(diag, gemm_results[i], shift_a + shift_at)\n",
    "                    c_fp64_frag[i] = value + beta_used * c_fp64_frag[i] if in_bounds else c_fp64_frag[i]\n",
    "\n",
    "                accumulator.partition_and_copy(c_fp64_frag, gmem_c)\n",
    "\n",
    "            beta_used = 1.0\n",
    "\n",
    "        tile_pipeline._del()\n",
    "\n",
    "    return dsyrk_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc99c07-5ced-493d-9d76-679ee5e39a99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def setup_func_solution(n, k, matrix_half):\n",
    "    ctx = setup_func(n, k, matrix_half)\n",
    "    BLAS = ctx[\"BLAS\"]\n",
    "    ctx[\"syrk_kernel\"] = get_emulated_dsyrk_kernel_solution(BLAS, matrix_half)\n",
    "\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80aa71f-a5f6-4541-85d2-e539865f7fe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "benchmark_fused_emulated_dsyrk(problems, setup_func_solution, fused_dsyrk_ozaki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bb3ae-e948-4833-b8b0-eb07835f6ba3",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a795c2-fcd8-4b45-baa5-880de4a4ceee",
   "metadata": {},
   "source": [
    "In this chapter you have customized the kernel from 3.3 even further to accelerate a different algorithm, using the underlying flexibility of writing custom kernels with libraries only as building blocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
