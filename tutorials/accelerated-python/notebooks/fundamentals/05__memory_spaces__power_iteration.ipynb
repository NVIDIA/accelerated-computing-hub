{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory Spaces & Power Iteration\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to Memory Spaces](#1-introduction-to-memory-spaces)\n",
        "2. [The CPU Baseline (NumPy)](#2-the-cpu-baseline-numpy)\n",
        "3. [The GPU Port (CuPy)](#3-the-gpu-port-cupy)\n",
        "4. [Optimizing Data Generation](#4-optimizing-data-generation)\n",
        "5. [Verification and Benchmarking](#5-verification-and-benchmarking)\n",
        "6. [Extra Credit](#extra-credit)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction to Memory Spaces\n",
        "\n",
        "Before we implement algorithms on the GPU, we must understand the hardware architecture. A heterogeneous system (like the one you are using) consists of two distinct memory spaces:\n",
        "\n",
        "1.  **Host Memory (CPU):** System RAM. Accessible by the CPU.\n",
        "2.  **Device Memory (GPU):** High-bandwidth memory (HBM) attached to the GPU. Accessible by the GPU.\n",
        "\n",
        "The CPU cannot directly calculate data stored on the GPU, and the GPU cannot directly calculate data stored in System RAM. To perform work on the GPU, you must explicitly manage data movement.\n",
        "\n",
        "* **Host $\\to$ Device:** Move data to the GPU to compute.\n",
        "    * Syntax: `x_device = cp.asarray(x_host)`\n",
        "* **Device $\\to$ Host:** Move results back to the CPU to save to disk, plot with Matplotlib, or print.\n",
        "    * Syntax: `y_host = cp.asnumpy(y_device)`\n",
        "\n",
        "### Implicit Transfers and Synchronization\n",
        "\n",
        "It is crucial to understand when CuPy interacts with the CPU implicitly. These interactions can kill performance because they force the GPU to pause (synchronize) while data moves.\n",
        "\n",
        "CuPy silently transfers and synchronizes when you:\n",
        "1.  **Print** a GPU array (`print(gpu_array)`).\n",
        "2.  **Convert** to a Python scalar (`float(gpu_array)` or `.item()`).\n",
        "3.  **Evaluate** a GPU scalar in a boolean context (`if gpu_scalar > 0:`).\n",
        "\n",
        "### The Task\n",
        "To understand the implications of these concepts, let's experiment with estimating the dominant eigenvalue of a matrix using the **Power Iteration** algorithm.\n",
        "\n",
        "Before we dive into the code, let's understand the math behind the algorithm we are implementing.\n",
        "\n",
        "**Power Iteration** is a classic iterative method used to find the dominant eigenvalue (the eigenvalue with the largest absolute value) and its corresponding eigenvector of a square matrix $A$.\n",
        "\n",
        "#### How It Works\n",
        "\n",
        "The core idea is simple: if you repeatedly multiply a vector by a matrix $A$, the vector will eventually converge towards the dominant eigenvector of $A$, regardless of the initial vector you started with (provided the initial vector has some component in the direction of the dominant eigenvector).\n",
        "\n",
        "#### The Mathematical Steps\n",
        "\n",
        "Given a square matrix $A$ and a random initial vector $x_0$, the algorithm proceeds as follows for each step $k$:\n",
        "\n",
        "**1. Matrix-Vector Multiplication:**\n",
        "\n",
        "We calculate the next approximation of the vector:\n",
        "\n",
        "$$y = A x_k$$\n",
        "\n",
        "**2. Eigenvalue Estimation (Rayleigh Quotient):**\n",
        "\n",
        "We estimate the eigenvalue $\\lambda$ using the current vector. This is essentially projecting $y$ onto $x$:\n",
        "\n",
        "$$\\lambda_k = \\frac{x_k^T y}{x_k^T x_k} = \\frac{x_k^T A x_k}{x_k^T x_k}$$\n",
        "\n",
        "**3. Residual Calculation (Error Check):**\n",
        "\n",
        "We check how close we are to the true definition of an eigenvector ($Ax = \\lambda x$) by calculating the \"residual\" (error):\n",
        "\n",
        "$$r = ||y - \\lambda_k x_k||$$\n",
        "\n",
        "If $r$ is close to 0, we have converged.\n",
        "\n",
        "**4. Normalization:**\n",
        "\n",
        "To prevent the numbers from exploding (overflow) or vanishing (underflow), we normalize the vector for the next iteration:\n",
        "\n",
        "$$x_{k+1} = \\frac{y}{||y||}$$\n",
        "\n",
        "We will start with a standard CPU implementation, port it to the GPU using CuPy, and analyze the performance impact of memory transfers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configuration for the algorithm\n",
        "@dataclass\n",
        "class PowerIterationConfig:\n",
        "    dim: int = 4096          # Matrix size (dim x dim)\n",
        "    dominance: float = 0.1   # How much larger the top eigenvalue is (controls convergence speed)\n",
        "    max_steps: int = 400     # Maximum iterations\n",
        "    check_frequency: int = 10 # Check for convergence every N steps\n",
        "    progress: bool = True    # Print progress logs\n",
        "    residual_threshold: float = 1e-10 # Stop if error is below this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The CPU Baseline (NumPy)\n",
        "\n",
        "We generate a random dense matrix that is diagonalizable. This data is generated on the **Host (CPU)** and resides in **Host Memory**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_host(cfg=PowerIterationConfig()):\n",
        "    \"\"\"Generates a random diagonalizable matrix on the CPU.\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Create eigenvalues: One large one (1.0), the rest smaller\n",
        "    weak_lam = np.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "    lam = np.random.permutation(np.concatenate(([1.0], weak_lam)))\n",
        "\n",
        "    # Construct matrix A = P * D * P^-1\n",
        "    P = np.random.random((cfg.dim, cfg.dim)) # Random invertible matrix\n",
        "    D = np.diag(np.random.permutation(lam))  # Diagonal matrix of eigenvalues\n",
        "    A = ((P @ D) @ np.linalg.inv(P))         # The final matrix\n",
        "    return A\n",
        "\n",
        "# Generate the data on Host\n",
        "print(\"Generating Host Data...\")\n",
        "A_host = generate_host()\n",
        "print(f\"Host Matrix Shape: {A_host.shape}\")\n",
        "print(f\"Data Type: {A_host.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Power Iteration (CPU)\n",
        "\n",
        "As described above, the Power Iteration algorithm repeatedly multiplies a vector $x$ by matrix $A$ ($y = Ax$) and normalizes the result. We initialize this algorithm with a vector of 1s ($x_0$) as our initial guess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_host(A, cfg=PowerIterationConfig()):\n",
        "    \"\"\"\n",
        "    Performs power iteration using purely NumPy (CPU).\n",
        "    \"\"\"\n",
        "    # Initialize vector of ones on Host\n",
        "    x = np.ones(A.shape[0], dtype=np.float64)\n",
        "\n",
        "    for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "        # Matrix-Vector multiplication\n",
        "        y = A @ x\n",
        "        \n",
        "        # Rayleigh quotient: (x . y) / (x . x)\n",
        "        lam = (x @ y) / (x @ x)\n",
        "        \n",
        "        # Calculate residual (error)\n",
        "        res = np.linalg.norm(y - lam * x)\n",
        "        \n",
        "        # Normalize vector for next step\n",
        "        x = y / np.linalg.norm(y)\n",
        "\n",
        "        if cfg.progress:\n",
        "            print(f\"Step {i}: residual = {res:.3e}\")\n",
        "\n",
        "        # Convergence check\n",
        "        if res < cfg.residual_threshold:\n",
        "            break\n",
        "\n",
        "        # Run intermediate steps without checking residual to save compute\n",
        "        for _ in range(cfg.check_frequency - 1):\n",
        "            y = A @ x\n",
        "            x = y / np.linalg.norm(y)\n",
        "\n",
        "    return (x.T @ (A @ x)) / (x.T @ x)\n",
        "\n",
        "# Run CPU Baseline\n",
        "print(\"\\nRunning CPU Estimate...\")\n",
        "start_time = time.time()\n",
        "lam_est_host = estimate_host(A_host)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"\\nEstimated Eigenvalue (CPU): {lam_est_host}\")\n",
        "print(f\"Time taken: {end_time - start_time:.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The GPU Port (CuPy)\n",
        "\n",
        "### Exercise: Port the CPU Implementation to GPU\n",
        "\n",
        "Now it's your turn! Your task is to convert the `estimate_host` function to run on the GPU using CuPy.\n",
        "\n",
        "**Remember the rules of Memory Spaces:**\n",
        "1.  **Transfer:** Move `A_host` from CPU to GPU using `cp.asarray()`.\n",
        "2.  **Compute:** Perform math using `cp` functions on the GPU.\n",
        "3.  **Retrieve:** Move result back to CPU using `cp.asnumpy()` or `.item()` if we need to print it or use it in standard Python.\n",
        "\n",
        "**Hint:** CuPy tries to replicate the NumPy API. In many cases, you can simply change `np.` to `cp.`. However, CuPy operations *must* run on data present in Device Memory.\n",
        "\n",
        "**Fill in the `TODO` sections in the skeleton code below:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_device_exercise(A, cfg=PowerIterationConfig()):\n",
        "    \"\"\"\n",
        "    TODO: Port the power iteration algorithm to the GPU using CuPy.\n",
        "    \n",
        "    Steps to complete:\n",
        "    1. Transfer the input matrix A to the GPU (if it's a numpy array)\n",
        "    2. Initialize the vector x on the GPU\n",
        "    3. Replace np operations with cp operations\n",
        "    4. Return the result as a Python scalar\n",
        "    \"\"\"\n",
        "    # ---------------------------------------------------------\n",
        "    # TODO 1: MEMORY TRANSFER (Host -> Device)\n",
        "    # Check if A is a numpy array. If so, move it to GPU using cp.asarray()\n",
        "    # Otherwise, assume it's already on the device.\n",
        "    # ---------------------------------------------------------\n",
        "    if isinstance(A, np.ndarray):\n",
        "        A_gpu = ...  # TODO: Transfer to GPU\n",
        "    else:\n",
        "        A_gpu = A\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # TODO 2: Initialize vector of ones ON THE GPU\n",
        "    # Hint: Use cp.ones() instead of np.ones()\n",
        "    # ---------------------------------------------------------\n",
        "    x = ...  # TODO: Create vector of ones on GPU\n",
        "    \n",
        "    for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "        # ---------------------------------------------------------\n",
        "        # TODO 3: Perform GPU computations\n",
        "        # Replace the operations below with CuPy equivalents\n",
        "        # ---------------------------------------------------------\n",
        "        \n",
        "        # Matrix-Vector multiplication (this works the same with CuPy!)\n",
        "        y = A_gpu @ x\n",
        "        \n",
        "        # Rayleigh quotient\n",
        "        lam = (x @ y) / (x @ x)\n",
        "        \n",
        "        # TODO: Calculate residual using cp.linalg.norm (not np.linalg.norm)\n",
        "        res = ...\n",
        "        \n",
        "        # TODO: Normalize x using cp.linalg.norm\n",
        "        x = ...\n",
        "        \n",
        "        if cfg.progress:\n",
        "            print(f\"Step {i}: residual = {res:.3e}\")\n",
        "        \n",
        "        if res < cfg.residual_threshold:\n",
        "            break\n",
        "        \n",
        "        for _ in range(cfg.check_frequency - 1):\n",
        "            y = A_gpu @ x\n",
        "            x = y / cp.linalg.norm(y)\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # TODO 4: MEMORY TRANSFER (Device -> Host)\n",
        "    # Return the eigenvalue as a Python scalar using .item()\n",
        "    # ---------------------------------------------------------\n",
        "    result = (x.T @ (A_gpu @ x)) / (x.T @ x)\n",
        "    return ...\n",
        "\n",
        "# Uncomment to test your implementation:\n",
        "# lam_test = estimate_device_exercise(A_host, PowerIterationConfig(max_steps=50))\n",
        "# print(f\"Your result: {lam_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal the solution</summary>\n",
        "\n",
        "The key changes from NumPy to CuPy are:\n",
        "1. `cp.asarray(A)` to transfer data to GPU\n",
        "2. `cp.ones()` instead of `np.ones()` to create arrays on GPU\n",
        "3. `cp.linalg.norm()` instead of `np.linalg.norm()`\n",
        "4. `.item()` to convert GPU scalar back to Python scalar\n",
        "\n",
        "</details>\n",
        "\n",
        "Run the cell below to see the complete implementation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_device(A, cfg=PowerIterationConfig()):\n",
        "    \"\"\"\n",
        "    Performs power iteration using CuPy (GPU).\n",
        "    Handles memory transfer internally.\n",
        "    \"\"\"\n",
        "    # ---------------------------------------------------------\n",
        "    # MEMORY TRANSFER: Host -> Device\n",
        "    # We use cp.asarray to move the numpy array to the GPU.\n",
        "    # ---------------------------------------------------------\n",
        "    if isinstance(A, np.ndarray):\n",
        "        A_gpu = cp.asarray(A)\n",
        "    else:\n",
        "        A_gpu = A  # Already on device\n",
        "\n",
        "    # Initialize vector on Device\n",
        "    x = cp.ones(A_gpu.shape[0], dtype=cp.float64)\n",
        "\n",
        "    for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "        # All operations below happen on the GPU\n",
        "        y = A_gpu @ x\n",
        "        lam = (x @ y) / (x @ x)\n",
        "        \n",
        "        # Note: using cp.linalg, not np.linalg\n",
        "        res = cp.linalg.norm(y - lam * x)\n",
        "        x = y / cp.linalg.norm(y)\n",
        "\n",
        "        if cfg.progress:\n",
        "            # IMPLICIT TRANSFER WARNING:\n",
        "            # Printing a GPU scalar/array forces a download to CPU \n",
        "            # and a synchronization.\n",
        "            print(f\"Step {i}: residual = {res:.3e}\")\n",
        "\n",
        "        # Boolean checks on GPU scalars also force synchronization\n",
        "        if res < cfg.residual_threshold:\n",
        "            break\n",
        "\n",
        "        for _ in range(cfg.check_frequency - 1):\n",
        "            y = A_gpu @ x\n",
        "            x = y / cp.linalg.norm(y)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # MEMORY TRANSFER: Device -> Host\n",
        "    # .item() converts a 0-dim GPU array to a Python scalar\n",
        "    # This implicitly copies data back to the host.\n",
        "    # ---------------------------------------------------------\n",
        "    return ((x.T @ (A_gpu @ x)) / (x.T @ x)).item()\n",
        "\n",
        "print(\"\\nRunning GPU Estimate (Input is Host Array)...\")\n",
        "# Note: The first run might be slower due to compilation/caching overhead\n",
        "start_time = time.time()\n",
        "lam_est_device = estimate_device(A_host)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"\\nEstimated Eigenvalue (GPU): {lam_est_device}\")\n",
        "print(f\"Time taken: {end_time - start_time:.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Optimizing Data Generation\n",
        "\n",
        "In the previous step, we generated data on the CPU and copied it to the GPU. For large datasets, the transfer time (`Host -> Device`) can be a bottleneck. \n",
        "\n",
        "It is almost always faster to **generate** the data directly on the GPU if possible.\n",
        "\n",
        "### Exercise: Generate Data Directly on the GPU\n",
        "\n",
        "Your task is to convert the `generate_host` function to generate the matrix directly on the GPU using CuPy's random functions.\n",
        "\n",
        "**Hints:**\n",
        "- Use `cp.random.seed()` instead of `np.random.seed()`\n",
        "- Use `cp.random.random()` instead of `np.random.random()`\n",
        "- Use `cp.random.permutation()` instead of `np.random.permutation()`\n",
        "- Use `cp.concatenate()`, `cp.array()`, `cp.diag()`, and `cp.linalg.inv()`\n",
        "\n",
        "**Fill in the `TODO` sections in the skeleton code below:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_device_exercise(cfg=PowerIterationConfig()):\n",
        "    \"\"\"\n",
        "    TODO: Generate a random diagonalizable matrix directly on the GPU.\n",
        "    \n",
        "    This should mirror the generate_host function but use CuPy instead of NumPy.\n",
        "    The key benefit: no Host->Device transfer needed!\n",
        "    \"\"\"\n",
        "    # ---------------------------------------------------------\n",
        "    # TODO 1: Set the random seed on the GPU\n",
        "    # ---------------------------------------------------------\n",
        "    ...  \n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # TODO 2: Create eigenvalues on the GPU\n",
        "    # Generate (dim-1) random values, scale them, then combine with 1.0\n",
        "    # ---------------------------------------------------------\n",
        "    # TODO: Generate weak eigenvalues using cp.random.random()\n",
        "    weak_lam = ...\n",
        "    \n",
        "    # TODO: Concatenate [1.0] with weak_lam using cp.concatenate and cp.array\n",
        "    # Then permute them using cp.random.permutation\n",
        "    lam = ...\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # TODO 3: Construct the matrix A = P * D * P^-1 on the GPU\n",
        "    # ---------------------------------------------------------\n",
        "    # TODO: Generate random matrix P using cp.random.random()\n",
        "    P = ...\n",
        "    \n",
        "    # TODO: Create diagonal matrix D using cp.diag()\n",
        "    D = ...\n",
        "    \n",
        "    # TODO: Compute A = P @ D @ P^-1 using cp.linalg.inv()\n",
        "    A = ...\n",
        "    \n",
        "    return A\n",
        "\n",
        "# Uncomment to test your implementation:\n",
        "# A_test = generate_device_exercise()\n",
        "# print(f\"Matrix shape: {A_test.shape}\")\n",
        "# print(f\"Matrix is on GPU: {isinstance(A_test, cp.ndarray)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal the solution</summary>\n",
        "\n",
        "The conversion is straightforward - replace all `np.` calls with `cp.` equivalents:\n",
        "- `np.random.seed(42)` → `cp.random.seed(42)`\n",
        "- `np.random.random()` → `cp.random.random()`\n",
        "- `np.random.permutation()` → `cp.random.permutation()`\n",
        "- `np.concatenate()` → `cp.concatenate()`\n",
        "- `np.array()` → `cp.array()`\n",
        "- `np.diag()` → `cp.diag()`\n",
        "- `np.linalg.inv()` → `cp.linalg.inv()`\n",
        "\n",
        "</details>\n",
        "\n",
        "Run the cell below to see the complete implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_device(cfg=PowerIterationConfig()):\n",
        "    \"\"\"Generates random matrix directly on the GPU.\"\"\"\n",
        "    # Use cupy.random instead of numpy.random\n",
        "    cp.random.seed(42)\n",
        "\n",
        "    weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "    # cp.concatenate joins arrays on the GPU\n",
        "    lam = cp.random.permutation(cp.concatenate((cp.array([1.0]), weak_lam)))\n",
        "\n",
        "    P = cp.random.random((cfg.dim, cfg.dim))\n",
        "    D = cp.diag(cp.random.permutation(lam))\n",
        "    A = ((P @ D) @ cp.linalg.inv(P))\n",
        "    return A\n",
        "\n",
        "print(\"\\nGenerating Data directly on GPU...\")\n",
        "start_time = time.time()\n",
        "A_device = generate_device()\n",
        "end_time = time.time()\n",
        "print(f\"Generation time: {end_time - start_time:.4f}s\")\n",
        "\n",
        "print(\"Running GPU Estimate (Input is Device Array)...\")\n",
        "start_time = time.time()\n",
        "# No transfer overhead here because A_device is already on GPU\n",
        "lam_est_device_gen = estimate_device(A_device)\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "end_time = time.time()\n",
        "print(f\"Compute time: {end_time - start_time:.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verification and Benchmarking\n",
        "\n",
        "Finally, let's verify our accuracy against a reference implementation (`numpy.linalg.eigvals`) and benchmark the speedup.\n",
        "\n",
        "**Note on CuPy Limitations:** You might wonder why we use `np.linalg.eigvals` on the CPU instead of a CuPy equivalent. The reason is that CuPy does not yet implement `eigvals`. While CuPy covers a large portion of the NumPy API, it does not support every function. Always check the [CuPy documentation](https://docs.cupy.dev/en/stable/reference/comparison.html) to verify which functions are available before assuming a direct NumPy-to-CuPy conversion will work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Calculating Reference Eigenvalue (numpy.linalg)...\")\n",
        "# Note: calculating all eigenvalues is computationally expensive\n",
        "lam_ref = np.linalg.eigvals(A_host).real.max()\n",
        "\n",
        "print(f\"\\n--- Results ---\")\n",
        "print(f\"Reference: {lam_ref}\")\n",
        "print(f\"CPU Est:   {lam_est_host}\")\n",
        "print(f\"GPU Est:   {lam_est_device_gen}\")\n",
        "\n",
        "# Assert correctness\n",
        "np.testing.assert_allclose(lam_est_host, lam_ref, rtol=1e-4)\n",
        "np.testing.assert_allclose(lam_est_device_gen, lam_ref, rtol=1e-4)\n",
        "print(\"\\nAccuracy verification passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmarking with `%timeit`\n",
        "\n",
        "We turn off progress printing to measure raw computation speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Benchmarking ---\")\n",
        "\n",
        "# 1. CPU\n",
        "print(\"Timing CPU...\")\n",
        "t_cpu = %timeit -o -q estimate_host(A_host, PowerIterationConfig(progress=False))\n",
        "\n",
        "# 2. GPU (with transfer overhead)\n",
        "print(\"Timing GPU (Host Input)...\")\n",
        "t_gpu_transfer = %timeit -o -q estimate_device(A_host, PowerIterationConfig(progress=False))\n",
        "\n",
        "# 3. GPU (Pure device)\n",
        "print(\"Timing GPU (Device Input)...\")\n",
        "t_gpu_pure = %timeit -o -q estimate_device(A_device, PowerIterationConfig(progress=False))\n",
        "\n",
        "print(f\"\\nAverage Execution Times:\")\n",
        "print(f\"CPU:                 {t_cpu.average:.4f} s\")\n",
        "print(f\"GPU (with transfer): {t_gpu_transfer.average:.4f} s\")\n",
        "print(f\"GPU (pure):          {t_gpu_pure.average:.4f} s\")\n",
        "\n",
        "speedup = t_cpu.average / t_gpu_pure.average\n",
        "print(f\"\\nSpeedup: {speedup:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Extra Credit\n",
        "\n",
        "**Explore the impact of changing the following parameters:**\n",
        "\n",
        "1. **Problem Size (`dim`):** How does the GPU speedup change as you increase or decrease the matrix dimensions? Try values like 1024, 2048, 4096, 8192.\n",
        "\n",
        "2. **Compute Workload (`max_steps` and `dominance`):** The `dominance` parameter controls how quickly the algorithm converges. A smaller dominance means eigenvalues are closer together, requiring more iterations. How does this affect the CPU vs GPU comparison?\n",
        "\n",
        "3. **Check Frequency (`check_frequency`):** This controls how often we check for convergence (and trigger implicit CPU synchronization via the print statement). What happens to GPU performance when you check every step (`check_frequency=1`) vs. less frequently (`check_frequency=50`)?\n",
        "\n",
        "**Experiment below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different configurations here!\n",
        "# Example:\n",
        "# cfg_large = PowerIterationConfig(dim=8192, progress=False)\n",
        "# cfg_slow_converge = PowerIterationConfig(dominance=0.01, progress=False)\n",
        "# cfg_frequent_check = PowerIterationConfig(check_frequency=1, progress=True)\n",
        "\n",
        "# Your experiments:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (RAPIDS 25.10)",
      "language": "python",
      "name": "cudf-cu12-25.10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
