{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9a5bf9",
   "metadata": {},
   "source": [
    "## CUDA Core Tutorial - Low-Level GPU Programming\n",
    "### Table of Contents\n",
    "\n",
    "1. Introduction to cuda.core\n",
    "2. Setting Up Your Environment\n",
    "3. Understanding CUDA Concepts\n",
    "4. Memory Management\n",
    "5. Kernel Compilation and Execution\n",
    "6. Error Handling\n",
    "7. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a456f7",
   "metadata": {},
   "source": [
    "### 1. Introduction to cuda.core\n",
    "The `cuda.core` module provides direct access to the CUDA driver API, giving you maximum control over GPU programming. Unlike high-level APIs, cuda.core requires you to manage everything manually:\n",
    "\n",
    "* Context management: Creating and managing execution contexts\n",
    "* Memory allocation: Explicitly allocating and freeing GPU memory\n",
    "* Kernel compilation: Compiling CUDA C/C++ code at runtime\n",
    "* Synchronization: Managing streams and events\n",
    "\n",
    "#### When to use cuda.core:\n",
    "**Great for:**\n",
    "* Learning GPU programming in a Python-friendly way\n",
    "* Prototyping GPU algorithms quickly\n",
    "* Custom parallel algorithms that existing libraries don't provide\n",
    "* When you need fine control over GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b043e1",
   "metadata": {},
   "source": [
    "### 2. Setting Up Your Environment\n",
    "#### Prerequisites\n",
    "\n",
    "* NVIDIA GPU with CUDA capability\n",
    "* CUDA driver version 12.2 or higher\n",
    "* Python 3.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dbcf1",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0771178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"cuda-core[cu12]\" numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ba0c0",
   "metadata": {},
   "source": [
    "**What this does**:\n",
    "* `cuda-core`: Installs the CUDA Core Python package\n",
    "* `numpy`: Installs NumPy for array operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2622788",
   "metadata": {},
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea7dbe",
   "metadata": {},
   "source": [
    "The following Python snippet checks whether CUDA is everything is set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import system, Device\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA driver version\n",
    "print(f\"CUDA driver version: {system.driver_version}\")\n",
    "\n",
    "# Get number of available devices\n",
    "print(f\"Number of CUDA devices: {system.num_devices}\")\n",
    "\n",
    "# Get device information\n",
    "if system.num_devices > 0:\n",
    "    device = Device(0)  # Get the first GPU\n",
    "    device.set_current()  # Tell CUDA we want to use this GPU\n",
    "    print(f\"Device name: {device.name}\")\n",
    "    print(f\"Device UUID: {device.uuid}\")\n",
    "    print(f\"PCI Bus ID: {device.pci_bus_id}\")\n",
    "else:\n",
    "    print(\"No CUDA devices found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df933877",
   "metadata": {},
   "source": [
    "This is a good first step before running any GPU workloads. If this fails, your drivers or installation may be incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76daf10e",
   "metadata": {},
   "source": [
    "### 3. Understanding CUDA Concepts\n",
    "#### Key Terminology\n",
    "**Host vs Device:**\n",
    "\n",
    "* **Host**: Your CPU and system memory\n",
    "* **Device**: Your GPU and video memory\n",
    "\n",
    "**Execution Model**:\n",
    "\n",
    "* **Kernel**: A function that runs on the GPU\n",
    "* **Thread**: Individual execution unit\n",
    "* **Block**: Group of threads that can cooperate\n",
    "* **Grid**: Collection of blocks\n",
    "\n",
    "**Memory Hierarchy**:\n",
    "\n",
    "* **Global Memory**: Main GPU memory (slow but large)\n",
    "* **Shared Memory**: Fast memory shared within a block\n",
    "* **Registers**: Fastest memory, private to each thread\n",
    "\n",
    "Think of registers as your desk (fast access, limited space), shared memory as your team's filing cabinet (fast for team members, more space), and global memory as the company warehouse (lots of space, but takes time to fetch things)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e7496",
   "metadata": {},
   "source": [
    "#### Device Management and Context\n",
    "**What is a Device and Context?**\n",
    "In CUDA, a **Device** represents your GPU hardware. A **Context** is like a workspace on that GPU where your programs can run. Think of the Device as the physical GPU card, and the Context as your personal workspace on that card."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2368b7",
   "metadata": {},
   "source": [
    "Try starting with basic device management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device\n",
    "\n",
    "# Get the first GPU device (device 0)\n",
    "device = Device(0)\n",
    "\n",
    "# Set as current device (this creates and activates a context)\n",
    "device.set_current()\n",
    "\n",
    "print(f\"Using device: {device.name}\")\n",
    "print(f\"Device id: {device.device_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d9594",
   "metadata": {},
   "source": [
    "**What this does**:\n",
    "1. `Device(0)`: Creates a Device object representing the first GPU (GPU numbering starts at 0)\n",
    "1. `device.set_current()`: Tells CUDA \"I want to use this GPU for my operations\"\n",
    "\n",
    "If you have multiple GPUs, CUDA needs to know which one you want to use, which is why we need `set_current`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeef144",
   "metadata": {},
   "source": [
    "#### Getting Device Properties\n",
    "Let's learn more about our GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device\n",
    "\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Get device properties\n",
    "props = device.properties\n",
    "print(f\"Device name: {device.name}\")\n",
    "print(f\"Compute capability: {props.compute_capability_major}.{props.compute_capability_minor}\")\n",
    "print(f\"Multiprocessor count: {props.multiprocessor_count}\")\n",
    "print(f\"Max threads per block: {props.max_threads_per_block}\")\n",
    "print(f\"Max block dimensions: ({props.max_block_dim_x}, {props.max_block_dim_y}, {props.max_block_dim_z})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034ea5b",
   "metadata": {},
   "source": [
    "**What these properties mean**:\n",
    "\n",
    "* Compute capability: Like a GPU \"version number\", where higher numbers support more features\n",
    "* Multiprocessor count: How many \"processor groups\" your GPU has (more = more parallel power)\n",
    "* Max threads per block: Maximum number of threads you can put in one block\n",
    "* Max block dimensions: How you can arrange threads in a block (1D, 2D, or 3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04351b",
   "metadata": {},
   "source": [
    "#### Device Synchronization\n",
    "Sometimes you need to wait for the GPU to finish all its work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# Wait for all previous operations on this device to complete\n",
    "device.sync()\n",
    "print(\"All GPU operations are now complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd5dbf",
   "metadata": {},
   "source": [
    "**When do you need synchronization?**\n",
    "* Before copying results back from GPU to CPU\n",
    "* Before timing how long GPU operations took\n",
    "* Before shutting down your program\n",
    "\n",
    "Almost as if you're saying: \"don't continue until all workers finish their current tasks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340cd1b6",
   "metadata": {},
   "source": [
    "### 4. Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b179d94",
   "metadata": {},
   "source": [
    "#### Understanding GPU Memory\n",
    "GPU memory is separate from your computer's main memory (RAM). To use the GPU, you need to:\n",
    "* Allocate space in GPU memory\n",
    "* Copy your data from CPU to GPU\n",
    "* Process the data on the GPU\n",
    "* Copy results back from GPU to CPU\n",
    "\n",
    "CPUs and GPUs have separate memory systems optimized for their different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda.core.experimental import Device\n",
    "\n",
    "# Initialize our GPU\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Calculate how much memory we need\n",
    "# We want to store 1000 float32 numbers\n",
    "# Each float32 takes 4 bytes, so we need 1000 * 4 = 4000 bytes\n",
    "size_bytes = 1000 * 4\n",
    "\n",
    "# Allocate memory on the GPU\n",
    "device_buffer = device.allocate(size_bytes)\n",
    "\n",
    "print(f\"Allocated {size_bytes} bytes on GPU\")\n",
    "print(f\"Buffer memory address: {device_buffer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf168b",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "1. Calculate size: We figure out how many bytes we need (1000 floats × 4 bytes each)\n",
    "2. Allocate memory: `device.allocate()` reserves space on the GPU\n",
    "3. Get a buffer: The returned device_buffer is like a \"handle\" to our GPU memory\n",
    "\n",
    "**Important**: Just like with regular Python programming, allocating memory doesn't put any meaningful data there yet. It's just reserved empty space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86c5a",
   "metadata": {},
   "source": [
    "### 5. Kernel Compilation and Execution\n",
    "#### First Kernel: Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28d657-0ea7-4592-98ae-d06c16b620ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Device, Program\n",
    "import numpy as np\n",
    "\n",
    "# CUDA C++ source code for our kernel\n",
    "vector_add_source = \"\"\"\n",
    "extern \"C\" __global__ void vector_add(float *a, float *b, float *c, int n) {\n",
    "    // Each thread calculates its unique index\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    // Make sure we don't go beyond our array bounds\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] + b[i];  // Add corresponding elements\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize our GPU\n",
    "device = Device(0)\n",
    "device.set_current()\n",
    "\n",
    "# Compile the CUDA code into a program\n",
    "program = Program(vector_add_source, code_type='c++')\n",
    "compiled_program = program.compile(target_type='cubin')\n",
    "\n",
    "# Get the specific kernel function we want to use\n",
    "kernel = compiled_program.get_kernel(\"vector_add\")\n",
    "\n",
    "print(\"Kernel compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c9216",
   "metadata": {},
   "source": [
    "**Breakdown:**\n",
    "\n",
    "1. `extern \"C\"`: Tells the compiler to use C-style function names\n",
    "2. `__global__`: Marks this as a kernel function (runs on GPU)\n",
    "3. `float *a, float *b, float *c`: Pointers to arrays in GPU memory\n",
    "4. `tid = threadIdx.x + blockIdx.x * blockDim.x`: Calculates unique thread ID\n",
    "* threadIdx.x: Position of this thread within its block\n",
    "* blockIdx.x: Which block this thread belongs to\n",
    "* blockDim.x: How many threads are in each block\n",
    "6. `c[i] = a[i] + b[i]`: The actual computation each thread performs\n",
    "\n",
    "**Why the complex index calculation?**\n",
    "Imagine you have 1000 elements to process with blocks of 256 threads:\n",
    "* Block 0: threads 0-255 handle elements 0-255\n",
    "* Block 1: threads 0-255 handle elements 256-511\n",
    "* Block 2: threads 0-255 handle elements 512-767\n",
    "* Block 3: threads 0-255 handle elements 768-999\n",
    "\n",
    "Each thread needs to know which element it should work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f1ae1-975d-437b-abbd-db432c259761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cuda.core.experimental import launch, LaunchConfig\n",
    "\n",
    "def execute_vector_add():\n",
    "    # Initialize device and create a stream\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    s = device.create_stream()\n",
    "\n",
    "    # Prepare our test data\n",
    "    N = 1000  # Number of elements\n",
    "    a = np.arange(N, dtype=np.float32)      # [0, 1, 2, ..., 999]\n",
    "    b = np.arange(N, dtype=np.float32)      # [0, 1, 2, ..., 999]\n",
    "    print(f\"Input arrays have {N} elements each\")\n",
    "\n",
    "    # Step 2: Copy input data from CPU to GPU\n",
    "    d_a = cp.asarray(a)\n",
    "    d_b = cp.asarray(b)\n",
    "    d_c = cp.empty(N, dtype=cp.float32)\n",
    "\n",
    "    # Configure how to launch the kernel\n",
    "    block_size = 256  # Number of threads per block\n",
    "    grid_size = (N + block_size - 1) // block_size  # Number of blocks needed\n",
    "\n",
    "    print(f\"Launch configuration: {grid_size} blocks of {block_size} threads each\")\n",
    "    print(f\"Total threads: {grid_size * block_size}\")\n",
    "\n",
    "    # Create the launch configuration\n",
    "    config = LaunchConfig(grid=(grid_size,), block=(block_size,))\n",
    "\n",
    "    # Launch the kernel\n",
    "    launch(s, config, kernel, d_a.data.ptr, d_b.data.ptr, d_c.data.ptr, cp.uint64(N))\n",
    "    s.sync() # Wait for kernel to complete\n",
    "    print(\"Kernel launched and executed\")\n",
    "\n",
    "    # Copy the result back from GPU to CPU\n",
    "    c = cp.asnumpy(d_c)\n",
    "    print(\"Results copied back to CPU\")\n",
    "\n",
    "    return c\n",
    "\n",
    "# Execute our vector addition\n",
    "result = execute_vector_add()\n",
    "\n",
    "# Verify the result\n",
    "expected = np.arange(1000, dtype=np.float32) * 2  # [0, 2, 4, ..., 1998]\n",
    "success = np.allclose(result, expected)\n",
    "print(f\"Kernel execution successful: {success}\")\n",
    "print(f\"First 10 results: {result[:10]}\")\n",
    "print(f\"Expected first 10: {expected[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987fc87",
   "metadata": {},
   "source": [
    "**Launch Configuration Deep Dive:**\n",
    "* Block size: 256 threads per block (common choice, powers of 2 work well)\n",
    "* Grid size: `(N + block_size - 1) // block_size` ensures we have enough threads\n",
    "    * For N=1000 and block_size=256: grid_size = (1000 + 255) // 256 = 4 blocks\n",
    "    * Total threads = 4 × 256 = 1024 threads (more than our 1000 elements, which is fine)\n",
    "\n",
    "**Why Use grid_size calculation?**\n",
    "\n",
    "This formula ensures we always have enough threads:\n",
    "* If N=1000 and block_size=256, we need at least 4 blocks\n",
    "* If N=256 and block_size=256, we need exactly 1 block\n",
    "* If N=257 and block_size=256, we need 2 blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d06f97",
   "metadata": {},
   "source": [
    "#### Advanced Kernel Example\n",
    "\n",
    "We can now try multiplying two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templated matrix multiplication kernel\n",
    "matmul_source = \"\"\"\n",
    "template<typename T>\n",
    "__global__ void matrix_multiply(const T *A, const T *B, T *C, size_t N) {\n",
    "    // Calculate which row and column this thread handles\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Make sure we're within the matrix bounds\n",
    "    if (row < N && col < N) {\n",
    "        T sum = T(0);\n",
    "\n",
    "        // Compute dot product of row from A and column from B\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "\n",
    "        // Store the result\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def matrix_multiply_gpu(A, B):\n",
    "    # Initialize device and create a stream\n",
    "    device = Device(0)\n",
    "    device.set_current()\n",
    "    s = device.create_stream()\n",
    "\n",
    "    N = A.shape[0]\n",
    "    assert A.shape == (N, N) and B.shape == (N, N), \"Matrices must be square and same size\"\n",
    "    print(f\"Multiplying {N}x{N} matrices\")\n",
    "\n",
    "    # Compile the templated matrix multiplication kernel with specific C++ compiler flags\n",
    "    program_options = ProgramOptions(std=\"c++17\", arch=f\"sm_{arch}\")\n",
    "    program = Program(matmul_source, code_type='c++', options=program_options)\n",
    "    compiled_program = program.compile(target_type='cubin', name_expressions=(\"matrix_multiply<float>\",))\n",
    "    kernel = compiled_program.get_kernel(\"matrix_multiply<float>\")\n",
    "\n",
    "    # Copy input matrices to GPU\n",
    "    d_A = cp.asarray(A)\n",
    "    d_B = cp.asarray(B)\n",
    "    d_C = cp.empty((N, N), dtype=cp.float32)\n",
    "\n",
    "    # Configure 2D launch (threads arranged in a 2D grid)\n",
    "    block_size = 16  # 16x16 = 256 threads per block\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "    print(f\"Launch config: {grid_size}x{grid_size} blocks of {block_size}x{block_size} threads\")\n",
    "\n",
    "    # Create 2D launch configuration\n",
    "    config = LaunchConfig(grid=(grid_size, grid_size), block=(block_size, block_size))\n",
    "\n",
    "    # Launch the kernel\n",
    "    launch(s, config, kernel, d_A.data.ptr, d_B.data.ptr, d_C.data.ptr, cp.uint64(N))\n",
    "    s.sync()\n",
    "\n",
    "    # Copy result back\n",
    "    C = cp.asnumpy(d_C)\n",
    "\n",
    "    print(\"Matrix multiplication completed on GPU\")\n",
    "    return C\n",
    "\n",
    "print(\"Testing matrix multiplication...\")\n",
    "A = np.random.random((64, 64)).astype(np.float32)\n",
    "B = np.random.random((64, 64)).astype(np.float32)\n",
    "\n",
    "# Compare GPU result with CPU result\n",
    "C_gpu = matrix_multiply_gpu(A, B)\n",
    "C_cpu = np.dot(A, B) # NumPy's optimized matrix multiplication\n",
    "\n",
    "# Check if results match (within floating-point precision)\n",
    "matches = np.allclose(C_gpu, C_cpu, atol=1e-5)\n",
    "print(f\"GPU and CPU results match: {matches}\")\n",
    "\n",
    "if matches:\n",
    "    print(\"Success! Matrix multiplication kernel works correctly.\")\n",
    "else:\n",
    "    print(\"Results don't match - there might be a bug in the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae789cd4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f5601",
   "metadata": {},
   "source": [
    "**Kernel Explanation:**\n",
    "\n",
    "1. 2D Thread Layout: Each thread handles one element of the result matrix\n",
    "* row = blockIdx.y * blockDim.y + threadIdx.y: Which row this thread computes\n",
    "* col = blockIdx.x * blockDim.x + threadIdx.x: Which column this thread computes\n",
    "2. Dot Product Calculation: For element C[row][col], we compute:\n",
    "* Sum of A[row][k] × B[k][col] for all k\n",
    "* This is the mathematical definition of matrix multiplication\n",
    "3. 2D Launch Configuration:\n",
    "* Blocks are arranged in a 2D grid to match the 2D nature of matrices\n",
    "* Each block is 16×16 threads (256 total threads per block)\n",
    "\n",
    "**Why 2D layout?**\n",
    "\n",
    "Matrix multiplication naturally maps to 2D: each thread computes one output element, and output elements are arranged in a 2D grid (the result matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9258bd0",
   "metadata": {},
   "source": [
    "### 6. Error Handling\n",
    "When working at the low-level CUDA driver API level, you are responsible for checking erros after every API call\n",
    "\n",
    "GPU programming can fail in many ways:\n",
    "* Out of memory: Asking for more GPU memory than available\n",
    "* Invalid kernels: Bugs in CUDA C code\n",
    "* Device errors: Hardware problems or driver issues\n",
    "* Launch failures: Invalid grid/block configurations\n",
    "\n",
    "Good error handling helps you:\n",
    "* Debug problems quickly\n",
    "* Write robust applications\n",
    "* Provide helpful error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cuda_operation():\n",
    "    device = None\n",
    "    try:\n",
    "        print(\"Attempting CUDA operation...\")\n",
    "\n",
    "        # Initialize device (this can fail)\n",
    "        device = Device(0)\n",
    "        device.set_current()\n",
    "        print(\"✓ Device initialized successfully\")\n",
    "\n",
    "        # Allocate memory (this can fail if requesting too much)\n",
    "        buffer = device.allocate(1000 * 4)\n",
    "        print(\"✓ Memory allocated successfully\")\n",
    "\n",
    "        # Your CUDA operations here\n",
    "        print(\"✓ All CUDA operations completed successfully\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"✗ CUDA Runtime Error: {e}\")\n",
    "        print(\"This usually means a problem with GPU drivers or hardware\")\n",
    "\n",
    "    except MemoryError as e:\n",
    "        print(f\"✗ GPU Memory Error: {e}\")\n",
    "        print(\"Try reducing the size of your data or closing other GPU programs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Unexpected error occurred: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup happens automatically in cuda.core\n",
    "        # But you can add custom cleanup here if needed\n",
    "        if device is not None:\n",
    "            print(\"✓ Cleanup completed\")\n",
    "\n",
    "# Test error handling\n",
    "safe_cuda_operation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff3f16",
   "metadata": {},
   "source": [
    "Error handling best practices:\n",
    "1. Use try-except blocks: Wrap CUDA operations in try-except\n",
    "2. Specific exceptions: Catch specific error types when possible\n",
    "3. Helpful messages: Explain what went wrong and how to fix it\n",
    "4. Cleanup: Use finally blocks for cleanup code\n",
    "5. Don't ignore errors: Always handle or propagate exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391999d2",
   "metadata": {},
   "source": [
    "#### Common Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_common_errors():\n",
    "    errors_encountered = []\n",
    "\n",
    "    # Error 1: Invalid device number\n",
    "    print(\"Testing invalid device...\")\n",
    "    try:\n",
    "        invalid_device = Device(999)  # Device 999 probably doesn't exist\n",
    "        invalid_device.set_current()\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Invalid device error: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "\n",
    "    # Error 2: Memory allocation failure\n",
    "    print(\"\\nTesting memory allocation failure...\")\n",
    "    try:\n",
    "        device = Device(0)\n",
    "        device.set_current()\n",
    "\n",
    "        # Try to allocate an impossibly large amount of memory (1TB)\n",
    "        huge_amount = 1024 * 1024 * 1024 * 1024  # 1TB in bytes\n",
    "        huge_alloc = device.allocate(huge_amount)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Memory allocation error: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "\n",
    "    # Error 3: Invalid kernel compilation\n",
    "    print(\"\\nTesting kernel compilation failure...\")\n",
    "    try:\n",
    "        # This CUDA code has syntax errors\n",
    "        bad_source = \"\"\"\n",
    "        extern \"C\" __global__ void broken_kernel( {\n",
    "            // Missing closing brace and parameter list\n",
    "            int i = this_variable_doesnt_exist;\n",
    "        \"\"\"\n",
    "        program = Program(bad_source, code_type='c++')\n",
    "        program.compile(target_type='cubin')\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Kernel compilation error: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "\n",
    "    # Error 4: Invalid launch configuration\n",
    "    print(\"\\nTesting invalid launch configuration...\")\n",
    "    try:\n",
    "        device = Device(0)\n",
    "        device.set_current()\n",
    "\n",
    "        # Create a valid kernel first\n",
    "        valid_source = \"\"\"\n",
    "        extern \"C\" __global__ void test_kernel(float *data, int n) {\n",
    "            int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (i < n) data[i] = i;\n",
    "        }\n",
    "        \"\"\"\n",
    "        program = Program(valid_source, code_type='c++')\n",
    "        kernel = program.compile(target_type='cubin').get_kernel(\"test_kernel\")\n",
    "\n",
    "        # Try to launch with invalid configuration (0 threads per block)\n",
    "        bad_config = LaunchConfig(grid=(1,), block=(0,))  # 0 threads is invalid\n",
    "\n",
    "        # This should fail\n",
    "        data = device.allocate(100 * 4)\n",
    "        launch(kernel, bad_config, data, np.int32(100))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Invalid launch configuration: {e}\"\n",
    "        errors_encountered.append(error_msg)\n",
    "        print(f\"✗ {error_msg}\")\n",
    "\n",
    "    print(f\"\\nSummary: Caught {len(errors_encountered)} expected errors\")\n",
    "    return errors_encountered\n",
    "\n",
    "# Run error handling tests\n",
    "errors = handle_common_errors()\n",
    "print(\"\\nError handling demonstration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc87bff",
   "metadata": {},
   "source": [
    "What this demonstrates:\n",
    "1. **Device errors**: Wrong device numbers, missing GPUs\n",
    "2. **Memory errors**: Requesting too much memory\n",
    "3. **Compilation errors**: Syntax errors in CUDA C code\n",
    "4. **Launch errors**: Invalid thread/block configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483d50f-5ea1-40c2-a1eb-a7725a0c9a35",
   "metadata": {},
   "source": [
    "### 7. Exercise: Vector Operations\n",
    "Write a CUDA kernel that performs element-wise multiplication of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a8aac-ee90-4a81-92c3-9cee5e4c0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile your kernel here\n",
    "from cuda.core.experimental import Device, Program\n",
    "\n",
    "multiply_kernel_source = \"\"\"\n",
    "// TODO: Implement vector multiplication kernel\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1c787-ac16-48c3-b6fd-72c33c32e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch your kernel here\n",
    "import cupy as cp\n",
    "from cuda.core.experimental import launch, LaunchConfig\n",
    "\n",
    "def vector_multiply(a, b):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b650263-4dc9-47b1-ac07-2bef0dab130f",
   "metadata": {},
   "source": [
    "### Resources\n",
    "CUDA Python Reference: https://numba.pydata.org/numba-doc/dev/cuda-reference/\n",
    "\n",
    "Repository: https://github.com/NVIDIA/cuda-python "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}