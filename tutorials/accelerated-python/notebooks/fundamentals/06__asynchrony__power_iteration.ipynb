{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KP1pYqmtXdr"
      },
      "source": [
        "# Exercise - Asynchrony - Power Iteration\n",
        "\n",
        "GPU programming is inherently asynchronous - in this exercise, we'll learn the implications that has when using CuPy, and how we can understand and analyze the flow of execution in our code.\n",
        "\n",
        "We'll revisit our power iteration example from earlier for this exercise.\n",
        "\n",
        "First, we need to make sure the Nsight Systems profiler, Nsightful, and NVTX are available in our notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO4kOPuP_0JG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab\n",
        "  !curl -s -L -O https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2025_3/NsightSystems-linux-cli-public-2025.3.1.90-3582212.deb\n",
        "  !sudo dpkg -i NsightSystems-linux-cli-public-2025.3.1.90-3582212.deb > /dev/null\n",
        "  !pip install \"nvtx\" \"nsightful[notebook] @ git+https://github.com/brycelelbach/nsightful.git\" > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sUvjAtMxI3h"
      },
      "source": [
        "All GPU work is launched asynchronously on a stream. The work items in a stream are executed in order. If you launch `f` on a stream and later launch `g` on that same stream, then `f` will be executed before `g`. But if `f` and `g` are launched on different streams, then their execution might overlap.\n",
        "\n",
        "With CuPy, much of this is hidden from us. Unless you specify otherwise, CuPy launches work on the default CUDA stream. That means that by default, all CuPy work is executed sequentially on the device, but with respect to the host, it's all happening asynchronously.\n",
        "\n",
        "CuPy supports explicitly synchronizing, creating, and manipulating streams.\n",
        "\n",
        "**TODO: However, there are also a few common operations in CuPy that will implicitly synchronize with the device. What operations do you think these are?**\n",
        "\n",
        "Let's run our code throught Nsight Systems profiler, which will help us visualize what's going on.\n",
        "\n",
        "**NOTE: The next cell won't actually run any code, it will just write its contents to a file. This is necessary because we have to run the code with the Nsight Systems profiler.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEicxhLO_9G9"
      },
      "outputs": [],
      "source": [
        "%%writefile power_iteration__baseline.py\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import cupyx as cpx\n",
        "import nvtx\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class PowerIterationConfig:\n",
        "  dim: int = 8192\n",
        "  dominance: float = 0.05\n",
        "  max_steps: int = 1000\n",
        "  check_frequency: int = 10\n",
        "  progress: bool = True\n",
        "  residual_threshold: float = 1e-10\n",
        "\n",
        "def generate_device(cfg=PowerIterationConfig()):\n",
        "  cp.random.seed(42)\n",
        "  weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "  lam = cp.random.permutation(cp.concatenate((cp.asarray([1.0]), weak_lam)))\n",
        "  P = cp.random.random((cfg.dim, cfg.dim))\n",
        "  D = cp.diag(cp.random.permutation(lam))\n",
        "  A = ((P @ D) @ cp.linalg.inv(P))\n",
        "  return A\n",
        "\n",
        "def estimate_device(A, cfg=PowerIterationConfig()):\n",
        "  A_gpu = cp.asarray(A) # If `A` is on the host, copy from host to device.\n",
        "                        # Otherwise, does nothing.\n",
        "\n",
        "  x = cp.ones(A_gpu.shape[0], dtype=np.float64)\n",
        "\n",
        "  for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "    y = A_gpu @ x\n",
        "    lam = (x @ y) / (x @ x)            # Rayleigh quotient.\n",
        "    res = cp.linalg.norm(y - lam * x)\n",
        "    x = y / cp.linalg.norm(y)          # Normalize for next step.\n",
        "\n",
        "    if cfg.progress:\n",
        "      print(f\"step {i}: residual = {res:.3e}\")\n",
        "\n",
        "    np.savetxt(f\"device_{i}.txt\", cp.asnumpy(x)) # Copy from device to host\n",
        "                                                  # and save a checkpoint.\n",
        "\n",
        "    if res < cfg.residual_threshold:\n",
        "      break\n",
        "\n",
        "    for _ in range(cfg.check_frequency - 1):\n",
        "      y = A_gpu @ x # We have to use `A_gpu` here as well.\n",
        "      x = y / cp.linalg.norm(y) # Normalize for next step.\n",
        "\n",
        "  return cp.asnumpy((x.T @ (A_gpu @ x)) / (x.T @ x)) # Copy from device to host.\n",
        "\n",
        "A_device = generate_device()\n",
        "\n",
        "# Warmup to ensure modules are loaded and code is JIT compiled before timing.\n",
        "estimate_device(A_device, cfg=PowerIterationConfig(progress=False))\n",
        "\n",
        "start = cp.cuda.get_current_stream().record()\n",
        "lam_est_device = estimate_device(A_device).item()\n",
        "stop = cp.cuda.get_current_stream().record()\n",
        "\n",
        "duration = cp.cuda.get_elapsed_time(start, stop) / 1e3\n",
        "\n",
        "print()\n",
        "print(f\"GPU Execution Time: {duration:.3f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lyhHnzrdXzI"
      },
      "source": [
        "Now let's profile our code by running it under the Nsight Systems `nsys` tool. The syntax for this is `nsys <nsys flags> <your program> <your program args>`. It will run your program while collecting a birdseye view of everything going on in your program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HU5p1IhAkTA"
      },
      "outputs": [],
      "source": [
        "!nsys profile --cuda-event-trace=false --force-overwrite true -o power_iteration__baseline python power_iteration__baseline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlGIAIEPe3SV"
      },
      "source": [
        "Now let's view our report and explore what's going on in our program.\n",
        "\n",
        "**TODO: Run the next cell, which will generate the report and create a button that when clicked will open it up in [Perfetto](https://ui.perfetto.dev/), a web-based no-install visual profiler.**\n",
        "\n",
        "**EXTRA CREDIT: Download the [Nsight Systems GUI](https://developer.nvidia.com/nsight-systems) and open the report in it to see even more information.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VVOnGQR3Ph"
      },
      "outputs": [],
      "source": [
        "import nsightful\n",
        "\n",
        "!nsys export --type sqlite --quiet true --force-overwrite true power_iteration__baseline.nsys-rep\n",
        "nsightful.display_nsys_sqlite_file_in_notebook(\"power_iteration__baseline.sqlite\", title=\"Power Iteration - Baseline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGxz6-spplcU"
      },
      "source": [
        "Nsight Systems shows us a lot of information - sometimes it's too much and not all relevant.\n",
        "\n",
        "There's two ways that we can filter and annotate what we see in Nsight systems.\n",
        "\n",
        "The first is to limit when we start and stop profiling in the program. In Python, we can do this with `cupyx.profiler.profile()`, which give us a Python context manager. Any CUDA code used during scope will be included in the profile.\n",
        "\n",
        "```\n",
        "not_in_the profile()\n",
        "with cpx.profiler.profile():\n",
        "  in_the_profile()\n",
        "not_in_the_profile()\n",
        "```\n",
        "\n",
        "For this to work, we have to pass `--capture-range=cudaProfilerApi --capture-range-end=stop` as flags to `nsys`.\n",
        "\n",
        "We can also annotate specific regions of our code, which will show up in the profiler. We can even add categories, domains, and colors to these regions, and they can be nested. To add these annotations, we use `nvtx.annnotate()`, another Python context manager, this time from a library called [NVTX](http://nvtx.readthedocs.io/en/latest/reference.html).\n",
        "\n",
        "```\n",
        "with nvtx.annotate(\"Loop\")\n",
        "  for i in range(20):\n",
        "     with nvtx.annotate(f\"Step {i}\"):\n",
        "       pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7PUALVfX3A"
      },
      "source": [
        "**TODO: Go back to the earlier cells and improve the profile results by adding:**\n",
        "- **`nvtx.annotate()` regions. Remember, you can nest them.**\n",
        "- **A `cpx.profiler.profile()` around the `start =`/`stop =` lines that run the solver.**\n",
        "- **`--capture-range=cudaProfilerApi --capture-range-end=stop` to the `nsys` flags.**\n",
        "\n",
        "**Then, capture another profile and see if you can identify how we can improve the code. Specifically, think about how we could add more asynchrony.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember what we've learned about streams and how to use them with CuPy:\n",
        "\n",
        "- By default, all CuPy operations within a single thread run on the same stream. You can access this stream with `cp.cuda.get_current_stream()`. \n",
        "- You can create a new stream with `cp.cuda.Stream(non_blocking=True)`. Use `with` statements to use the stream for all CuPy operations within a block.\n",
        "- You can record an event on a stream by calling `.record()` on it.\n",
        "- You can synchronize on an event (or an entire stream) by calling `.synchronize()` on it.\n",
        "- Memory transfers will block by default. You can launch them asynchronously with `cp.asarray(..., blocking=False)` (for host to device transfers) and `cp.asnumpy(..., blocking=False)` (for device to host transfers).\n",
        "\n",
        "**TODO: Copy the kernel from the earlier cell with your NVTX and CuPy profiler regions into the cell below. Then, try to improve performance by adding asynchrony. Make sure that you don't copy and paste the `%%writefile` directive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pszz-k8cDfqy"
      },
      "outputs": [],
      "source": [
        "%%writefile power_iteration__async.py\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import cupyx as cpx\n",
        "import nvtx\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class PowerIterationConfig:\n",
        "  dim: int = 8192\n",
        "  dominance: float = 0.05\n",
        "  max_steps: int = 1000\n",
        "  check_frequency: int = 10\n",
        "  progress: bool = True\n",
        "  residual_threshold: float = 1e-10\n",
        "\n",
        "def generate_device(cfg=PowerIterationConfig()):\n",
        "  cp.random.seed(42)\n",
        "  weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "  lam = cp.random.permutation(cp.concatenate((cp.asarray([1.0]), weak_lam)))\n",
        "  P = cp.random.random((cfg.dim, cfg.dim))\n",
        "  D = cp.diag(cp.random.permutation(lam))\n",
        "  A = ((P @ D) @ cp.linalg.inv(P))\n",
        "  return A\n",
        "\n",
        "def estimate_device(A, cfg=PowerIterationConfig()):\n",
        "  raise NotImplementedError(\"TODO: You need to implement this kernel!\")\n",
        "\n",
        "A_device = generate_device()\n",
        "\n",
        "# Warmup to ensure modules are loaded and code is JIT compiled before timing.\n",
        "estimate_device(A_device, cfg=PowerIterationConfig(progress=False))\n",
        "\n",
        "start = cp.cuda.get_current_stream().record()\n",
        "lam_est_device = estimate_device(A_device).item()\n",
        "stop = cp.cuda.get_current_stream().record()\n",
        "\n",
        "duration = cp.cuda.get_elapsed_time(start, stop) / 1e3\n",
        "\n",
        "print()\n",
        "print(f\"GPU Execution Time: {duration:.3f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFCYIqwaKYqy"
      },
      "source": [
        "Now let's make sure it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSPFNIb9KcPb"
      },
      "outputs": [],
      "source": [
        "!python power_iteration__async.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4WJVFBkkRaN"
      },
      "source": [
        "Before we profile the improved code, let's compare the execution times of both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtQR4CHikWFK"
      },
      "outputs": [],
      "source": [
        "power_iteration_baseline_output   = !python power_iteration__baseline.py\n",
        "power_iteration_baseline_duration = float(power_iteration_baseline_output[-1].split()[-2])\n",
        "power_iteration_async_output      = !python power_iteration__async.py\n",
        "power_iteration_async_duration    = float(power_iteration_async_output[-1].split()[-2])\n",
        "speedup = power_iteration_baseline_duration / power_iteration_async_duration\n",
        "\n",
        "print(f\"GPU Execution Time\")\n",
        "print()\n",
        "print(f\"power_iteration_baseline: {power_iteration_baseline_duration:.3f} s\")\n",
        "print(f\"power_iteration_async:    {power_iteration_async_duration:.3f} s\")\n",
        "print(f\"power_iteration_async speedup over power_iteration_baseline: {speedup:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnvne_F4jYTh"
      },
      "source": [
        "Next, let's capture a profile report of our improved code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWXBvi-hFGhU"
      },
      "outputs": [],
      "source": [
        "!nsys profile --cuda-event-trace=false --capture-range=cudaProfilerApi --capture-range-end=stop --force-overwrite true -o power_iteration__async python power_iteration__async.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1sI3RPojxP1"
      },
      "source": [
        "Finally, let's look at the profile in Perfetto and confirm we've gotten rid of the idling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KANrefszlkmF"
      },
      "outputs": [],
      "source": [
        "!nsys export --type sqlite --quiet true --force-overwrite true power_iteration__async.nsys-rep\n",
        "nsightful.display_nsys_sqlite_file_in_notebook(\"power_iteration__async.sqlite\", title=\"Power Iteration - Async Event\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
