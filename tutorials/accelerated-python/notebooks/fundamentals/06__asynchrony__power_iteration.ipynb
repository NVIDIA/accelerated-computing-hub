{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KP1pYqmtXdr"
      },
      "source": [
        "# Asynchrony and Power Iteration\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction and Setup](#1-Introduction-and-Setup)\n",
        "   - [1.1 Environment Setup](#11-Environment-Setup)\n",
        "2. [Theory: Streams and Synchronization](#2-Theory:-Streams-and-Synchronization)\n",
        "3. [The Baseline Implementation](#3-The-Baseline-Implementation)\n",
        "4. [Profiling the Baseline](#4-Profiling-the-Baseline)\n",
        "5. [Better Visibility with NVTX](#5-Better-Visibility-with-NVTX)\n",
        "6. [Implementing Asynchrony](#6-Implementing-Asynchrony)\n",
        "7. [Performance Analysis](#7-Performance-Analysis)\n",
        "\n",
        "## 1. Introduction and Setup\n",
        "\n",
        "GPU programming is inherently asynchronous. In this exercise, we will explore the implications of this behavior when using CuPy and learn how to analyze the flow of execution using profiling tools.\n",
        "\n",
        "We will revisit the Power Iteration algorithm. Our goal is to take a standard implementation, profile it to identify bottlenecks caused by implicit synchronization, and then optimize it using CUDA streams and asynchronous memory transfers.\n",
        "\n",
        "### 1.1 Environment Setup\n",
        "\n",
        "First, we need to ensure the Nsight Systems profiler (nsys), Nsightful, and NVTX are installed and available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO4kOPuP_0JG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Install necessary tools if running in Google Colab\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "  !curl -s -L -O https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2025_3/NsightSystems-linux-cli-public-2025.3.1.90-3582212.deb\n",
        "  !sudo dpkg -i NsightSystems-linux-cli-public-2025.3.1.90-3582212.deb > /dev/null\n",
        "  !pip install \"nvtx\" \"nsightful[notebook] @ git+https://github.com/brycelelbach/nsightful.git\" > /dev/null 2>&1\n",
        "\n",
        "print(\"Environment setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sUvjAtMxI3h"
      },
      "source": [
        "## 2. Theory: Streams and Synchronization\n",
        "\n",
        "All GPU work is launched asynchronously on a stream. The work items in a stream are executed in order. If you launch `f` on a stream and later launch `g` on that same stream, then `f` will be executed before `g`. But if `f` and `g` are launched on different streams, then their execution might overlap.\n",
        "\n",
        "**How CuPy handles this:**\n",
        "\n",
        "- **Default Stream:** Unless specified, CuPy launches work on the default CUDA stream.\n",
        "\n",
        "- **Sequential Device Execution:** By default, CuPy work executes sequentially on the GPU.\n",
        "\n",
        "- **Asynchronous Host Execution:** From the Python (Host) perspective, the code often returns immediately after launching the GPU kernel, before the work is actually finished.\n",
        "\n",
        "**TODO:** Even though CuPy is asynchronous, certain operations force the CPU to wait for the GPU to finish. What operations do you think implicitly synchronize the host and device?\n",
        "\n",
        "## 3. The Baseline Implementation\n",
        "\n",
        "We will start with a baseline implementation of the Power Iteration algorithm.\n",
        "\n",
        "**Note:** The cell below writes the code to a file named `power_iteration__baseline.py`. We do this because we must run the code through the Nsight Systems profiler via the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEicxhLO_9G9"
      },
      "outputs": [],
      "source": [
        "%%writefile power_iteration__baseline.py\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import cupyx as cpx\n",
        "import nvtx\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class PowerIterationConfig:\n",
        "  dim: int = 8192\n",
        "  dominance: float = 0.05\n",
        "  max_steps: int = 1000\n",
        "  check_frequency: int = 10\n",
        "  progress: bool = True\n",
        "  residual_threshold: float = 1e-10\n",
        "\n",
        "def generate_device(cfg=PowerIterationConfig()):\n",
        "  cp.random.seed(42)\n",
        "  weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "  lam = cp.random.permutation(cp.concatenate((cp.asarray([1.0]), weak_lam)))\n",
        "  P = cp.random.random((cfg.dim, cfg.dim))\n",
        "  D = cp.diag(cp.random.permutation(lam))\n",
        "  A = ((P @ D) @ cp.linalg.inv(P))\n",
        "  return A\n",
        "\n",
        "def estimate_device(A, cfg=PowerIterationConfig()):\n",
        "  # If `A` is on the host, copy from host to device. Otherwise, does nothing.\n",
        "  A_gpu = cp.asarray(A)\n",
        "\n",
        "  x = cp.ones(A_gpu.shape[0], dtype=np.float64)\n",
        "\n",
        "  for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "    y = A_gpu @ x\n",
        "    lam = (x @ y) / (x @ x)            # Rayleigh quotient.\n",
        "    res = cp.linalg.norm(y - lam * x)\n",
        "    x = y / cp.linalg.norm(y)          # Normalize for next step.\n",
        "\n",
        "    if cfg.progress:\n",
        "      print(f\"step {i}: residual = {res:.3e}\")\n",
        "\n",
        "    # Copy from device to host and save a checkpoint.\n",
        "    np.savetxt(f\"device_{i}.txt\", cp.asnumpy(x))\n",
        "\n",
        "    if res < cfg.residual_threshold:\n",
        "      break\n",
        "\n",
        "    for _ in range(cfg.check_frequency - 1):\n",
        "      y = A_gpu @ x # We have to use `A_gpu` here as well.\n",
        "      x = y / cp.linalg.norm(y) # Normalize for next step.\n",
        "\n",
        "  # Copy from device to host.\n",
        "  return cp.asnumpy((x.T @ (A_gpu @ x)) / (x.T @ x))\n",
        "\n",
        "A_device = generate_device()\n",
        "\n",
        "# Warmup to ensure modules are loaded and code is JIT compiled before timing.\n",
        "estimate_device(A_device, cfg=PowerIterationConfig(progress=False))\n",
        "\n",
        "start = cp.cuda.get_current_stream().record()\n",
        "lam_est_device = estimate_device(A_device).item()\n",
        "stop = cp.cuda.get_current_stream().record()\n",
        "\n",
        "duration = cp.cuda.get_elapsed_time(start, stop) / 1e3\n",
        "\n",
        "print()\n",
        "print(f\"GPU Execution Time: {duration:.3f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lyhHnzrdXzI"
      },
      "source": [
        "## 4. Profiling the Baseline\n",
        "\n",
        "Now let's profile our code by running it under the Nsight Systems `nsys` tool. The syntax for this is `nsys <nsys flags> <your program> <your program args>`. It will run your program while collecting a birdseye view of everything going on in your program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HU5p1IhAkTA"
      },
      "outputs": [],
      "source": [
        "!sudo nsys profile --cuda-event-trace=false --force-overwrite true -o power_iteration__baseline python power_iteration__baseline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlGIAIEPe3SV"
      },
      "source": [
        "Now let's view our report and explore what's going on in our program.\n",
        "\n",
        "**TODO:** Run the next cell, which will generate the report and create a button that when clicked will open it up in Perfetto, a web-based no-install visual profiler.\n",
        "\n",
        "**EXTRA CREDIT:** Download the Nsight Systems GUI and open the report in it to see even more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VVOnGQR3Ph"
      },
      "outputs": [],
      "source": [
        "import nsightful\n",
        "\n",
        "!nsys export --type sqlite --quiet true --force-overwrite true power_iteration__baseline.nsys-rep\n",
        "nsightful.display_nsys_sqlite_file_in_notebook(\"power_iteration__baseline.sqlite\", title=\"Power Iteration - Baseline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGxz6-spplcU"
      },
      "source": [
        "## 5. Better Visibility with NVTX\n",
        "\n",
        "Nsight Systems shows us a lot of information - sometimes it's too much and not all relevant.\n",
        "\n",
        "There's two ways that we can filter and annotate what we see in Nsight systems.\n",
        "\n",
        "The first is to limit when we start and stop profiling in the program. In Python, we can do this with `cupyx.profiler.profile()`, which give us a Python context manager. Any CUDA code used during scope will be included in the profile.\n",
        "\n",
        "```\n",
        "not_in_the profile()\n",
        "with cpx.profiler.profile():\n",
        "  in_the_profile()\n",
        "not_in_the_profile()\n",
        "```\n",
        "\n",
        "For this to work, we have to pass `--capture-range=cudaProfilerApi --capture-range-end=stop` as flags to `nsys`.\n",
        "\n",
        "We can also annotate specific regions of our code, which will show up in the profiler. We can even add categories, domains, and colors to these regions, and they can be nested. To add these annotations, we use `nvtx.annnotate()`, another Python context manager, this time from a library called NVTX.\n",
        "\n",
        "```\n",
        "with nvtx.annotate(\"Loop\")\n",
        "  for i in range(20):\n",
        "     with nvtx.annotate(f\"Step {i}\"):\n",
        "       pass\n",
        "```\n",
        "\n",
        "**TODO:** Go back to the earlier cells and improve the profile results by adding:\n",
        "\n",
        "- `nvtx.annotate()` regions. Remember, you can nest them.\n",
        "\n",
        "- A `cpx.profiler.profile()` around the `start =`/`stop =` lines that run the solver.\n",
        "\n",
        "- `--capture-range=cudaProfilerApi --capture-range-end=stop` to the `nsys` flags.\n",
        "\n",
        "Then, capture another profile and see if you can identify how we can improve the code. Specifically, think about how we could add more asynchrony."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7PUALVfX3A"
      },
      "source": [
        "## 6. Implementing Asynchrony\n",
        "\n",
        "Remember what we've learned about streams and how to use them with CuPy:\n",
        "\n",
        "- By default, all CuPy operations within a single thread run on the same stream. You can access this stream with `cp.cuda.get_current_stream()`.\n",
        "\n",
        "- You can create a new stream with `cp.cuda.Stream(non_blocking=True)`. Use `with` statements to use the stream for all CuPy operations within a block.\n",
        "\n",
        "- You can record an event on a stream by calling `.record()` on it.\n",
        "\n",
        "- You can synchronize on an event (or an entire stream) by calling `.synchronize()` on it.\n",
        "\n",
        "- Memory transfers will block by default. You can launch them asynchronously with `cp.asarray(..., blocking=False)` (for host to device transfers) and `cp.asnumpy(..., blocking=False)` (for device to host transfers).\n",
        "\n",
        "**TODO:** Copy the kernel from the earlier cell with your NVTX and CuPy profiler regions into the cell below. Then, try to improve performance by adding asynchrony. Make sure that you don't copy and paste the `%%writefile` directive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile power_iteration__async.py\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import cupyx as cpx\n",
        "import nvtx\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class PowerIterationConfig:\n",
        "  dim: int = 8192\n",
        "  dominance: float = 0.05\n",
        "  max_steps: int = 1000\n",
        "  check_frequency: int = 10\n",
        "  progress: bool = True\n",
        "  residual_threshold: float = 1e-10\n",
        "\n",
        "def generate_device(cfg=PowerIterationConfig()):\n",
        "  cp.random.seed(42)\n",
        "  weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "  lam = cp.random.permutation(cp.concatenate((cp.asarray([1.0]), weak_lam)))\n",
        "  P = cp.random.random((cfg.dim, cfg.dim))\n",
        "  D = cp.diag(cp.random.permutation(lam))\n",
        "  A = ((P @ D) @ cp.linalg.inv(P))\n",
        "  return A\n",
        "\n",
        "def estimate_device(A, cfg=PowerIterationConfig()):\n",
        "  raise NotImplementedError(\"TODO: You need to implement this kernel!\")\n",
        "\n",
        "A_device = generate_device()\n",
        "\n",
        "# Warmup to ensure modules are loaded and code is JIT compiled before timing.\n",
        "estimate_device(A_device, cfg=PowerIterationConfig(progress=False))\n",
        "\n",
        "start = cp.cuda.get_current_stream().record()\n",
        "lam_est_device = estimate_device(A_device).item()\n",
        "stop = cp.cuda.get_current_stream().record()\n",
        "\n",
        "duration = cp.cuda.get_elapsed_time(start, stop) / 1e3\n",
        "\n",
        "print()\n",
        "print(f\"GPU Execution Time: {duration:.3f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's make sure it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pszz-k8cDfqy"
      },
      "outputs": [],
      "source": [
        "!python power_iteration__async.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFCYIqwaKYqy"
      },
      "source": [
        "## 7. Performance Analysis\n",
        "\n",
        "Before we profile the improved code, let's compare the execution times of both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSPFNIb9KcPb"
      },
      "outputs": [],
      "source": [
        "power_iteration_baseline_output   = !python power_iteration__baseline.py\n",
        "power_iteration_baseline_duration = float(power_iteration_baseline_output[-1].split()[-2])\n",
        "power_iteration_async_output      = !python power_iteration__async.py\n",
        "power_iteration_async_duration    = float(power_iteration_async_output[-1].split()[-2])\n",
        "speedup = power_iteration_baseline_duration / power_iteration_async_duration\n",
        "\n",
        "print(f\"GPU Execution Time\")\n",
        "print()\n",
        "print(f\"power_iteration_baseline: {power_iteration_baseline_duration:.3f} s\")\n",
        "print(f\"power_iteration_async:    {power_iteration_async_duration:.3f} s\")\n",
        "print(f\"power_iteration_async speedup over power_iteration_baseline: {speedup:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4WJVFBkkRaN"
      },
      "source": [
        "Next, let's capture a profile report of our improved code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtQR4CHikWFK"
      },
      "outputs": [],
      "source": [
        "!sudo nsys profile --cuda-event-trace=false --capture-range=cudaProfilerApi --capture-range-end=stop --force-overwrite true -o power_iteration__async python power_iteration__async.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnvne_F4jYTh"
      },
      "source": [
        "Finally, let's look at the profile in Perfetto and confirm we've gotten rid of the idling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWXBvi-hFGhU"
      },
      "outputs": [],
      "source": [
        "!nsys export --type sqlite --quiet true --force-overwrite true power_iteration__async.nsys-rep\n",
        "nsightful.display_nsys_sqlite_file_in_notebook(\"power_iteration__async.sqlite\", title=\"Power Iteration - Async Event\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
