{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - Memory Spaces - Power Iteration - SOLUTION\n",
        "\n",
        "Let's learn about memory spaces and transfers! In this exercise, we'll learn:\n",
        "\n",
        "- How to explicitly transfer data between host and device.\n",
        "  - `d = cupy.asarray(h)` to copy from a host array to a device array.\n",
        "  - `h = cupy.asnumpy(d)` to copy from a device array to a host array.\n",
        "- What happens if we mix NumPy and CuPy code.\n",
        "- Some ways in which NumPy and CuPy produce different results.\n",
        "- Some of the limitations of CuPy.\n",
        "- How problem size and compute workload impacts performance.\n",
        "\n",
        "We're going to estimate the dominant eigenvalue of a matrix with the [power iteration algorithm](https://en.wikipedia.org/wiki/Power_iteration).\n",
        "First, we'll randomly generate a dense diagonalizable square matrix."
      ],
      "metadata": {
        "id": "6YkNAlM91iGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp # Don't forget to add the CuPy include!"
      ],
      "metadata": {
        "id": "ZHpg3aVXSeix"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class PowerIterationConfig:\n",
        "  dim: int = 4096 # Number of rows and columns in the square matrix.\n",
        "\n",
        "  # Value from 0 to 1 that controls how much greater the dominant eigenvalue is\n",
        "  # from the rest of the eigenvalues. A higher value means quicker convergence.\n",
        "  dominance: float = 0.1\n",
        "\n",
        "  # Maximum number of steps to perform.\n",
        "  max_steps: int = 400\n",
        "\n",
        "  # Every `check_frequency` steps we save a checkpoint and compute the residual.\n",
        "  check_frequency: int = 10\n",
        "\n",
        "  # Whether the residual should be printed every `check_frequency` steps.\n",
        "  progress: bool = True\n",
        "\n",
        "  # If the residual is below `residual_threshold`, terminate early.\n",
        "  residual_threshold: float = 1e-10"
      ],
      "metadata": {
        "id": "BCToOdVdSONp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5ak3mn2hIsKo",
        "outputId": "bcb16176-5c36-45fc-c420-f4761481b8ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.4937 -0.519  -0.2935 ... -0.1628  0.8361  0.531 ]\n",
            " [-1.0859  0.0087 -0.0661 ... -0.1706  1.0955  0.7075]\n",
            " [-0.4291 -0.3628  0.3393 ...  0.1813  0.2238 -0.2124]\n",
            " ...\n",
            " [-1.1089 -0.4564 -0.3024 ...  0.2075  1.2864  0.9066]\n",
            " [-0.8714 -0.5109 -0.1201 ... -0.072   1.3048  0.4372]\n",
            " [-1.6421 -0.6629 -0.2001 ... -0.2997  1.8579  1.5576]]\n"
          ]
        }
      ],
      "source": [
        "def generate_host(cfg=PowerIterationConfig()):\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # Vector with a single 1 & `cfg.dim - 1` values from 0 to `1 - cfg.dominance`.\n",
        "  weak_lam = np.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "  lam = np.random.permutation(np.concatenate(([1.0], weak_lam)))\n",
        "\n",
        "  P = np.random.random((cfg.dim, cfg.dim)) # Random invertible matrix.\n",
        "  D = np.diag(np.random.permutation(lam))  # Diagonal matrix w/ random eigenvalues.\n",
        "  A = ((P @ D) @ np.linalg.inv(P))         # Diagonalizable matrix.\n",
        "  return A\n",
        "\n",
        "A_host = generate_host()\n",
        "\n",
        "with np.printoptions(precision=4):\n",
        "  print(A_host)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we perform the power iteration with NumPy, using a vector of 1s as our initial guess.\n",
        "\n",
        "We'll perform at most `cfg.max_steps`. Every `cfg.check_frequency` steps, we'll output a checkpoint, compute the absolute residual, check whether it's below a `cfg.residual_threshold`. If it is, then we'll stop early."
      ],
      "metadata": {
        "id": "VHv9uXHI_6e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_host(A, cfg=PowerIterationConfig()):\n",
        "  x = np.ones(A.shape[0], dtype=np.float64)\n",
        "\n",
        "  for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "    y = A @ x\n",
        "    lam = (x @ y) / (x @ x)            # Rayleigh quotient.\n",
        "    res = np.linalg.norm(y - lam * x)\n",
        "    x = y / np.linalg.norm(y)          # Normalize for next step.\n",
        "\n",
        "    if cfg.progress:\n",
        "      print(f\"step {i}: residual = {res:.3e}\")\n",
        "\n",
        "    np.savetxt(f\"host_{i}.txt\", x) # Save a checkpoint.\n",
        "\n",
        "    if res < cfg.residual_threshold:\n",
        "      break\n",
        "\n",
        "    for _ in range(cfg.check_frequency - 1):\n",
        "      y = A @ x\n",
        "      x = y / np.linalg.norm(y) # Normalize for next step.\n",
        "\n",
        "  return (x.T @ (A @ x)) / (x.T @ x)\n",
        "\n",
        "lam_est_host = estimate_host(A_host).item()\n",
        "\n",
        "print()\n",
        "print(lam_est_host)"
      ],
      "metadata": {
        "id": "q0x0_p4pvAdD",
        "outputId": "c7a727fa-8c84-452c-e0b0-4ffabe8688f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: residual = 7.594e+00\n",
            "step 10: residual = 1.699e-02\n",
            "step 20: residual = 2.148e-02\n",
            "step 30: residual = 1.295e-02\n",
            "step 40: residual = 4.494e-03\n",
            "step 50: residual = 1.366e-03\n",
            "step 60: residual = 4.129e-04\n",
            "step 70: residual = 1.274e-04\n",
            "step 80: residual = 4.013e-05\n",
            "step 90: residual = 1.286e-05\n",
            "step 100: residual = 4.181e-06\n",
            "step 110: residual = 1.374e-06\n",
            "step 120: residual = 4.550e-07\n",
            "step 130: residual = 1.517e-07\n",
            "step 140: residual = 5.085e-08\n",
            "step 150: residual = 1.712e-08\n",
            "step 160: residual = 5.784e-09\n",
            "step 170: residual = 1.960e-09\n",
            "step 180: residual = 6.663e-10\n",
            "step 190: residual = 2.269e-10\n",
            "step 200: residual = 7.741e-11\n",
            "\n",
            "0.9999999999615732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's our power iteration function ported to CuPy.\n",
        "\n",
        "Did you forget to port any calls and get an error message? Sometimes they can be tricky to decipher. AI coding assistants are a great tool for identifying the underlying issue. On Google Colab, when a cell fails to compile, there will be a helpful AI-powered \"explain error\" button."
      ],
      "metadata": {
        "id": "LMtzZzVNBexb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_device(A, cfg=PowerIterationConfig):\n",
        "  A_gpu = cp.asarray(A) # If `A` is on the host, copy from host to device.\n",
        "                        # Otherwise, does nothing.\n",
        "\n",
        "  x = cp.ones(A_gpu.shape[0], dtype=np.float64)\n",
        "\n",
        "  for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
        "    y = A_gpu @ x # If we used `A` here instead of `A_gpu`, we would get an error.\n",
        "                  # CuPy doesn't allow implicit conversions between NumPy <-> CuPy.\n",
        "\n",
        "    lam = (x @ y) / (x @ x)            # Rayleigh quotient.\n",
        "    res = cp.linalg.norm(y - lam * x)\n",
        "    x = y / cp.linalg.norm(y)          # Normalize for next step.\n",
        "\n",
        "    if cfg.progress:\n",
        "      print(f\"step {i}: residual = {cp.asnumpy(res):.3e}\")\n",
        "\n",
        "    np.savetxt(f\"device_{i}.txt\", cp.asnumpy(x)) # Copy from device to host\n",
        "                                                 # and save a checkpoint.\n",
        "\n",
        "    if cp.asnump(res) < cfg.residual_threshold:\n",
        "      break\n",
        "\n",
        "    for _ in range(cfg.check_frequency - 1):\n",
        "      y = A_gpu @ x # We have to use `A_gpu` here as well.\n",
        "      x = y / cp.linalg.norm(y) # Normalize for next step.\n",
        "\n",
        "  return cp.asnumpy((x.T @ (A_gpu @ x)) / (x.T @ x)) # Copy from device to host.\n",
        "\n",
        "lam_est_device = estimate_device(A_host).item()\n",
        "\n",
        "print()\n",
        "print(lam_est_device)"
      ],
      "metadata": {
        "id": "sulx6gabBd1w",
        "outputId": "6053b4d8-8584-4b56-991f-c45ea531622c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: residual = 7.594e+00\n",
            "step 10: residual = 1.699e-02\n",
            "step 20: residual = 2.148e-02\n",
            "step 30: residual = 1.295e-02\n",
            "step 40: residual = 4.494e-03\n",
            "step 50: residual = 1.366e-03\n",
            "step 60: residual = 4.129e-04\n",
            "step 70: residual = 1.274e-04\n",
            "step 80: residual = 4.013e-05\n",
            "step 90: residual = 1.286e-05\n",
            "step 100: residual = 4.181e-06\n",
            "step 110: residual = 1.374e-06\n",
            "step 120: residual = 4.550e-07\n",
            "step 130: residual = 1.517e-07\n",
            "step 140: residual = 5.085e-08\n",
            "step 150: residual = 1.712e-08\n",
            "step 160: residual = 5.785e-09\n",
            "step 170: residual = 1.960e-09\n",
            "step 180: residual = 6.660e-10\n",
            "step 190: residual = 2.267e-10\n",
            "step 200: residual = 7.735e-11\n",
            "\n",
            "0.9999999999610745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also port the matrix generation function to CuPy. However, you may have noticed that the residuals and number of steps you get are different. That's because we're actually generating a different matrix than NumPy!\n",
        "\n",
        "CuPy uses a different pseudo random number generator algorithm than NumPy, because it needs one that can be parallelized. So even with the same code and same seed, we'll get different results between NumPy and CuPy.\n",
        "\n",
        "There are a number of operations in CuPy that have the same functionality as their NumPy counterparts but are not guaranteed to be bit-for-bit equivalent.\n",
        "\n",
        "For the purpose of our benchmarking, we'll use the NumPy generated matrix."
      ],
      "metadata": {
        "id": "yEOYYIc41PPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_device(cfg=PowerIterationConfig):\n",
        "  cp.random.seed(42)\n",
        "\n",
        "  # Vector with a single 1 & `cfg.dim - 1` values from 0 to `1 - cfg.dominance`.\n",
        "  weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
        "  lam = cp.random.permutation(cp.concatenate((cp.asarray([1.0]), weak_lam)))\n",
        "  # In NumPy, `concatenate` takes anything array-like, so a list is fine, but\n",
        "  # in CuPy, all args must be device arrays, as there are no implicit transfers!\n",
        "\n",
        "  P = cp.random.random((cfg.dim, cfg.dim)) # Random invertible matrix.\n",
        "  D = cp.diag(cp.random.permutation(lam))  # Diagonal matrix with random eigenvalues.\n",
        "  A = ((P @ D) @ cp.linalg.inv(P))         # Diagonalizable matrix.\n",
        "  return A\n",
        "\n",
        "A_device = generate_device()\n",
        "\n",
        "with np.printoptions(precision=4):\n",
        "  print(\"A_host:\")\n",
        "  print(A_host)\n",
        "  print()\n",
        "  print(\"A_device:\")\n",
        "  print(A_device)\n",
        "  print()\n",
        "\n",
        "lam_est_device_generation = estimate_device(A_device).item()\n",
        "\n",
        "print()\n",
        "print(lam_est_device_generation)"
      ],
      "metadata": {
        "id": "HXFZXGhU1Xj2",
        "outputId": "d40a6a9f-b9fb-4107-e5bb-82bc8553d5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Unsupported type <class 'list'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2575190627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mA_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2575190627.py\u001b[0m in \u001b[0;36mgenerate_device\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Vector with a single 1 & `cfg.dim - 1` values from 0 to `1 - cfg.dominance`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mweak_lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdominance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_lam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m# In NumPy, `concatenate` takes anything array-like, so a list is fine, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# in CuPy, all args must be device arrays, as there are no implicit transfers!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cupy/_manipulation/join.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(tup, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_routines_manipulation.pyx\u001b[0m in \u001b[0;36mcupy._core._routines_manipulation.concatenate_method\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_routines_manipulation.pyx\u001b[0m in \u001b[0;36mcupy._core._routines_manipulation.concatenate_method\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._preprocess_args\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._preprocess_arg\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'list'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's compute the eigenvalues of the matrix with `numpy.linalg.eigvals`. This may take a little while.\n",
        "\n",
        "Did you try changing this to use CuPy?\n",
        "Unfortunately we can't, because CuPy doesn't have `eigvals` yet. It's important to remember that while CuPy supports much of NumPy, it has limitations!"
      ],
      "metadata": {
        "id": "BXXawDcfHSol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lam_ref = np.linalg.eigvals(A_host).real.max()"
      ],
      "metadata": {
        "id": "fL7QYIVesehd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check whether our power iteration estimation is correct."
      ],
      "metadata": {
        "id": "BF1HHlkOHtw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solution\")\n",
        "print()\n",
        "print(f\"Power iteration (host)   = {lam_est_host:.6e}\")\n",
        "print(f\"Power iteration (device) = {lam_est_device:.6e}\")\n",
        "print(f\"`eigvals` reference      = {lam_ref:.6e}\")\n",
        "\n",
        "rel_err_host   = abs(lam_est_host - lam_ref) / abs(lam_ref)\n",
        "rel_err_device = abs(lam_est_device - lam_ref) / abs(lam_ref)\n",
        "print()\n",
        "print(f\"Relative error (host)    = {rel_err_host:.3e}\")\n",
        "print(f\"Relative error (device)  = {rel_err_device:.3e}\")\n",
        "\n",
        "np.testing.assert_allclose(lam_est_host, lam_ref, rtol=1e-4)\n",
        "np.testing.assert_allclose(lam_est_device, lam_ref, rtol=1e-4)"
      ],
      "metadata": {
        "id": "7E7MUYsYsjwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's benchmark all three solutions."
      ],
      "metadata": {
        "id": "2UXGSFs2H70q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Execution Time\")\n",
        "print()\n",
        "\n",
        "time_host = %timeit -q -o estimate_host(A_host, PowerIterationConfig(progress=False)).item()\n",
        "print(f\"Power iteration (host)   = {time_host}\")\n",
        "\n",
        "# We intentionally use `A_host`, not `A_device`, because they're not\n",
        "# the same matrices due to differences in NumPy and CuPy's random facilities.\n",
        "time_device = %timeit -q -o estimate_device(A_host, PowerIterationConfig(progress=False)).item()\n",
        "print(f\"Power iteration (device) = {time_device}\")\n",
        "\n",
        "time_ref = %timeit -q -o -r 1 -n 1 np.linalg.eigvals(A_host).real.max()\n",
        "print(f\"`eigvals` reference      = {time_ref}\")"
      ],
      "metadata": {
        "id": "v_2HmcBFERhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}