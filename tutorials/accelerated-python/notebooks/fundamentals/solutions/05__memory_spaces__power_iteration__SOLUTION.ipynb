{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Spaces & Power Iteration\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Memory Spaces](#1-introduction-to-memory-spaces)\n",
    "2. [The CPU Baseline (NumPy)](#2-the-cpu-baseline-numpy)\n",
    "3. [The GPU Port (CuPy)](#3-the-gpu-port-cupy)\n",
    "4. [Optimizing Data Generation](#4-optimizing-data-generation)\n",
    "5. [Verification and Benchmarking](#5-verification-and-benchmarking)\n",
    "6. [Extra Credit](#extra-credit)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Memory Spaces\n",
    "\n",
    "Before we implement algorithms on the GPU, we must understand the hardware architecture. A heterogeneous system (like the one you are using) consists of two distinct memory spaces:\n",
    "\n",
    "1.  **Host Memory (CPU):** System RAM. Accessible by the CPU.\n",
    "2.  **Device Memory (GPU):** High-bandwidth memory (HBM) attached to the GPU. Accessible by the GPU.\n",
    "\n",
    "The CPU cannot directly calculate data stored on the GPU, and the GPU cannot directly calculate data stored in System RAM. To perform work on the GPU, you must explicitly manage data movement.\n",
    "\n",
    "* **Host $\\to$ Device:** Move data to the GPU to compute.\n",
    "    * Syntax: `x_device = cp.asarray(x_host)`\n",
    "* **Device $\\to$ Host:** Move results back to the CPU to save to disk, plot with Matplotlib, or print.\n",
    "    * Syntax: `y_host = cp.asnumpy(y_device)`\n",
    "\n",
    "### Implicit Transfers and Synchronization\n",
    "\n",
    "It is crucial to understand when CuPy interacts with the CPU implicitly. These interactions can kill performance because they force the GPU to pause (synchronize) while data moves.\n",
    "\n",
    "CuPy silently transfers and synchronizes when you:\n",
    "1.  **Print** a GPU array (`print(gpu_array)`).\n",
    "2.  **Convert** to a Python scalar (`float(gpu_array)` or `.item()`).\n",
    "3.  **Evaluate** a GPU scalar in a boolean context (`if gpu_scalar > 0:`).\n",
    "\n",
    "### The Task\n",
    "To understand the implications of these concepts, let's experiment with estimating the dominant eigenvalue of a matrix using the **Power Iteration** algorithm.\n",
    "\n",
    "Before we dive into the code, let's understand the math behind the algorithm we are implementing.\n",
    "\n",
    "**Power Iteration** is a classic iterative method used to find the dominant eigenvalue (the eigenvalue with the largest absolute value) and its corresponding eigenvector of a square matrix $A$.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "The core idea is simple: if you repeatedly multiply a vector by a matrix $A$, the vector will eventually converge towards the dominant eigenvector of $A$, regardless of the initial vector you started with (provided the initial vector has some component in the direction of the dominant eigenvector).\n",
    "\n",
    "#### The Mathematical Steps\n",
    "\n",
    "Given a square matrix $A$ and a random initial vector $x_0$, the algorithm proceeds as follows for each step $k$:\n",
    "\n",
    "**1. Matrix-Vector Multiplication:**\n",
    "\n",
    "We calculate the next approximation of the vector:\n",
    "\n",
    "$$y = A x_k$$\n",
    "\n",
    "**2. Eigenvalue Estimation (Rayleigh Quotient):**\n",
    "\n",
    "We estimate the eigenvalue $\\lambda$ using the current vector. This is essentially projecting $y$ onto $x$:\n",
    "\n",
    "$$\\lambda_k = \\frac{x_k^T y}{x_k^T x_k} = \\frac{x_k^T A x_k}{x_k^T x_k}$$\n",
    "\n",
    "**3. Residual Calculation (Error Check):**\n",
    "\n",
    "We check how close we are to the true definition of an eigenvector ($Ax = \\lambda x$) by calculating the \"residual\" (error):\n",
    "\n",
    "$$r = ||y - \\lambda_k x_k||$$\n",
    "\n",
    "If $r$ is close to 0, we have converged.\n",
    "\n",
    "**4. Normalization:**\n",
    "\n",
    "To prevent the numbers from exploding (overflow) or vanishing (underflow), we normalize the vector for the next iteration:\n",
    "\n",
    "$$x_{k+1} = \\frac{y}{||y||}$$\n",
    "\n",
    "We will start with a standard CPU implementation, port it to the GPU using CuPy, and analyze the performance impact of memory transfers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configuration for the algorithm\n",
    "@dataclass\n",
    "class PowerIterationConfig:\n",
    "    dim: int = 4096                    # Matrix size (dim x dim)\n",
    "    dominance: float = 0.1             # How much larger the top eigenvalue is (controls convergence speed)\n",
    "    max_steps: int = 400               # Maximum iterations\n",
    "    check_frequency: int = 10          # Check for convergence every N steps\n",
    "    progress: bool = True              # Print progress logs\n",
    "    residual_threshold: float = 1e-10  # Stop if error is below this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The CPU Baseline (NumPy)\n",
    "\n",
    "We generate a random dense matrix that is diagonalizable. This data is generated on the **Host (CPU)** and resides in **Host Memory**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_host(cfg=PowerIterationConfig()):\n",
    "    \"\"\"Generates a random diagonalizable matrix on the CPU.\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create eigenvalues: One large one (1.0), the rest smaller\n",
    "    weak_lam = np.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
    "    lam = np.random.permutation(np.concatenate(([1.0], weak_lam)))\n",
    "\n",
    "    # Construct matrix A = P * D * P^-1\n",
    "    P = np.random.random((cfg.dim, cfg.dim))  # Random invertible matrix\n",
    "    D = np.diag(np.random.permutation(lam))   # Diagonal matrix of eigenvalues\n",
    "    A = ((P @ D) @ np.linalg.inv(P))          # The final matrix\n",
    "    return A\n",
    "\n",
    "# Generate the data on Host\n",
    "print(\"Generating Host Data...\")\n",
    "A_host = generate_host()\n",
    "print(f\"Host Matrix Shape: {A_host.shape}\")\n",
    "print(f\"Data Type: {A_host.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Power Iteration (CPU)\n",
    "\n",
    "As described above, the Power Iteration algorithm repeatedly multiplies a vector $x$ by matrix $A$ ($y = Ax$) and normalizes the result. We initialize this algorithm with a vector of 1s ($x_0$) as our initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_host(A, cfg=PowerIterationConfig()):\n",
    "    \"\"\"\n",
    "    Performs power iteration using purely NumPy (CPU).\n",
    "    \"\"\"\n",
    "    # Initialize vector of ones on Host\n",
    "    x = np.ones(A.shape[0], dtype=np.float64)\n",
    "\n",
    "    for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
    "        # Matrix-Vector multiplication\n",
    "        y = A @ x\n",
    "        \n",
    "        # Rayleigh quotient: (x . y) / (x . x)\n",
    "        lam = (x @ y) / (x @ x)\n",
    "        \n",
    "        # Calculate residual (error)\n",
    "        res = np.linalg.norm(y - lam * x)\n",
    "        \n",
    "        # Normalize vector for next step\n",
    "        x = y / np.linalg.norm(y)\n",
    "\n",
    "        if cfg.progress:\n",
    "            print(f\"Step {i}: residual = {res:.3e}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if res < cfg.residual_threshold:\n",
    "            break\n",
    "\n",
    "        # Run intermediate steps without checking residual to save compute\n",
    "        for _ in range(cfg.check_frequency - 1):\n",
    "            y = A @ x\n",
    "            x = y / np.linalg.norm(y)\n",
    "\n",
    "    return (x.T @ (A @ x)) / (x.T @ x)\n",
    "\n",
    "# Run CPU Baseline\n",
    "print(\"\\nRunning CPU Estimate...\")\n",
    "start_time = time.time()\n",
    "lam_est_host = estimate_host(A_host)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nEstimated Eigenvalue (CPU): {lam_est_host}\")\n",
    "print(f\"Time taken: {end_time - start_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The GPU Port (CuPy)\n",
    "\n",
    "### Exercise: Port the CPU Implementation to GPU\n",
    "\n",
    "Now it's your turn! Your task is to convert the `estimate_host` function to run on the GPU using CuPy.\n",
    "\n",
    "**Remember the rules of Memory Spaces:**\n",
    "1.  **Transfer:** Move `A_host` from CPU to GPU using `cp.asarray()`.\n",
    "2.  **Compute:** Perform math using `cp` functions on the GPU.\n",
    "3.  **Retrieve:** Move result back to CPU using `cp.asnumpy()` or `.item()` if we need to print it or use it in standard Python.\n",
    "\n",
    "**Hint:** CuPy tries to replicate the NumPy API. In many cases, you can simply change `np.` to `cp.`. However, CuPy operations *must* run on data present in Device Memory.\n",
    "\n",
    "**Fill in the `TODO` sections in the skeleton code below:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_device_exercise(A, cfg=PowerIterationConfig()):\n",
    "    \"\"\"\n",
    "    Port the power iteration algorithm to the GPU using CuPy.\n",
    "    \n",
    "    Steps to complete:\n",
    "    1. Transfer the input matrix A to the GPU (if it's a numpy array)\n",
    "    2. Initialize the vector x on the GPU\n",
    "    3. Replace np operations with cp operations\n",
    "    4. Return the result as a Python scalar\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: MEMORY TRANSFER (Host -> Device)\n",
    "    # Check if A is a numpy array. If so, move it to GPU using cp.asarray()\n",
    "    # Otherwise, assume it's already on the device.\n",
    "    # ---------------------------------------------------------\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A_gpu = cp.asarray(A)  # SOLUTION: Transfer to GPU\n",
    "    else:\n",
    "        A_gpu = A\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: Initialize vector of ones ON THE GPU\n",
    "    # ---------------------------------------------------------\n",
    "    x = cp.ones(A_gpu.shape[0], dtype=cp.float64)  # SOLUTION: Create vector of ones on GPU\n",
    "    \n",
    "    for i in range(0, cfg.max_steps, cfg.check_frequency):\n",
    "        # ---------------------------------------------------------\n",
    "        # SOLUTION: Perform GPU computations using CuPy\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        # Matrix-Vector multiplication (this works the same with CuPy!)\n",
    "        y = A_gpu @ x\n",
    "        \n",
    "        # Rayleigh quotient\n",
    "        lam = (x @ y) / (x @ x)\n",
    "        \n",
    "        # SOLUTION: Calculate residual using cp.linalg.norm\n",
    "        res = cp.linalg.norm(y - lam * x)\n",
    "        \n",
    "        # SOLUTION: Normalize x using cp.linalg.norm\n",
    "        x = y / cp.linalg.norm(y)\n",
    "        \n",
    "        if cfg.progress:\n",
    "            print(f\"Step {i}: residual = {res:.3e}\")\n",
    "        \n",
    "        if res < cfg.residual_threshold:\n",
    "            break\n",
    "        \n",
    "        for _ in range(cfg.check_frequency - 1):\n",
    "            y = A_gpu @ x\n",
    "            x = y / cp.linalg.norm(y)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: MEMORY TRANSFER (Device -> Host)\n",
    "    # Return the eigenvalue as a Python scalar using .item()\n",
    "    # ---------------------------------------------------------\n",
    "    result = (x.T @ (A_gpu @ x)) / (x.T @ x)\n",
    "    return result.item()  # SOLUTION: Convert GPU scalar to Python scalar\n",
    "\n",
    "# Run the GPU implementation\n",
    "print(\"\\nRunning GPU Estimate (Input is Host Array)...\")\n",
    "start_time = time.time()\n",
    "lam_est_device = estimate_device_exercise(A_host)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nEstimated Eigenvalue (GPU): {lam_est_device}\")\n",
    "print(f\"Time taken: {end_time - start_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizing Data Generation\n",
    "\n",
    "In the previous step, we generated data on the CPU and copied it to the GPU. For large datasets, the transfer time (`Host -> Device`) can be a bottleneck. \n",
    "\n",
    "It is almost always faster to **generate** the data directly on the GPU if possible.\n",
    "\n",
    "### Exercise: Generate Data Directly on the GPU\n",
    "\n",
    "Your task is to convert the `generate_host` function to generate the matrix directly on the GPU using CuPy's random functions.\n",
    "\n",
    "**Hints:**\n",
    "- Use `cp.random.seed()` instead of `np.random.seed()`\n",
    "- Use `cp.random.random()` instead of `np.random.random()`\n",
    "- Use `cp.random.permutation()` instead of `np.random.permutation()`\n",
    "- Use `cp.concatenate()`, `cp.array()`, `cp.diag()`, and `cp.linalg.inv()`\n",
    "\n",
    "**Fill in the `TODO` sections in the skeleton code below:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_device_exercise(cfg=PowerIterationConfig()):\n",
    "    \"\"\"\n",
    "    Generate a random diagonalizable matrix directly on the GPU.\n",
    "    \n",
    "    This should mirror the generate_host function but use CuPy instead of NumPy.\n",
    "    The key benefit: no Host->Device transfer needed!\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: Set the random seed on the GPU\n",
    "    # ---------------------------------------------------------\n",
    "    cp.random.seed(42)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: Create eigenvalues on the GPU\n",
    "    # Generate (dim-1) random values, scale them, then combine with 1.0\n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: Generate weak eigenvalues using cp.random.random()\n",
    "    weak_lam = cp.random.random(cfg.dim - 1) * (1.0 - cfg.dominance)\n",
    "    \n",
    "    # SOLUTION: Concatenate [1.0] with weak_lam using cp.concatenate and cp.array\n",
    "    # Then permute them using cp.random.permutation\n",
    "    lam = cp.random.permutation(cp.concatenate((cp.array([1.0]), weak_lam)))\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: Construct the matrix A = P * D * P^-1 on the GPU\n",
    "    # ---------------------------------------------------------\n",
    "    # SOLUTION: Generate random matrix P using cp.random.random()\n",
    "    P = cp.random.random((cfg.dim, cfg.dim))\n",
    "    \n",
    "    # SOLUTION: Create diagonal matrix D using cp.diag()\n",
    "    D = cp.diag(cp.random.permutation(lam))\n",
    "    \n",
    "    # SOLUTION: Compute A = P @ D @ P^-1 using cp.linalg.inv()\n",
    "    A = ((P @ D) @ cp.linalg.inv(P))\n",
    "    \n",
    "    return A\n",
    "\n",
    "print(\"\\nGenerating Data directly on GPU...\")\n",
    "start_time = time.time()\n",
    "A_device = generate_device_exercise()\n",
    "end_time = time.time()\n",
    "print(f\"Generation time: {end_time - start_time:.4f}s\")\n",
    "\n",
    "print(\"Running GPU Estimate (Input is Device Array)...\")\n",
    "start_time = time.time()\n",
    "# No transfer overhead here because A_device is already on GPU\n",
    "lam_est_device_gen = estimate_device_exercise(A_device)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "end_time = time.time()\n",
    "print(f\"Compute time: {end_time - start_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think About It\n",
    "\n",
    "Both functions use `seed(42)`. Are `A_host` and `A_device` identical? Try comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NumPy:\", A_host[0, :3])\n",
    "print(\"CuPy:\", A_device[0, :3].get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**\n",
    "\n",
    "This reveals that `np.random` and `cp.random` use **different random number generator (RNG) implementations**, even with the same seed.\n",
    "\n",
    "- NumPy uses the Mersenne Twister algorithm (or PCG64 in newer versions) on the CPU.\n",
    "- CuPy uses a GPU-optimized RNG (typically XORWOW from cuRAND) that runs efficiently in parallel on thousands of GPU threads.\n",
    "\n",
    "Even with the same seed value (`42`), these different algorithms produce completely different sequences of \"random\" numbers. This is why `A_host` and `A_device` contain different values.\n",
    "\n",
    "**Key Takeaway:** If you need *identical* data on both CPU and GPU for verification purposes, you should:\n",
    "1. Generate the data on one device (e.g., CPU with NumPy)\n",
    "2. Transfer it to the other device (e.g., `cp.asarray(A_host)`)\n",
    "\n",
    "This guarantees bit-for-bit identical data, which is essential for debugging and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification and Benchmarking\n",
    "\n",
    "Finally, let's verify our accuracy against a reference implementation (`numpy.linalg.eigvals`) and benchmark the speedup.\n",
    "\n",
    "**Note on CuPy Limitations:** You might wonder why we use `np.linalg.eigvals` on the CPU instead of a CuPy equivalent. The reason is that CuPy does not yet implement `eigvals`. While CuPy covers a large portion of the NumPy API, it does not support every function. Always check the [CuPy documentation](https://docs.cupy.dev/en/stable/reference/comparison.html) to verify which functions are available before assuming a direct NumPy-to-CuPy conversion will work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Reference Eigenvalue (numpy.linalg)...\")\n",
    "# Note: calculating all eigenvalues is computationally expensive\n",
    "lam_ref = np.linalg.eigvals(A_host).real.max()\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"Reference: {lam_ref}\")\n",
    "print(f\"CPU Est:   {lam_est_host}\")\n",
    "print(f\"GPU Est:   {lam_est_device_gen}\")\n",
    "\n",
    "# Assert correctness\n",
    "np.testing.assert_allclose(lam_est_host, lam_ref, rtol=1e-4)\n",
    "np.testing.assert_allclose(lam_est_device_gen, lam_ref, rtol=1e-4)\n",
    "print(\"\\nAccuracy verification passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Verification\n",
    "\n",
    "As mentioned before, `A_host` and `A_device` are **different matrices** (NumPy and CuPy use different RNG implementations). Yet the verification passes. Why?\n",
    "\n",
    "Both matrices are *constructed* with one eigenvalue explicitly set to **1.0**. The verification confirms that power iteration correctly finds this dominant eigenvalueâ€”not that the matrices are identical.\n",
    "\n",
    "**Key takeaway:** If you need to verify GPU computation against CPU on the *exact same data*, generate on one device and transfer to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking with `cupyx.profiler.benchmark`\n",
    "\n",
    "We use CuPy's built-in benchmarking utility for accurate GPU timing. This handles warmup and synchronization automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx.profiler import benchmark\n",
    "\n",
    "cfg = PowerIterationConfig(progress=False)\n",
    "\n",
    "# 1. CPU\n",
    "print(\"Timing CPU...\")\n",
    "result_cpu = benchmark(estimate_host, args=(A_host, cfg), n_repeat=10)\n",
    "t_cpu_ms = result_cpu.cpu_times.mean() * 1000\n",
    "\n",
    "# 2. GPU (with transfer overhead)\n",
    "print(\"Timing GPU (Host Input)...\")\n",
    "result_transfer = benchmark(estimate_device_exercise, args=(A_host, cfg), n_repeat=10)\n",
    "t_gpu_transfer_ms = result_transfer.gpu_times.mean() * 1000\n",
    "\n",
    "# 3. GPU (pure device)\n",
    "print(\"Timing GPU (Device Input)...\")\n",
    "result_pure = benchmark(estimate_device_exercise, args=(A_device, cfg), n_repeat=10)\n",
    "t_gpu_pure_ms = result_pure.gpu_times.mean() * 1000\n",
    "\n",
    "print(f\"\\n--- Average Compute Times ---\")\n",
    "print(f\"CPU:                 {t_cpu_ms:.2f} ms\")\n",
    "print(f\"GPU (with transfer): {t_gpu_transfer_ms:.2f} ms\")\n",
    "print(f\"GPU (pure):          {t_gpu_pure_ms:.2f} ms\")\n",
    "\n",
    "speedup = t_cpu_ms / t_gpu_pure_ms\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extra Credit\n",
    "\n",
    "**Explore the impact of changing the following parameters:**\n",
    "\n",
    "1. **Problem Size (`dim`):** How does the GPU speedup change as you increase or decrease the matrix dimensions? Try values like 1024, 2048, 4096, 8192.\n",
    "\n",
    "2. **Compute Workload (`max_steps` and `dominance`):** The `dominance` parameter controls how quickly the algorithm converges. A smaller dominance means eigenvalues are closer together, requiring more iterations. How does this affect the CPU vs GPU comparison?\n",
    "\n",
    "3. **Check Frequency (`check_frequency`):** This controls how often we check for convergence (and trigger implicit CPU synchronization via the print statement). What happens to GPU performance when you check every step (`check_frequency=1`) vs. less frequently (`check_frequency=50`)?\n",
    "\n",
    "**Experiment below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different configurations here!\n",
    "# Example:\n",
    "# cfg_large = PowerIterationConfig(dim=8192, progress=False)\n",
    "# cfg_slow_converge = PowerIterationConfig(dominance=0.01, progress=False)\n",
    "# cfg_frequent_check = PowerIterationConfig(check_frequency=1, progress=True)\n",
    "\n",
    "# Your experiments:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
