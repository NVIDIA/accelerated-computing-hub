{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accelerated Computing with CuPy\n",
        "\n",
        "## Table of Contents\n",
        "1. [Creating Arrays: CPU vs. GPU](#1.-Creating-Arrays:-CPU-vs.-GPU)\n",
        "2. [Basic Operations](#2.-Basic-Operations)\n",
        "   - [Sequential Operations & Memory](#Sequential-Operations-&-Memory)\n",
        "3. [Complex Operations (Linear Algebra)](#3.-Complex-Operations-(Linear-Algebra))\n",
        "   - [Agnostic Code (NumPy Dispatch)](#Agnostic-Code-(NumPy-Dispatch))\n",
        "4. [Device Management](#4.-Device-Management)\n",
        "5. [Exercise - NumPy to CuPy](#Exercise---NumPy-to-CuPy)\n",
        "   - [Part 1](#Part-1)\n",
        "   - [Part 2](#Part-2)\n",
        "\n",
        "---\n",
        "\n",
        "Let's shift gears to high-level array functionality using **[CuPy](https://cupy.dev/)**.\n",
        "\n",
        "### What is CuPy?\n",
        "CuPy is a library that implements the familiar **NumPy API** but runs on the GPU (using CUDA C++ in the backend). \n",
        "\n",
        "**Why use it?**\n",
        "* **Zero Friction:** If you know NumPy, you already know CuPy.\n",
        "* **Speed:** It provides out-of-the-box GPU acceleration for array operations.\n",
        "* **Ease of use:** You can often port CPU code to GPU simply by changing `import numpy as np` to `import cupy as cp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from cupyx.profiler import benchmark\n",
        "\n",
        "# Helper to display benchmark results concisely.\n",
        "# We use CuPy's benchmark() throughout this notebook for accurate GPU timing.\n",
        "def print_benchmark(result, device=\"gpu\"):\n",
        "    \"\"\"Print benchmark result showing only the relevant time.\"\"\"\n",
        "    if device == \"gpu\":\n",
        "        avg_ms = result.gpu_times.mean() * 1000\n",
        "        std_ms = result.gpu_times.std() * 1000\n",
        "        print(f\"{result.name}: {avg_ms:.3f} ms +/- {std_ms:.3f} ms\")\n",
        "    else:\n",
        "        avg_ms = result.cpu_times.mean() * 1000\n",
        "        std_ms = result.cpu_times.std() * 1000\n",
        "        print(f\"{result.name}: {avg_ms:.3f} ms +/- {std_ms:.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Creating Arrays: CPU vs. GPU\n",
        "\n",
        "Let's compare the performance of creating a large 3D array (approx. 2GB in size) on the CPU versus the GPU.\n",
        "\n",
        "We will use `np.ones` for the CPU and `cp.ones` for the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CPU creation\n",
        "print_benchmark(benchmark(np.ones, ((1000, 500, 500),), n_repeat=10), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU creation\n",
        "print_benchmark(benchmark(cp.ones, ((1000, 500, 500),), n_repeat=10), device=\"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see here that creating this array on the GPU is much faster than doing so on the CPU!\n",
        "\n",
        "**About `cupyx.profiler.benchmark`:**\n",
        "\n",
        "We use CuPy's built-in `benchmark` utility for timing GPU operations. This is important because GPU operations are **asynchronous** - when you call a CuPy function, the CPU places a task in the GPU's \"to-do list\" (stream) and immediately moves on without waiting.\n",
        "\n",
        "The `benchmark` function handles all the complexity of proper GPU timing for us:\n",
        "- It automatically synchronizes GPU streams to get accurate measurements.\n",
        "- It runs warm-up iterations to avoid cold-start overhead.\n",
        "- It reports both CPU and GPU times separately.\n",
        "\n",
        "This makes it the recommended way to time CuPy code, as it's both accurate and convenient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic Operations\n",
        "\n",
        "The syntax for mathematical operations is identical. Let's multiply every value in our arrays by `5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fresh arrays for the benchmark\n",
        "x_cpu = np.ones((1000, 500, 500))\n",
        "x_gpu = cp.ones((1000, 500, 500))\n",
        "\n",
        "def multiply(x):\n",
        "    return x * 5\n",
        "\n",
        "# CPU Operation\n",
        "print_benchmark(benchmark(multiply, (x_cpu,), n_repeat=10), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Operation\n",
        "print_benchmark(benchmark(multiply, (x_gpu,), n_repeat=10), device=\"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GPU completes this operation notably faster, with the code staying the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequential Operations & Memory\n",
        "\n",
        "Now let's do a couple of operations sequentially, something which would suffer from memory transfer times in Numba examples without explicit memory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sequential_math(x):\n",
        "    x = x * 5\n",
        "    x = x * x\n",
        "    x = x + x\n",
        "    return x\n",
        "\n",
        "# CPU: Sequential math\n",
        "print_benchmark(benchmark(sequential_math, (x_cpu,), n_repeat=10), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU: Sequential math\n",
        "print_benchmark(benchmark(sequential_math, (x_gpu,), n_repeat=10), device=\"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GPU ran that much faster even without us explicitly managing memory. This is because CuPy is handling all of this for us transparently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Complex Operations (Linear Algebra)\n",
        "\n",
        "GPUs excel at Linear Algebra. Let's look at **Singular Value Decomposition (SVD)**, a computationally heavy $O(N^3)$ operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CPU SVD\n",
        "x_cpu = np.random.random((1000, 1000))\n",
        "print_benchmark(benchmark(np.linalg.svd, (x_cpu,), n_repeat=5), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU SVD\n",
        "x_gpu = cp.random.random((1000, 1000))\n",
        "print_benchmark(benchmark(cp.linalg.svd, (x_gpu,), n_repeat=5), device=\"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GPU outperforms the CPU again with exactly the same API!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agnostic Code (NumPy Dispatch)\n",
        "\n",
        "A key feature of CuPy is that many **NumPy functions work on CuPy arrays without changing your code**.\n",
        "\n",
        "When you pass a CuPy GPU array (`x_gpu`) into a NumPy function that supports the `__array_function__` protocol (e.g., `np.linalg.svd`), NumPy detects the CuPy input and **delegates the operation to CuPy's own implementation**, which runs on the GPU.\n",
        "\n",
        "This allows you to write code using standard `np.*` syntax and have it run on either CPU or GPU seamlessly - **as long as CuPy implements an override for that function.**\n",
        "\n",
        "CuPy also protects you from hidden performance penalties: **it forbids implicit GPU → CPU copies**, raising a `TypeError` when NumPy tries to convert a `cupy.ndarray` into a `numpy.ndarray` behind the scenes. This ensures all device-to-host transfers are **explicit and intentional**, never silent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We create the data on the GPU\n",
        "x_gpu = cp.random.random((1000, 1000))\n",
        "\n",
        "# BUT we call the standard NumPy function - CuPy dispatches it to the GPU!\n",
        "print_benchmark(benchmark(np.linalg.svd, (x_gpu,), n_repeat=5), device=\"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Device Management\n",
        "\n",
        "If you have multiple GPUs, CuPy uses the concept of a \"Current Device\" context. \n",
        "\n",
        "You can use a `with` statement to ensure specific arrays are created on specific cards (e.g., GPU 0 vs GPU 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with cp.cuda.Device(0):\n",
        "   x_on_gpu0 = cp.random.random((100000, 1000))\n",
        "\n",
        "print(f\"Array is on device: {x_on_gpu0.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** CuPy functions generally expect all input arrays to be on the **same** device. Passing an array stored on a non-current device may work depending on the hardware configuration but is generally discouraged as it may not be performant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise - NumPy to CuPy\n",
        "\n",
        "### Part 1\n",
        "Let's put the \"Drop-in Replacement\" philosophy to the test with the same data pipeline as the previous notebook. Specifically, the single block of code below performs the following steps:\n",
        "1) Generate a massive dataset (50 million elements).\n",
        "2) Process it using a heavy operation (Sorting).\n",
        "3) Manipulate the shape and normalize the data (Broadcasting).\n",
        "4) Verify the integrity of the result.\n",
        "\n",
        "**TODO:**\n",
        "1. Run the cell below with `xp = np` (CPU Mode). Note the benchmark output.\n",
        "2. Change the setup line to `xp = cp` (GPU Mode). Run it again.\n",
        "3. Observe how the exact same logic runs significantly faster on the GPU with CuPy while retaining the implementation properties of NumPy.\n",
        "\n",
        "Note: We use `cupyx.profiler.benchmark` for timing, which automatically handles GPU synchronization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from cupyx.profiler import benchmark\n",
        "\n",
        "# Re-defined here so this exercise cell is self-contained and can run independently.\n",
        "def print_benchmark(result, device=\"gpu\"):\n",
        "    \"\"\"Print benchmark result showing only the relevant time.\"\"\"\n",
        "    if device == \"gpu\":\n",
        "        avg_ms = result.gpu_times.mean() * 1000\n",
        "        std_ms = result.gpu_times.std() * 1000\n",
        "    else:\n",
        "        avg_ms = result.cpu_times.mean() * 1000\n",
        "        std_ms = result.cpu_times.std() * 1000\n",
        "    print(f\"  -> {result.name}: {avg_ms:.3f} ms +/- {std_ms:.3f} ms\")\n",
        "\n",
        "# --- 1. SETUP: CHOOSE YOUR DEVICE ---\n",
        "# SOLUTION: Changed from 'np' to 'cp' for GPU acceleration\n",
        "xp = cp  # Toggle this to 'np' for CPU mode\n",
        "\n",
        "print(f\"Running on: {xp.__name__.upper()}\")\n",
        "\n",
        "# --- 2. DATA GENERATION ---\n",
        "N = 50_000_000\n",
        "print(f\"Generating {N:,} random elements ({N*8/1e9:.2f} GB)...\")\n",
        "arr = xp.random.rand(N)\n",
        "\n",
        "# --- 3. HEAVY COMPUTATION (TIMED) ---\n",
        "print(\"Sorting data...\")\n",
        "# benchmark() handles GPU synchronization automatically\n",
        "result = benchmark(xp.sort, (arr,), n_repeat=5)\n",
        "print_benchmark(result, device=\"gpu\" if xp == cp else \"cpu\")\n",
        "\n",
        "# --- 4. MANIPULATION & BROADCASTING ---\n",
        "# Purpose: Demonstrate that CuPy supports complex reshaping and broadcasting rules exactly like NumPy.\n",
        "# This shows you don't need to rewrite your data processing logic.\n",
        "\n",
        "# Reshape to a matrix with 5 columns\n",
        "arr_new = arr.reshape((-1, 5))\n",
        "\n",
        "# Normalize: Divide every row by its sum using broadcasting\n",
        "row_sums = arr_new.sum(axis=1)\n",
        "normalized_matrix = arr_new / row_sums[:, xp.newaxis]\n",
        "\n",
        "# --- 5. VERIFICATION ---\n",
        "# Purpose: Verify mathematical correctness/integrity of the result.\n",
        "check_sums = xp.sum(normalized_matrix, axis=1)\n",
        "xp.testing.assert_allclose(check_sums, 1.0)\n",
        "\n",
        "print(\"  -> Verification: PASSED (All rows sum to 1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TODO: When working with CuPy arrays, try changing `xp.testing.assert_allclose` to `np.testing.assert_allclose`. What happens and why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SOLUTION:**\n",
        "\n",
        "When you change `xp.testing.assert_allclose` to `np.testing.assert_allclose` while working with CuPy arrays (`xp = cp`), you will get a **`TypeError`**.\n",
        "\n",
        "This happens because:\n",
        "\n",
        "1. `np.testing.assert_allclose` internally tries to convert its inputs to NumPy arrays.\n",
        "2. CuPy arrays live on the GPU, and CuPy **explicitly forbids implicit GPU → CPU transfers**.\n",
        "3. When NumPy's `assert_allclose` attempts to call `np.asarray()` on the CuPy array, CuPy raises a `TypeError` to prevent a silent (and potentially slow) data copy from GPU to CPU memory.\n",
        "\n",
        "This is a **safety feature** of CuPy! It ensures that all device-to-host transfers are **explicit and intentional**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2\n",
        "We will now create a massive dataset (50 million points) representing a sine wave and see how fast the GPU can sort it compared to the CPU. \n",
        "\n",
        "**TODO:** \n",
        "1) **Generate Data:** Create a NumPy array (`y_cpu`) and a CuPy array (`y_gpu`) representing $\\sin(x)$ from $0$ to $2\\pi$ with `50,000,000` points.\n",
        "2) **Benchmark CPU and GPU:** Use `benchmark()` from `cupyx.profiler` to measure both `np.sort` and `cp.sort`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from cupyx.profiler import benchmark\n",
        "\n",
        "# --- Step 1: Generate Data ---\n",
        "N = 50_000_000\n",
        "print(f\"Generating {N:,} points...\")\n",
        "\n",
        "# SOLUTION: Create x_cpu using np.linspace from 0 to 2*pi\n",
        "x_cpu = np.linspace(0, 2 * np.pi, N)\n",
        "# SOLUTION: Create y_cpu by taking np.sin(x_cpu)\n",
        "y_cpu = np.sin(x_cpu)\n",
        "\n",
        "# SOLUTION: Create x_gpu using cp.linspace from 0 to 2*pi\n",
        "x_gpu = cp.linspace(0, 2 * cp.pi, N)\n",
        "# SOLUTION: Create y_gpu by taking cp.sin(x_gpu)\n",
        "y_gpu = cp.sin(x_gpu)\n",
        "\n",
        "print(f\"  CPU array shape: {y_cpu.shape}, dtype: {y_cpu.dtype}\")\n",
        "print(f\"  GPU array shape: {y_gpu.shape}, dtype: {y_gpu.dtype}\")\n",
        "\n",
        "# --- Step 2: Benchmark NumPy (CPU) ---\n",
        "print(\"\\nBenchmarking NumPy Sort (this may take a few seconds)...\")\n",
        "# SOLUTION: Use benchmark with np.sort\n",
        "cpu_result = benchmark(np.sort, (y_cpu,), n_repeat=5)\n",
        "cpu_avg_ms = cpu_result.cpu_times.mean() * 1000\n",
        "cpu_std_ms = cpu_result.cpu_times.std() * 1000\n",
        "print(f\"  NumPy (CPU): {cpu_avg_ms:.3f} ms +/- {cpu_std_ms:.3f} ms\")\n",
        "\n",
        "# --- Step 3: Benchmark CuPy (GPU) ---\n",
        "print(\"\\nBenchmarking CuPy Sort...\")\n",
        "# SOLUTION: Use benchmark with cp.sort\n",
        "gpu_result = benchmark(cp.sort, (y_gpu,), n_repeat=5)\n",
        "gpu_avg_ms = gpu_result.gpu_times.mean() * 1000\n",
        "gpu_std_ms = gpu_result.gpu_times.std() * 1000\n",
        "print(f\"  CuPy (GPU): {gpu_avg_ms:.3f} ms +/- {gpu_std_ms:.3f} ms\")\n",
        "\n",
        "# --- Summary ---\n",
        "print(f\"\\n*** GPU Speedup: {cpu_avg_ms / gpu_avg_ms:.1f}x faster ***\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**EXTRA CREDIT: Benchmark with different array sizes and find the size at which CuPy and NumPy take the same amount of time. Try to extract the timing data from `cupyx.profiler.benchmark`'s return value and customize how the output is displayed. You could even make a graph.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION: Extra Credit - Finding the crossover point between CPU and GPU performance\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from cupyx.profiler import benchmark\n",
        "\n",
        "# Define array sizes to test (evenly spaced from 1K to 5K)\n",
        "sizes = [1_000, 2_000, 3_000, 4_000, 5_000]\n",
        "\n",
        "cpu_times = []\n",
        "gpu_times = []\n",
        "\n",
        "print(\"Benchmarking different array sizes...\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Size':>15} | {'NumPy (CPU)':>15} | {'CuPy (GPU)':>15} | {'Winner':>10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for N in sizes:\n",
        "    # Generate sine wave data\n",
        "    x_cpu = np.linspace(0, 2 * np.pi, N)\n",
        "    y_cpu = np.sin(x_cpu)\n",
        "    \n",
        "    x_gpu = cp.linspace(0, 2 * cp.pi, N)\n",
        "    y_gpu = cp.sin(x_gpu)\n",
        "    \n",
        "    # Benchmark CPU\n",
        "    cpu_result = benchmark(np.sort, (y_cpu,), n_repeat=10)\n",
        "    cpu_time_ms = cpu_result.cpu_times.mean() * 1000\n",
        "    cpu_times.append(cpu_time_ms)\n",
        "    \n",
        "    # Benchmark GPU\n",
        "    gpu_result = benchmark(cp.sort, (y_gpu,), n_repeat=10)\n",
        "    gpu_time_ms = gpu_result.gpu_times.mean() * 1000\n",
        "    gpu_times.append(gpu_time_ms)\n",
        "    \n",
        "    # Determine winner\n",
        "    winner = \"GPU\" if gpu_time_ms < cpu_time_ms else \"CPU\"\n",
        "    \n",
        "    print(f\"{N:>15,} | {cpu_time_ms:>12.3f} ms | {gpu_time_ms:>12.3f} ms | {winner:>10}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find approximate crossover point\n",
        "crossover_idx = None\n",
        "for i in range(len(sizes) - 1):\n",
        "    # Check if GPU becomes faster between size[i] and size[i+1]\n",
        "    if cpu_times[i] <= gpu_times[i] and cpu_times[i+1] > gpu_times[i+1]:\n",
        "        crossover_idx = i\n",
        "        break\n",
        "\n",
        "if crossover_idx is not None:\n",
        "    print(f\"\\nCrossover point: GPU becomes faster between {sizes[crossover_idx]:,} and {sizes[crossover_idx+1]:,} elements\")\n",
        "else:\n",
        "    if gpu_times[0] < cpu_times[0]:\n",
        "        print(f\"\\nGPU is faster for all tested sizes (even at {sizes[0]:,} elements)\")\n",
        "    else:\n",
        "        print(f\"\\nCPU is faster for all tested sizes (even at {sizes[-1]:,} elements)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION: Extra Credit (continued) - Visualization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Absolute times (log-log scale)\n",
        "ax1.loglog(sizes, cpu_times, 'b-o', label='NumPy (CPU)', linewidth=2, markersize=8)\n",
        "ax1.loglog(sizes, gpu_times, 'r-s', label='CuPy (GPU)', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Array Size (elements)', fontsize=12)\n",
        "ax1.set_ylabel('Time (ms)', fontsize=12)\n",
        "ax1.set_title('Sort Performance: CPU vs GPU', fontsize=14)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Speedup ratio\n",
        "speedups = [cpu / gpu for cpu, gpu in zip(cpu_times, gpu_times)]\n",
        "colors = ['green' if s > 1 else 'red' for s in speedups]\n",
        "ax2.bar(range(len(sizes)), speedups, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax2.axhline(y=1.0, color='black', linestyle='--', linewidth=2, label='Break-even')\n",
        "ax2.set_xticks(range(len(sizes)))\n",
        "ax2.set_xticklabels([f'{s:,}' for s in sizes], rotation=45, ha='right', fontsize=9)\n",
        "ax2.set_xlabel('Array Size (elements)', fontsize=12)\n",
        "ax2.set_ylabel('GPU Speedup (CPU time / GPU time)', fontsize=12)\n",
        "ax2.set_title('GPU Speedup Factor', fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (speedup, color) in enumerate(zip(speedups, colors)):\n",
        "    label = f'{speedup:.1f}x'\n",
        "    ax2.annotate(label, (i, speedup), textcoords=\"offset points\", \n",
        "                 xytext=(0, 5), ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n*** Analysis Complete ***\")\n",
        "print(f\"Maximum GPU speedup: {max(speedups):.1f}x at {sizes[speedups.index(max(speedups))]:,} elements\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
