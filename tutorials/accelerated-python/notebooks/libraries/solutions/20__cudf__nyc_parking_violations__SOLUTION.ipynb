{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrames with Pandas and cuDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#1.-Introduction)\n",
        "2. [Pandas Essentials](#2.-Pandas-Essentials)\n",
        "   - [2.1 Series and DataFrame Objects](#2.1-Series-and-DataFrame-Objects)\n",
        "   - [2.2 Selecting and Filtering Data](#2.2-Selecting-and-Filtering-Data)\n",
        "   - [2.3 Sorting](#2.3-Sorting)\n",
        "   - [2.4 Summarizing Data](#2.4-Summarizing-Data)\n",
        "   - [2.5 Grouped Aggregations (groupby)](#2.5-Grouped-Aggregations-(groupby))\n",
        "   - [2.6 String Operations](#2.6-String-Operations)\n",
        "   - [2.7 Time Series](#2.7-Time-Series)\n",
        "   - [2.8 User-Defined Operations (apply)](#2.8-User-Defined-Operations-(apply))\n",
        "3. [Enter cuDF: GPU DataFrames](#3.-Enter-cuDF:-GPU-DataFrames)\n",
        "   - [3.1 Exercise: Date Formatting Failure](#3.1-Exercise:-Date-Formatting-Failure)\n",
        "   - [3.2 Exercise: Why `.apply()` Breaks Down in cuDF](#3.2-Exercise:-Why-`.apply()`-Breaks-Down-in-cuDF)\n",
        "4. [Exercise: Analyzing Real Data (NYC Parking Violations)](#4.-Exercise:-Analyzing-Real-Data-(NYC-Parking-Violations))\n",
        "   - [Step 0: Download Data](#Step-0:-Download-Data)\n",
        "   - [Task 1: Data Inspection (Pandas)](#ðŸ“-Task-1:-Data-Inspection-(Pandas))\n",
        "   - [Task 2: Analyze Taxis (Pandas)](#ðŸ“-Task-2:-Analyze-Taxis-(Pandas))\n",
        "   - [Task 3: GPU Acceleration (cuDF)](#ðŸ“-Task-3:-GPU-Acceleration-(cuDF))\n",
        "5. [Conclusion](#Conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "In this notebook, we will build a foundation in data manipulation using **Pandas**, a popular tool for Python data analysis. Then, we will transition to **cuDF**, which allows us to run standard Pandas-like code on the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "- **Introduce core Pandas operations:** Indexing, Filtering, Aggregating, and Time Series.\n",
        "- **Learn the subtle differences** (and speed benefits) when porting code to cuDF.\n",
        "- **Exercise:** Apply these skills to analyze a real-world NYC Parking Violations dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Throughout this notebook, we provide \"Quick Docs\" sections to remind you of common syntax. However, these are not exhaustive. For complete API details, parameters, and edge cases, you should always reference the official [Pandas Documentation](https://pandas.pydata.org/docs/) or the [cuDF Documentation](https://docs.rapids.ai/api/cudf/stable/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Pandas Essentials\n",
        "\n",
        "Before we accelerate with GPUs, let's ensure we are comfortable with the DataFrame API. Even if you are a Pandas pro, this refresher sets the baseline syntax we will replicate later.\n",
        "\n",
        "First, import the library:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Series and DataFrame Objects\n",
        "\n",
        "- **Series:** A one-dimensional labeled array (like a powerful list or a single column).\n",
        "- **DataFrame:** A two-dimensional labeled data structure (like a spreadsheet or SQL table).\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `pd.Series(data)`: Create a Series.\n",
        "- `pd.DataFrame(data, index)`: Create a DataFrame.\n",
        "- `df.head(n)` / `df.tail(n)`: View the first/last n rows.\n",
        "- `df.index` / `df.columns`: Access row labels and column names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A Series acts like a single column of data\n",
        "s = pd.Series([10, 20, 30])\n",
        "print(f\"Max value in series: {s.max()}\")\n",
        "\n",
        "# A DataFrame is a collection of Series sharing an index\n",
        "df = pd.DataFrame({\n",
        "    \"a\": [1, 2, 1, 3, 2],\n",
        "    \"b\": [1, 4, 7, 2, 0],\n",
        "    \"c\": [3, 3, 3, 4, 5]\n",
        "}, index=[1, 2, 3, 4, 5])\n",
        "\n",
        "# View the structure\n",
        "print(\"Columns:\", df.columns)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Selecting and Filtering Data\n",
        "\n",
        "Selecting specific subsets of data is the most common task in analysis. You can select by column name, label index, or integer position.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `df['col']`: Select a single column (returns a Series).\n",
        "- `df[['col1', 'col2']]`: Select multiple columns (returns a DataFrame).\n",
        "- `df.loc[label]`: Select row(s) by index label.\n",
        "- `df.iloc[position]`: Select row(s) by integer position (0-based).\n",
        "- `df[condition]`: Boolean indexing (filtering).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select specific columns\n",
        "subset = df[[\"b\", \"c\"]]\n",
        "\n",
        "# Select rows by label (loc) and position (iloc)\n",
        "row_label_2 = df.loc[2]     # Row with index label 2\n",
        "row_pos_0 = df.iloc[0]      # First row (physically)\n",
        "\n",
        "# Boolean Indexing: Filter rows where column 'a' is greater than 1\n",
        "filtered_df = df[df['a'] > 1]\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Sorting\n",
        "\n",
        "Ordering data helps in ranking and visualization.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `df.sort_values(by='col', ascending=True/False)`: Sort by one or more columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort by column 'a' in ascending order\n",
        "sorted_df = df.sort_values(\"a\")\n",
        "sorted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Summarizing Data\n",
        "\n",
        "It's straightforward to get a quick overview of your data's distribution.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `df.describe()`: Summary statistics (count, mean, std, etc.).\n",
        "- `df.mean()`, `df.sum()`, `df.max()`: Aggregations across columns.\n",
        "- `df['col'].value_counts()`: Count unique values (useful for histograms).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the sum of every column\n",
        "print(\"Sum of columns:\\n\", df.sum())\n",
        "\n",
        "# Count frequency of values in column 'a'\n",
        "print(\"\\nValue counts for 'a':\\n\", df[\"a\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Grouped Aggregations (groupby)\n",
        "\n",
        "The \"Split-Apply-Combine\" strategy. Split data into groups based on some criteria, apply a function to each group, and combine the results.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `df.groupby('col')`: Group data.\n",
        "- `.mean()`, `.count()`: Apply aggregation.\n",
        "- `.agg({'col': ['min', 'max']})`: Apply complex, specific aggregations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by 'a' and calculate the mean of 'b' and 'c' for each group\n",
        "grouped_mean = df.groupby(\"a\").mean()\n",
        "print(grouped_mean)\n",
        "\n",
        "# Complex aggregation: Get min and mean of 'b', and max of 'c'\n",
        "agg_df = df.groupby(\"a\").agg({\n",
        "    \"b\": [\"min\", \"mean\"],\n",
        "    \"c\": [\"max\"]\n",
        "})\n",
        "agg_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 String Operations\n",
        "\n",
        "Pandas provides vectorized string functions via the `.str` accessor.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `df['col'].str.upper()`: Convert to uppercase.\n",
        "- `df['col'].str.contains('pattern')`: Boolean check for substring.\n",
        "- `df['col'].str.replace('old', 'new')`: Replace text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a string column\n",
        "df[\"names\"] = [\"mario\", \"luigi\", \"yoshi\", \"peach\", \"toad\"]\n",
        "\n",
        "# Convert to uppercase\n",
        "df[\"names_upper\"] = df[\"names\"].str.upper()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.7 Time Series\n",
        "\n",
        "Pandas was originally developed for financial time series analysis. It handles dates and times robustly via the `.dt` accessor.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `pd.to_datetime()`: Convert strings to datetime objects.\n",
        "- `df['date'].dt.year`: Extract year component.\n",
        "- `df['date'].dt.dayofweek`: Extract day of week.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a date range\n",
        "date_df = pd.DataFrame()\n",
        "date_df[\"date\"] = pd.date_range(\"2018-11-20\", periods=5, freq=\"D\")\n",
        "date_df[\"value\"] = np.random.sample(len(date_df))\n",
        "\n",
        "# Filter by date\n",
        "subset_dates = date_df[date_df[\"date\"] < \"2018-11-23\"]\n",
        "\n",
        "# Extract features\n",
        "date_df[\"year\"] = date_df[\"date\"].dt.year\n",
        "date_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.8 User-Defined Operations (apply)\n",
        "\n",
        "When built-in functions aren't enough, you can apply custom Python functions.\n",
        "\n",
        "**Quick Docs:**\n",
        "\n",
        "- `df['col'].apply(func)`: Apply function `func` to every element.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_ten(x):\n",
        "    return x + 10\n",
        "\n",
        "# Apply the custom function\n",
        "df[\"a_plus_10\"] = df[\"a\"].apply(add_ten)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Enter cuDF: GPU DataFrames\n",
        "\n",
        "cuDF mimics the Pandas API but runs on the GPU. The transition is often as simple as changing the import, but there are some constraints you must know.\n",
        "\n",
        "First, let's create a GPU DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cudf\n",
        "\n",
        "# Create a cuDF DataFrame (data resides on GPU)\n",
        "gdf = cudf.DataFrame({\n",
        "    \"a\": [1, 2, 1, 3, 2],\n",
        "    \"b\": [1, 4, 7, 2, 0],\n",
        "    \"c\": [1, 1, 8, 2, 9]\n",
        "}, index=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Operations work exactly the same!\n",
        "print(type(gdf))\n",
        "gdf.groupby(\"a\").mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Exercise: Date Formatting Failure\n",
        "\n",
        "Pandas is very forgiving with date formats. cuDF is stricter. Run the cell below to see what happens when you use a non-standard date string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXECUTE THIS CELL TO SEE THE ERROR\n",
        "try:\n",
        "    date_df = cudf.DataFrame()\n",
        "    # Pandas handles \"11/20/2018\" easily. Does cuDF?\n",
        "    date_df[\"date\"] = cudf.date_range(\"11/20/2018\", periods=72, freq=\"D\")\n",
        "except Exception as e:\n",
        "    print(f\"Error caught: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why did this fail?** Unlike Pandas, cuDF currently requires ISO-standard date formats (Year-Month-Day) for creating date ranges.\n",
        "\n",
        "- **Pandas:** Guesses `11/20/2018` is Nov 20th.\n",
        "- **cuDF:** Requires `2018-11-20`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Exercise: Why `.apply()` Breaks Down in cuDF\n",
        "\n",
        "In Pandas, `.apply()` works by executing Python-level code over the data, which makes it very flexible but CPU-centric. On the GPU, this model does not translate directly: GPUs cannot execute arbitrary Python bytecode. In cuDF, `.apply()`-style user-defined functions must be JIT-compiled with Numba for the CUDA target, which imposes constraints:\n",
        "- The function must be Numba-compilable (no general Python objects or unsupported features).\n",
        "- Only operations that Numba can lower to GPU device code are allowed.\n",
        "- Code must follow a more restricted, compilation-friendly execution model.\n",
        "\n",
        "Note that cuDF UDFs are not limited to pure math. Some string operations and well-defined null-handling patterns are supported. See the [cuDF apply() documentation](https://docs.rapids.ai/api/cudf/latest/user_guide/api_docs/api/cudf.dataframe.apply/) for the full set of supported features and limitations.\n",
        "\n",
        "Even simple-looking Python functions often fall outside these constraints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A function that looks innocent but is NOT GPU-safe\n",
        "def add_ten_verbose(x):\n",
        "    # Python branching + dynamic typing make this un-compilable for the GPU\n",
        "    if isinstance(x, (int, float)):\n",
        "        return x + 10\n",
        "    else:\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is perfectly legal in Pandas. But in cuDF, Numba cannot:\n",
        "\n",
        "- interpret `isinstance`\n",
        "- handle Python branching on object types\n",
        "- JIT-compile dynamic return values\n",
        "\n",
        "Now try running it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute this cell to observe the cuDF limitation\n",
        "try:\n",
        "    gdf[\"a\"] = gdf[\"a\"].apply(add_ten_verbose)\n",
        "except Exception as e:\n",
        "    print(\"cuDF apply() constraint caught:\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What happens?** cuDF attempts to compile the function â†’ compilation fails â†’ you get a runtime error. This mirrors real-world failure modes: anything that is not pure numerical logic will break.\n",
        "\n",
        "Here is the same logic, rewritten in a way the GPU can compile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU-safe version: no Python, no branching, pure math\n",
        "def add_ten_gpu(x):\n",
        "    return x + 10\n",
        "try:\n",
        "    gdf[\"a\"] = gdf[\"a\"].apply(add_ten_gpu)\n",
        "except Exception as e:\n",
        "    print(\"cuDF apply() constraint caught:\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is one of the few forms that Numba can translate. But even this version is not preferred. **cuDF `.apply()` is a last resort**. Even if your function compiles, `.apply()` still triggers:\n",
        "\n",
        "- JIT compilation overhead (slow startup)\n",
        "- Kernel launch overhead\n",
        "- Reduced optimization compared to built-in GPU operations\n",
        "\n",
        "For typical column transformations, this is simply unnecessary. \n",
        "**Best practice is to always use vectorized operations:**\n",
        "\n",
        "```python\n",
        "gdf[\"a\"] + 10\n",
        "```\n",
        "The vectorized version is:\n",
        "- faster\n",
        "- simpler\n",
        "- more readable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Exercise: Analyzing Real Data (NYC Parking Violations)\n",
        "\n",
        "Now you will apply what you learned to a large, real-world dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0: Download Data\n",
        "\n",
        "We will fetch a subset of the NYC Parking Violations dataset (Fiscal Year 2022).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -nc https://data.rapids.ai/datasets/nyc_parking/nyc_parking_violations_2022.parquet -O nyc_parking_violations_2022.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Data Inspection (Pandas)\n",
        "\n",
        "**Goal:** Load the data and inspect its structure.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Read the file `nyc_parking_violations_2022.parquet` into a Pandas DataFrame.\n",
        "2. Print the columns.\n",
        "3. Create a subset DataFrame with only: `Registration State`, `Violation Description`, `Vehicle Body Type`, `Issue Date`.\n",
        "4. Display the head of this subset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read parquet file\n",
        "df = pd.read_parquet(\"nyc_parking_violations_2022.parquet\")\n",
        "\n",
        "# Print columns\n",
        "print(df.columns)\n",
        "\n",
        "# Select specific columns\n",
        "df_subset = df[[\"Registration State\", \"Violation Description\", \"Vehicle Body Type\", \"Issue Date\"]]\n",
        "\n",
        "# Display head\n",
        "df_subset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Analyze Taxis (Pandas)\n",
        "\n",
        "**Goal:** Filter, Group, and Count.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Filter the DataFrame to find rows where `Vehicle Body Type` is `\"TAXI\"`.\n",
        "2. Group by `Registration State`.\n",
        "3. Count the occurrences to see which states the taxis are registered in.\n",
        "4. Sort the results descending to find the top states.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Filter for TAXI\n",
        "taxi_df = df_subset[df_subset[\"Vehicle Body Type\"] == \"TAXI\"]\n",
        "\n",
        "# Group by State and count\n",
        "state_counts = taxi_df.groupby(\"Registration State\").size()\n",
        "\n",
        "# Sort and display top results\n",
        "state_counts.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: GPU Acceleration (cuDF)\n",
        "\n",
        "**Goal:** Measure the speedup.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Import `cudf`.\n",
        "2. Use `%%time` at the top of the cell.\n",
        "3. Replicate the entire pipeline (Read -> Filter columns -> Filter Rows -> Group -> Sort) using `cudf`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "import cudf\n",
        "\n",
        "# Read parquet file into cuDF DataFrame\n",
        "gdf = cudf.read_parquet(\"nyc_parking_violations_2022.parquet\")\n",
        "\n",
        "# Select specific columns (Filter columns)\n",
        "gdf_subset = gdf[[\"Registration State\", \"Violation Description\", \"Vehicle Body Type\", \"Issue Date\"]]\n",
        "\n",
        "# Filter for TAXI (Filter Rows)\n",
        "taxi_gdf = gdf_subset[gdf_subset[\"Vehicle Body Type\"] == \"TAXI\"]\n",
        "\n",
        "# Group by State and count\n",
        "state_counts_gpu = taxi_gdf.groupby(\"Registration State\").size()\n",
        "\n",
        "# Sort and display top results\n",
        "state_counts_gpu.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Compare the **Wall time** of Task 2 vs Task 3. You should see a significant performance improvement with cuDF, especially as data size grows!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (RAPIDS 25.10)",
      "language": "python",
      "name": "cudf-cu12-25.10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
