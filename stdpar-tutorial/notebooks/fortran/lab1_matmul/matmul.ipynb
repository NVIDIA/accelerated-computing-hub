{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with ISO Fortran\n",
    "===\n",
    "\n",
    "# Lab 1: Accelerated Math Intrinsics\n",
    "\n",
    "This exercise demonstrates how to use the accelerated math intrinsics and compile them for CPU and GPU execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: matrix-matrix multiply (`matmul`)\n",
    "\n",
    "The exercise template [exercise1.f90](./exercise1.f90) implements a matrix-matrix multiply in Fortran as follows:\n",
    "\n",
    "```fortran\n",
    "do j= 1, nj\n",
    " do k= 1, nk\n",
    "  do i = 1, ni\n",
    "   d(i,j) = d(i,j) + a(i,k) * b(k,j)\n",
    "  end do      \n",
    " end do\n",
    "end do\n",
    "```\n",
    "The command line arguments for the binary are `./matmul ni nj nk niterations`.\n",
    "\n",
    "The goal of this first exercise is to use Fortran math intrinsic [matmul](https://gcc.gnu.org/onlinedocs/gfortran/MATMUL.html) to implement this instead.\n",
    "\n",
    "Replace the loops under the `! TODO`s in [exercise1.f90](./exercise1.f90) to use the `matmul` intrinsic.\n",
    "\n",
    "The following cells compile the code and measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f matmul\n",
    "!nvfortran -Ofast exercise1.f90 -o matmul\n",
    "!./matmul 1024 1024 1024 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, let's use `nvfortran` accelerated math intrinsics.\n",
    "\n",
    "These are selected using:\n",
    "\n",
    "- `-stdpar=multicore` for multicore CPU execution\n",
    "- `-stdpar=gpu` for GPU execution, which requires also passing the following flags:\n",
    "  - `-cuda` for enabling CUDA Fortran\n",
    "  - `-cudalib=cutensor,curand` for linking the `cuTensor` library, which the accelerated math intrinsics use\n",
    " \n",
    "Let's try these out. We may pick a larger matrix size, since now performance is much higher than with the hand-rolled triple-nested loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f matmul\n",
    "!nvfortran -Ofast -stdpar=gpu -cuda -cudalib=cutensor,curand exercise1.f90 -o matmul\n",
    "!./matmul 1024 1024 1024 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f matmul\n",
    "!nvfortran -Ofast -stdpar=gpu -cuda -cudalib=cutensor,curand solutions/exercise1.f90 -o matmul\n",
    "!./matmul 4096 4096 4096 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have a multi-threaded implementation of `matmul` in our runtime yet, so the following run sequentially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f matmul\n",
    "!nvfortran -Ofast -stdpar=multicore solutions/exercise1.f90 -o matmul\n",
    "!./matmul 4096 4096 4096 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
