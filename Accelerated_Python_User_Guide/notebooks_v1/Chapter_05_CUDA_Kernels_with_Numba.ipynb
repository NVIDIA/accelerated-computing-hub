{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fab639be-7012-4227-8179-077ce5e40816",
   "metadata": {},
   "source": [
    "# Chapter 5: CUDA Kernels with Numba\n",
    "\n",
    "<img src=\"images/chapter-05/numba_title.png\" style=\"width:442px;\"/>\n",
    "\n",
    "Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n",
    "\n",
    "Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00a9d1-46df-4e79-bf44-a40944144cff",
   "metadata": {},
   "source": [
    "## Numba Basics\n",
    "\n",
    "Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically.  Numba’s integrated compilation system allows the creation of code using the characteristics of both the CPU and GPU in such a way that does not require many changes to the Python language.\n",
    "\n",
    "### Installation\n",
    "\n",
    "Before setting up your Numba programming environment, first ensure that you have fulfilled the following prerequisites (if you followed the instructions for installing CuPy, you can skip these steps):\n",
    "- CUDA-compatible GPU.  (see https://developer.nvidia.com/cuda-gpus for a list of NVIDIA GPUs)\n",
    "- CUDA-compatible NVIDIA Drivers.\n",
    "- CUDA Toolkit\n",
    "\n",
    "See installation instructions here: https://numba.pydata.org/numba-doc/latest/user/installing.html \n",
    "\n",
    "### Creating a Kernel Function with `@cuda.jit`\n",
    "\n",
    "In Numba, the `@jit` decorator is used to specify a function to be optimized by the Numba just in time compiler.  Within the context of GPU’s we use a version called `@cuda.jit` to specify kernel functions to be optimized for execution by multiple threads on the GPU simultaneously.\n",
    "\n",
    "```python\n",
    "from numba import cuda\n",
    "from numba import config as numba_config\n",
    "numba_config.CUDA_ENABLE_PYNVJITLINK = True\n",
    "\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    # code block goes here\n",
    "\n",
    "This should look very familiar to using numba on the CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083129b4-dd95-47e3-b3e5-0113e6f47b01",
   "metadata": {},
   "source": [
    "### Launching a Kernel Function\n",
    "\n",
    "Before running a kernel function, the number of blocks and threads per block need\n",
    " to be specified.  This will define the execution grid’s shape.\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    # Thread id in a 1D block\n",
    "    thread_id = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    block_id = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    block_width = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    i = thread_id + block_id * block_width\n",
    "    if i < an_array.size:  # Check array boundaries\n",
    "        output_array[i] = input_array[i]\n",
    "```\n",
    "\n",
    "To call the `foo()` function, we have to specify the block and grid size.\n",
    "\n",
    "```python\n",
    "input = np.asarray(range(10))\n",
    "output = np.zeros(len(input))\n",
    "\n",
    "\n",
    "block_threads = 32\n",
    "grid_blocks = (input.size + (block_threads - 1)) // block_threads\n",
    "\n",
    "foo[grid_blocks, block_threads](input, output)\n",
    "```\n",
    "\n",
    "For simple examples, the `cuda.grid()` function is a convenient way to manage thr\n",
    "eads, blocks and grids.  The complete script can be re-written this way:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba import config as numba_config\n",
    "numba_config.CUDA_ENABLE_PYNVJITLINK = True\n",
    "\n",
    "input = np.asarray(range(10))\n",
    "output = np.zeros(len(input))\n",
    "\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    i = cuda.grid(1)\n",
    "    output_array[i] = input_array[i]\n",
    "    \n",
    "foo[1, len(input)](input, output)\n",
    "\n",
    "output\n",
    "```\n",
    "\n",
    "Note: When a CUDA kernel executes, the call returns immediately before the kernel execution is complete.  The kernel execution then needs to be synchronized in order to ensure the results are transferred back to the CPU.  Without completing this step, you may run into memory errors where subsequent calls are trying to read or write to restricted memory.  Use cuda.synchronize() to ensure data consistency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de366c-b222-498e-861a-36acc997c9d7",
   "metadata": {},
   "source": [
    "### Specifying the Number of Threads and Blocks\n",
    "\n",
    "Don't worry too much about this now. Just take away the idea that we need to specify the number of times we want our kernel to be called, and that is given as two numbers which are multiplied together to give your overall grid size.  This setup will ensure a grid size that has enough threads to handle the size of the data, even if that number is not an exact multiple of the threads per block.\n",
    "\n",
    "Rules of thumb for threads per block:\n",
    "- Optimal block size is usually a multiple of 32 (warp size).\n",
    "- Profiling and benchmarking are required to determine the optimal value. \n",
    "\n",
    "Getting Started:\n",
    "- NSight’s Occupancy Calculator: https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator)\n",
    "- Several sources recommend starting with a number between 128 and 256 to begin tuning.\n",
    "\n",
    "Block and grid dimensions will affect CUDA performance.  Larger blocks can lead to better utilization of the shared memory and reduce the overhead of launching many small blocks.  However, excessively large blocks might reduce the number of blocks that can execute concurrently which will underutilize the GPU.  Finding the right balance is necessary in order to take advantage of the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5185d-1507-44dd-8930-dd3c33a908bd",
   "metadata": {},
   "source": [
    "## Numba with CuPy\n",
    "\n",
    "CuPy’s `cupy.ndarray` implements `__cuda_array_interface__`, which is the CUDA array interchange interface compatible with Numba v0.39.0 or later (see Numba’s CUDA Array Interface for details https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html). It means you can pass CuPy arrays to kernels JITed with Numba.\n",
    "\n",
    "In this example, we use `cupy` arrays instead of `numpy` arrays:\n",
    "\n",
    "```python\n",
    "import cupy\n",
    "from numba import cuda\n",
    "from numba import config as numba_config\n",
    "numba_config.CUDA_ENABLE_PYNVJITLINK = True\n",
    "\n",
    "@cuda.jit\n",
    "def add(x_array, y_array, output_array):\n",
    "        start = cuda.grid(1)\n",
    "        stride = cuda.gridsize(1)\n",
    "        for i in range(start, x.shape[0], stride):\n",
    "                output_array[i] = x_array[i] + y_array[i]\n",
    "\n",
    "a = cupy.arange(10)\n",
    "b = a * 2\n",
    "out = cupy.zeros_like(a)\n",
    "\n",
    "add[1, 32](a, b, out)\n",
    "\n",
    "print(out)  # => [ 0  3  6  9 12 15 18 21 24 27]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e753a7-17e9-49b7-b32b-8ea84fcdcbc8",
   "metadata": {},
   "source": [
    "## Links to Handy References\n",
    "Numba for CUDA GPU’s: https://numba.pydata.org/numba-doc/latest/cuda/index.html \n",
    "\n",
    "CuPy’s interoperability guide (includes Numba): https://docs.cupy.dev/en/stable/user_guide/interoperability.html \n",
    "\n",
    "Numba Github repository: https://github.com/numba/numba \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90e798-1097-432c-8464-df20bcd44ca2",
   "metadata": {},
   "source": [
    "# Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f01c8-7a37-459e-96b3-8d8c6fef7b40",
   "metadata": {},
   "source": [
    "## Defining and Launching a Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee68a00-ed02-46b9-be8d-33f44cd0dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba import config as numba_config\n",
    "numba_config.CUDA_ENABLE_PYNVJITLINK = True\n",
    "\n",
    "input = np.asarray(range(10))\n",
    "output = np.zeros(len(input))\n",
    "\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    # Thread id in a 1D block\n",
    "    thread_id = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    block_id = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    block_width = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    i = thread_id + block_id * block_width\n",
    "    if i < an_array.size:  # Check array boundaries\n",
    "        output_array[i] = input_array[i]\n",
    "\n",
    "block_threads = 32\n",
    "grid_blocks = (input.size + (block_threads - 1)) // block_threads\n",
    "\n",
    "foo[grid_blocks, block_threads](input, output)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fdb03-8c5a-4ace-a92c-0a3e9b8e5a2d",
   "metadata": {},
   "source": [
    "## Simplified Kernel Function Using grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04229f-cf77-431b-a194-d9b591aac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba import config as numba_config\n",
    "numba_config.CUDA_ENABLE_PYNVJITLINK = True\n",
    "\n",
    "input = np.asarray(range(10))\n",
    "output = np.zeros(len(input))\n",
    "\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    i = cuda.grid(1)\n",
    "    output_array[i] = input_array[i]\n",
    "    \n",
    "foo[1, len(input)](input, output)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ecfb6-d788-4706-b0f7-97ce7dd7d14d",
   "metadata": {},
   "source": [
    "## Using Numba with CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4242a8-ac95-4c7a-b9e6-ce1dff726519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "from numba import cuda\n",
    "from numba import config as numba_config\n",
    "numba_config.CUDA_ENABLE_PYNVJITLINK = True\n",
    "\n",
    "@cuda.jit\n",
    "def add(x_array, y_array, output_array):\n",
    "        start = cuda.grid(1)\n",
    "        stride = cuda.gridsize(1)\n",
    "        for i in range(start, x.shape[0], stride):\n",
    "                output_array[i] = x_array[i] + y_array[i]\n",
    "\n",
    "a = cupy.arange(10)\n",
    "b = a * 2\n",
    "out = cupy.zeros_like(a)\n",
    "\n",
    "add[1, 32](a, b, out)\n",
    "\n",
    "print(out)  # => [ 0  3  6  9 12 15 18 21 24 27]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
