{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9a5bf9",
   "metadata": {},
   "source": [
    "# CUDA Core Tutorial - Low-Level GPU Programming\n",
    "## Table of Contents\n",
    "\n",
    "1. Introduction to cuda.core\n",
    "2. Setting Up Your Environment\n",
    "3. Understanding CUDA Concepts\n",
    "4. Memory Management\n",
    "5. Kernel Compilation and Execution\n",
    "6. Streams and Synchronization\n",
    "7. Error Handling\n",
    "8. Performance Optimization\n",
    "9. Practical Examples\n",
    "10. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a456f7",
   "metadata": {},
   "source": [
    "## 1. Introduction to cuda.core\n",
    "The `cuda.core` module provides direct access to the CUDA driver API, giving you maximum control over GPU programming. Unlike high-level APIs, cuda.core requires you to manage everything manually:\n",
    "\n",
    "* Context management: Creating and managing execution contexts\n",
    "* Memory allocation: Explicitly allocating and freeing GPU memory\n",
    "* Kernel compilation: Compiling CUDA C/C++ code at runtime\n",
    "* Synchronization: Managing streams and events\n",
    "\n",
    "When to use cuda.core:\n",
    "\n",
    "* Maximum performance control\n",
    "* Custom memory management patterns\n",
    "* Integration with existing CUDA C/C++ code\n",
    "* Fine-grained GPU resource management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b043e1",
   "metadata": {},
   "source": [
    "## 2. Setting Up Your Environment\n",
    "### Prerequisites\n",
    "\n",
    "* NVIDIA GPU with CUDA capability\n",
    "* CUDA driver version 12.2 or higher\n",
    "* Python 3.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dbcf1",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0771178",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2111655890.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 -m pip install cuda-python numpy\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install cuda-python numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2622788",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda import core\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available\n",
    "try:\n",
    "    core.cuInit(0)\n",
    "    print(\"CUDA initialized successfully!\")\n",
    "    \n",
    "    # Get device count\n",
    "    device_count = core.cuDeviceGetCount()\n",
    "    print(f\"Number of CUDA devices: {device_count}\")\n",
    "    \n",
    "    # Get device properties\n",
    "    device = core.cuDeviceGet(0)\n",
    "    name = core.cuDeviceGetName(device)\n",
    "    print(f\"Device 0: {name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CUDA initialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76daf10e",
   "metadata": {},
   "source": [
    "## 3. Understanding CUDA Concepts\n",
    "### Key Terminology\n",
    "**Host vs Device:**\n",
    "\n",
    "* **Host**: Your CPU and system memory\n",
    "* **Device**: Your GPU and video memory\n",
    "\n",
    "**Execution Model**:\n",
    "\n",
    "* **Kernel**: A function that runs on the GPU\n",
    "* **Thread**: Individual execution unit\n",
    "* **Block**: Group of threads that can cooperate\n",
    "* **Grid**: Collection of blocks\n",
    "\n",
    "**Memory Hierarchy**:\n",
    "\n",
    "* **Global Memory**: Main GPU memory (slow but large)\n",
    "* **Shared Memory**: Fast memory shared within a block\n",
    "* **Registers**: Fastest memory, private to each thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e7496",
   "metadata": {},
   "source": [
    "### Creating a CUDA Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda import core\n",
    "\n",
    "# Initialize CUDA\n",
    "core.cuInit(0)\n",
    "\n",
    "# Get the first GPU device\n",
    "device = core.cuDeviceGet(0)\n",
    "\n",
    "# Create a context for this device\n",
    "context = core.cuCtxCreate(0, device)\n",
    "\n",
    "print(\"CUDA context created successfully!\")\n",
    "\n",
    "# Always clean up when done\n",
    "# core.cuCtxDestroy(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340cd1b6",
   "metadata": {},
   "source": [
    "## 4. Memory Management\n",
    "### Basic Memory Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda import core\n",
    "\n",
    "# Initialize CUDA context (assume already done)\n",
    "core.cuInit(0)\n",
    "device = core.cuDeviceGet(0)\n",
    "context = core.cuCtxCreate(0, device)\n",
    "\n",
    "# Create host data\n",
    "host_array = np.arange(1000, dtype=np.float32)\n",
    "print(f\"Host array size: {host_array.nbytes} bytes\")\n",
    "\n",
    "# Allocate device memory\n",
    "device_ptr = core.cuMemAlloc(host_array.nbytes)\n",
    "print(f\"Device memory allocated at: {device_ptr}\")\n",
    "\n",
    "# Copy data from host to device\n",
    "core.cuMemcpyHtoD(device_ptr, host_array)\n",
    "print(\"Data copied to device\")\n",
    "\n",
    "# Allocate memory for result\n",
    "result_ptr = core.cuMemAlloc(host_array.nbytes)\n",
    "\n",
    "# Copy data back from device to host\n",
    "result_array = np.zeros_like(host_array)\n",
    "core.cuMemcpyDtoH(result_array, device_ptr)\n",
    "print(\"Data copied back to host\")\n",
    "\n",
    "# Free device memory\n",
    "core.cuMemFree(device_ptr)\n",
    "core.cuMemFree(result_ptr)\n",
    "print(\"Device memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef81c7",
   "metadata": {},
   "source": [
    "### Memory Transfer Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93417b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_memory_patterns():\n",
    "    # Pattern 1: Host to Device\n",
    "    host_data = np.random.random(1000).astype(np.float32)\n",
    "    device_data = core.cuMemAlloc(host_data.nbytes)\n",
    "    core.cuMemcpyHtoD(device_data, host_data)\n",
    "    \n",
    "    # Pattern 2: Device to Device\n",
    "    device_copy = core.cuMemAlloc(host_data.nbytes)\n",
    "    core.cuMemcpyDtoD(device_copy, device_data, host_data.nbytes)\n",
    "    \n",
    "    # Pattern 3: Device to Host\n",
    "    result = np.zeros_like(host_data)\n",
    "    core.cuMemcpyDtoH(result, device_copy)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuMemFree(device_data)\n",
    "    core.cuMemFree(device_copy)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage\n",
    "result = demonstrate_memory_patterns()\n",
    "print(f\"Memory transfer completed. Result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86c5a",
   "metadata": {},
   "source": [
    "## 5. Kernel Compilation and Execution\n",
    "### Writing Your First Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.core.experimental import Program\n",
    "import numpy as np\n",
    "\n",
    "# CUDA C source code\n",
    "vector_add_source = \"\"\"\n",
    "extern \"C\" __global__ void vector_add(float *a, float *b, float *c, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile the kernel\n",
    "program = Program(source=vector_add_source, name=\"vector_add\")\n",
    "compiled_program = program.compile()\n",
    "kernel = compiled_program.get_function(\"vector_add\")\n",
    "\n",
    "print(\"Kernel compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc566b",
   "metadata": {},
   "source": [
    "### Executing the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_vector_add():\n",
    "    # Prepare data\n",
    "    N = 1000\n",
    "    a = np.arange(N, dtype=np.float32)\n",
    "    b = np.arange(N, dtype=np.float32)\n",
    "    c = np.zeros(N, dtype=np.float32)\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_a = core.cuMemAlloc(a.nbytes)\n",
    "    d_b = core.cuMemAlloc(b.nbytes)\n",
    "    d_c = core.cuMemAlloc(c.nbytes)\n",
    "    \n",
    "    # Copy data to device\n",
    "    core.cuMemcpyHtoD(d_a, a)\n",
    "    core.cuMemcpyHtoD(d_b, b)\n",
    "    \n",
    "    # Launch kernel\n",
    "    block_size = 256\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    \n",
    "    kernel.launch(\n",
    "        grid=(grid_size,),\n",
    "        block=(block_size,),\n",
    "        args=(d_a, d_b, d_c, np.int32(N))\n",
    "    )\n",
    "    \n",
    "    # Copy result back\n",
    "    core.cuMemcpyDtoH(c, d_c)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuMemFree(d_a)\n",
    "    core.cuMemFree(d_b)\n",
    "    core.cuMemFree(d_c)\n",
    "    \n",
    "    return c\n",
    "\n",
    "# Execute and verify\n",
    "result = execute_vector_add()\n",
    "expected = np.arange(1000, dtype=np.float32) * 2\n",
    "print(f\"Kernel execution successful: {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d06f97",
   "metadata": {},
   "source": [
    "### Advanced Kernel Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication kernel\n",
    "matmul_source = \"\"\"\n",
    "extern \"C\" __global__ void matrix_multiply(float *A, float *B, float *C, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def matrix_multiply_gpu(A, B):\n",
    "    N = A.shape[0]\n",
    "    assert A.shape == (N, N) and B.shape == (N, N)\n",
    "    \n",
    "    # Compile kernel\n",
    "    program = Program(source=matmul_source, name=\"matrix_multiply\")\n",
    "    kernel = program.compile().get_function(\"matrix_multiply\")\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_A = core.cuMemAlloc(A.nbytes)\n",
    "    d_B = core.cuMemAlloc(B.nbytes)\n",
    "    d_C = core.cuMemAlloc(A.nbytes)\n",
    "    \n",
    "    # Copy data\n",
    "    core.cuMemcpyHtoD(d_A, A)\n",
    "    core.cuMemcpyHtoD(d_B, B)\n",
    "    \n",
    "    # Launch kernel\n",
    "    block_size = 16\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    \n",
    "    kernel.launch(\n",
    "        grid=(grid_size, grid_size),\n",
    "        block=(block_size, block_size),\n",
    "        args=(d_A, d_B, d_C, np.int32(N))\n",
    "    )\n",
    "    \n",
    "    # Get result\n",
    "    C = np.zeros_like(A)\n",
    "    core.cuMemcpyDtoH(C, d_C)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuMemFree(d_A)\n",
    "    core.cuMemFree(d_B)\n",
    "    core.cuMemFree(d_C)\n",
    "    \n",
    "    return C\n",
    "\n",
    "# Test matrix multiplication\n",
    "A = np.random.random((64, 64)).astype(np.float32)\n",
    "B = np.random.random((64, 64)).astype(np.float32)\n",
    "C_gpu = matrix_multiply_gpu(A, B)\n",
    "C_cpu = np.dot(A, B)\n",
    "\n",
    "print(f\"Matrix multiplication correct: {np.allclose(C_gpu, C_cpu, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ec75a",
   "metadata": {},
   "source": [
    "## 6. Streams and Synchronization\n",
    "### Understanding Streams\n",
    "Streams allow asynchronous execution of CUDA operations. This enables overlapping computation with memory transfers for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d25de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_streams():\n",
    "    # Create stream\n",
    "    stream = core.cuStreamCreate()\n",
    "    \n",
    "    # Prepare data\n",
    "    N = 1000\n",
    "    a = np.arange(N, dtype=np.float32)\n",
    "    b = np.arange(N, dtype=np.float32)\n",
    "    c = np.zeros(N, dtype=np.float32)\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_a = core.cuMemAlloc(a.nbytes)\n",
    "    d_b = core.cuMemAlloc(b.nbytes)\n",
    "    d_c = core.cuMemAlloc(c.nbytes)\n",
    "    \n",
    "    # Asynchronous memory copy\n",
    "    core.cuMemcpyHtoDAsync(d_a, a, stream)\n",
    "    core.cuMemcpyHtoDAsync(d_b, b, stream)\n",
    "    \n",
    "    # Launch kernel in stream\n",
    "    kernel.launch(\n",
    "        grid=(4,),\n",
    "        block=(256,),\n",
    "        args=(d_a, d_b, d_c, np.int32(N)),\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    # Asynchronous copy back\n",
    "    core.cuMemcpyDtoHAsync(c, d_c, stream)\n",
    "    \n",
    "    # Wait for stream to complete\n",
    "    core.cuStreamSynchronize(stream)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuStreamDestroy(stream)\n",
    "    core.cuMemFree(d_a)\n",
    "    core.cuMemFree(d_b)\n",
    "    core.cuMemFree(d_c)\n",
    "    \n",
    "    return c\n",
    "\n",
    "result = demonstrate_streams()\n",
    "print(\"Stream execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9e031",
   "metadata": {},
   "source": [
    "### Events for Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebde116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_kernel_execution():\n",
    "    # Create events\n",
    "    start_event = core.cuEventCreate()\n",
    "    end_event = core.cuEventCreate()\n",
    "    \n",
    "    # Prepare data\n",
    "    N = 1000000\n",
    "    a = np.random.random(N).astype(np.float32)\n",
    "    b = np.random.random(N).astype(np.float32)\n",
    "    \n",
    "    d_a = core.cuMemAlloc(a.nbytes)\n",
    "    d_b = core.cuMemAlloc(b.nbytes)\n",
    "    d_c = core.cuMemAlloc(a.nbytes)\n",
    "    \n",
    "    core.cuMemcpyHtoD(d_a, a)\n",
    "    core.cuMemcpyHtoD(d_b, b)\n",
    "    \n",
    "    # Record start time\n",
    "    core.cuEventRecord(start_event)\n",
    "    \n",
    "    # Launch kernel\n",
    "    block_size = 256\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    kernel.launch(\n",
    "        grid=(grid_size,),\n",
    "        block=(block_size,),\n",
    "        args=(d_a, d_b, d_c, np.int32(N))\n",
    "    )\n",
    "    \n",
    "    # Record end time\n",
    "    core.cuEventRecord(end_event)\n",
    "    core.cuEventSynchronize(end_event)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = core.cuEventElapsedTime(start_event, end_event)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuEventDestroy(start_event)\n",
    "    core.cuEventDestroy(end_event)\n",
    "    core.cuMemFree(d_a)\n",
    "    core.cuMemFree(d_b)\n",
    "    core.cuMemFree(d_c)\n",
    "    \n",
    "    return elapsed_time\n",
    "\n",
    "execution_time = time_kernel_execution()\n",
    "print(f\"Kernel execution time: {execution_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9258bd0",
   "metadata": {},
   "source": [
    "## 7. Error Handling\n",
    "### Proper Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cuda_operation():\n",
    "    try:\n",
    "        # Initialize CUDA\n",
    "        core.cuInit(0)\n",
    "        \n",
    "        # Get device\n",
    "        device = core.cuDeviceGet(0)\n",
    "        \n",
    "        # Create context\n",
    "        context = core.cuCtxCreate(0, device)\n",
    "        \n",
    "        # Your CUDA operations here\n",
    "        print(\"CUDA operations completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CUDA error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Always cleanup\n",
    "        try:\n",
    "            core.cuCtxDestroy(context)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "safe_cuda_operation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391999d2",
   "metadata": {},
   "source": [
    "### Common Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_common_errors():\n",
    "    errors = []\n",
    "    \n",
    "    # Error 1: Invalid device\n",
    "    try:\n",
    "        invalid_device = core.cuDeviceGet(999)\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Invalid device: {e}\")\n",
    "    \n",
    "    # Error 2: Memory allocation failure\n",
    "    try:\n",
    "        # Try to allocate too much memory\n",
    "        huge_alloc = core.cuMemAlloc(2**40)  # 1TB\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Memory allocation failed: {e}\")\n",
    "    \n",
    "    # Error 3: Invalid kernel launch\n",
    "    try:\n",
    "        # Invalid grid/block dimensions\n",
    "        kernel.launch(grid=(0,), block=(0,), args=())\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Invalid kernel launch: {e}\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "errors = handle_common_errors()\n",
    "for error in errors:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4f555",
   "metadata": {},
   "source": [
    "## 8. Performance Optimization\n",
    "### Memory Coalescing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Coalesced memory access\n",
    "coalesced_kernel = \"\"\"\n",
    "extern \"C\" __global__ void coalesced_access(float *data, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        data[i] = data[i] * 2.0f;  // Sequential access\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Bad: Strided memory access\n",
    "strided_kernel = \"\"\"\n",
    "extern \"C\" __global__ void strided_access(float *data, int n, int stride) {\n",
    "    int i = (blockIdx.x * blockDim.x + threadIdx.x) * stride;\n",
    "    if (i < n) {\n",
    "        data[i] = data[i] * 2.0f;  // Strided access\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab3328",
   "metadata": {},
   "source": [
    "### Shared Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_memory_kernel = \"\"\"\n",
    "extern \"C\" __global__ void shared_memory_example(float *input, float *output, int n) {\n",
    "    extern __shared__ float shared_data[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x * blockDim.x + tid;\n",
    "    \n",
    "    // Load data into shared memory\n",
    "    if (i < n) {\n",
    "        shared_data[tid] = input[i];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Process data in shared memory\n",
    "    if (tid > 0 && tid < blockDim.x - 1 && i < n) {\n",
    "        output[i] = (shared_data[tid-1] + shared_data[tid] + shared_data[tid+1]) / 3.0f;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def use_shared_memory():\n",
    "    N = 1000\n",
    "    data = np.random.random(N).astype(np.float32)\n",
    "    result = np.zeros_like(data)\n",
    "    \n",
    "    # Compile kernel\n",
    "    program = Program(source=shared_memory_kernel, name=\"shared_memory_example\")\n",
    "    kernel = program.compile().get_function(\"shared_memory_example\")\n",
    "    \n",
    "    # Allocate memory\n",
    "    d_input = core.cuMemAlloc(data.nbytes)\n",
    "    d_output = core.cuMemAlloc(result.nbytes)\n",
    "    \n",
    "    core.cuMemcpyHtoD(d_input, data)\n",
    "    \n",
    "    # Launch with shared memory\n",
    "    block_size = 256\n",
    "    shared_mem_size = block_size * 4  # 4 bytes per float\n",
    "    \n",
    "    kernel.launch(\n",
    "        grid=((N + block_size - 1) // block_size,),\n",
    "        block=(block_size,),\n",
    "        args=(d_input, d_output, np.int32(N)),\n",
    "        shared_mem_bytes=shared_mem_size\n",
    "    )\n",
    "    \n",
    "    core.cuMemcpyDtoH(result, d_output)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuMemFree(d_input)\n",
    "    core.cuMemFree(d_output)\n",
    "    \n",
    "    return result\n",
    "\n",
    "smoothed_data = use_shared_memory()\n",
    "print(f\"Shared memory kernel executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56af27",
   "metadata": {},
   "source": [
    "## 9. Practical Examples {tag here}\n",
    "### Example 1: Image Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_kernel = \"\"\"\n",
    "extern \"C\" __global__ void convolution_2d(float *input, float *output, float *kernel, \n",
    "                                         int width, int height, int kernel_size) {\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (col < width && row < height) {\n",
    "        float sum = 0.0f;\n",
    "        int half_kernel = kernel_size / 2;\n",
    "        \n",
    "        for (int i = -half_kernel; i <= half_kernel; i++) {\n",
    "            for (int j = -half_kernel; j <= half_kernel; j++) {\n",
    "                int input_row = row + i;\n",
    "                int input_col = col + j;\n",
    "                \n",
    "                if (input_row >= 0 && input_row < height && \n",
    "                    input_col >= 0 && input_col < width) {\n",
    "                    int input_idx = input_row * width + input_col;\n",
    "                    int kernel_idx = (i + half_kernel) * kernel_size + (j + half_kernel);\n",
    "                    sum += input[input_idx] * kernel[kernel_idx];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[row * width + col] = sum;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def gpu_convolution(image, conv_kernel):\n",
    "    height, width = image.shape\n",
    "    kernel_size = conv_kernel.shape[0]\n",
    "    \n",
    "    # Flatten arrays\n",
    "    image_flat = image.flatten().astype(np.float32)\n",
    "    kernel_flat = conv_kernel.flatten().astype(np.float32)\n",
    "    output_flat = np.zeros_like(image_flat)\n",
    "    \n",
    "    # Compile kernel\n",
    "    program = Program(source=convolution_kernel, name=\"convolution_2d\")\n",
    "    kernel = program.compile().get_function(\"convolution_2d\")\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_input = core.cuMemAlloc(image_flat.nbytes)\n",
    "    d_output = core.cuMemAlloc(output_flat.nbytes)\n",
    "    d_kernel = core.cuMemAlloc(kernel_flat.nbytes)\n",
    "    \n",
    "    # Copy data\n",
    "    core.cuMemcpyHtoD(d_input, image_flat)\n",
    "    core.cuMemcpyHtoD(d_kernel, kernel_flat)\n",
    "    \n",
    "    # Launch kernel\n",
    "    block_size = 16\n",
    "    grid_x = (width + block_size - 1) // block_size\n",
    "    grid_y = (height + block_size - 1) // block_size\n",
    "    \n",
    "    kernel.launch(\n",
    "        grid=(grid_x, grid_y),\n",
    "        block=(block_size, block_size),\n",
    "        args=(d_input, d_output, d_kernel, \n",
    "              np.int32(width), np.int32(height), np.int32(kernel_size))\n",
    "    )\n",
    "    \n",
    "    # Copy result back\n",
    "    core.cuMemcpyDtoH(output_flat, d_output)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuMemFree(d_input)\n",
    "    core.cuMemFree(d_output)\n",
    "    core.cuMemFree(d_kernel)\n",
    "    \n",
    "    return output_flat.reshape(height, width)\n",
    "\n",
    "# Test convolution\n",
    "image = np.random.random((100, 100)).astype(np.float32)\n",
    "edge_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=np.float32)\n",
    "result = gpu_convolution(image, edge_kernel)\n",
    "print(f\"Convolution completed. Output shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470a44f",
   "metadata": {},
   "source": [
    "## 10. Lab\n",
    "### Exercise 1: Vector Operations\n",
    "Write a CUDA kernel that performs element-wise multiplication of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "multiply_kernel_source = \"\"\"\n",
    "// TODO: Implement vector multiplication kernel\n",
    "\"\"\"\n",
    "\n",
    "def vector_multiply(a, b):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e2001",
   "metadata": {},
   "source": [
    "### Exercise 2: Reduction Operation\n",
    "Implement a parallel reduction to find the maximum value in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8444f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "max_reduction_source = \"\"\"\n",
    "// TODO: Implement reduction kernel\n",
    "\"\"\"\n",
    "\n",
    "def find_max_gpu(arr):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb5891",
   "metadata": {},
   "source": [
    "### Exercise 3: Matrix Transpose\n",
    "Write a kernel that transposes a matrix efficiently using shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "transpose_kernel_source = \"\"\"\n",
    "// TODO: Implement matrix transpose kernel with shared memory\n",
    "\"\"\"\n",
    "\n",
    "def matrix_transpose_gpu(matrix):\n",
    "    # TODO: Implement the wrapper function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d85b09",
   "metadata": {},
   "source": [
    "### Exercise 4: Performance Comparison\n",
    "Compare the performance of your GPU implementations with their CPU counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operations():\n",
    "    # TODO: Implement benchmarking code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5c494",
   "metadata": {},
   "source": [
    "## Solutions to Exercises\n",
    "## Solution 1: Vector Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_kernel_source = \"\"\"\n",
    "extern \"C\" __global__ void vector_multiply(float *a, float *b, float *c, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] * b[i];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def vector_multiply(a, b):\n",
    "    assert len(a) == len(b), \"Vectors must have same length\"\n",
    "    n = len(a)\n",
    "    \n",
    "    # Compile kernel\n",
    "    program = Program(source=multiply_kernel_source, name=\"vector_multiply\")\n",
    "    kernel = program.compile().get_function(\"vector_multiply\")\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_a = core.cuMemAlloc(a.nbytes)\n",
    "    d_b = core.cuMemAlloc(b.nbytes)\n",
    "    d_c = core.cuMemAlloc(a.nbytes)\n",
    "    \n",
    "    # Copy data\n",
    "    core.cuMemcpyHtoD(d_a, a)\n",
    "    core.cuMemcpyHtoD(d_b, b)\n",
    "    \n",
    "    # Launch kernel\n",
    "    block_size = 256\n",
    "    grid_size = (n + block_size - 1) // block_size\n",
    "    \n",
    "    kernel.launch(\n",
    "        grid=(grid_size,),\n",
    "        block=(block_size,),\n",
    "        args=(d_a, d_b, d_c, np.int32(n))\n",
    "    )\n",
    "    \n",
    "    # Get result\n",
    "    c = np.zeros_like(a)\n",
    "    core.cuMemcpyDtoH(c, d_c)\n",
    "    \n",
    "    # Cleanup\n",
    "    core.cuMemFree(d_a)\n",
    "    core.cuMemFree(d_b)\n",
    "    core.cuMemFree(d_c)\n",
    "    \n",
    "    return c\n",
    "\n",
    "# Test\n",
    "a = np.arange(1000, dtype=np.float32)\n",
    "b = np.arange(1000, dtype=np.float32)\n",
    "result = vector_multiply(a, b)\n",
    "expected = a * b\n",
    "print(f\"Vector multiplication correct: {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f38be",
   "metadata": {},
   "source": [
    "### Best Practices Summary\n",
    "\n",
    "1. **Always initialize CUDA properly** with cuInit(0)\n",
    "2. **Manage memory carefully** - allocate, copy, free in proper order\n",
    "3. **Handle errors gracefully** - wrap CUDA calls in try-catch blocks\n",
    "4. **Use appropriate block sizes** - typically 128, 256, or 512 threads\n",
    "5. **Consider memory access patterns** - coalesced access is faster\n",
    "6. **Use shared memory** for data reuse within blocks\n",
    "7. **Profile your code** - use events for timing\n",
    "8. **Clean up resources** - always free memory and destroy contexts\n",
    "\n",
    "This tutorial provides a solid foundation for using cuda.core effectively. Remember that low-level CUDA programming requires careful attention to detail, but it offers maximum performance and flexibility for GPU computing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a743d1a",
   "metadata": {},
   "source": [
    "## Resources\n",
    "CUDA Python Reference: https://numba.pydata.org/numba-doc/dev/cuda-reference/\n",
    "\n",
    "Repository: https://github.com/NVIDIA/cuda-python "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
