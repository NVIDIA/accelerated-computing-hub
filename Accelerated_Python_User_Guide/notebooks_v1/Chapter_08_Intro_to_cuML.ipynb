{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a102bd-381c-40a7-9c01-c06e37de889f",
   "metadata": {},
   "source": [
    "# Chapter 8: Machine Learning using cuML\n",
    "\n",
    "<img src=\"images/chapter-08/rapids_logo.png\" style=\"width:600px;\"/>\n",
    "\n",
    "As part of the NVIDIA RAPIDS suite, cuML is incredibly useful for accelerating the end-to-end machine learning pipeline, from data preprocessing to model training and evaluation, utilizing the parallel processing capabilities of NVIDIA GPUs. \n",
    "\n",
    "Like other RAPIDS libraries, cuML strives to mimic the behavior of its counterpart, scikit-learn, in the Python scientific computing ecosystem.  By matching the scikit-learn API, users who are already familiar with the syntax and functionality of scikit-learn will easily be able to transition to cuML for GPU acceleration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c5f13-81f2-4bdc-a5d7-f61ee1c8f9ac",
   "metadata": {},
   "source": [
    "## cuML Basics\n",
    "\n",
    "cuML is a library that implements a suite of machine learning algorithms within an easy-to-use scikit-learn like interface but on GPUs, enabling dramatic performance improvements. \n",
    "\n",
    "### Advantages of Using cuML\n",
    "\n",
    "cuML offers several advantages that make it an attractive choice for data scientists looking to accelerate their machine learning workflows:\n",
    "- **Speed:** By leveraging GPU acceleration, cuML can significantly reduce the time required to train models and make predictions.\n",
    "- **Scalability:** Itâ€™s designed to scale from a single GPU to multi-GPU setups, enabling the processing of large datasets more efficiently.\n",
    "- **Ease of Use:** cuMLâ€™s API mirrors that of scikit-learn, making it accessible to those already familiar with the popular Python library for machine learning.\n",
    "\n",
    "\n",
    "### When to Use cuML\n",
    "If you encounter any of these scenarios, cuML offers an extraordinary advantage:\n",
    "- Large datasets slowing down your computations\n",
    "- Performance-critical machine learning applications\n",
    "- Desire to tap into the raw power of GPU processing\n",
    "\n",
    "\n",
    "### Use Cases\n",
    "- Big Data Analytics: Ideal for applications requiring the processing of large volumes of data, such as financial analysis or real-time analytics.\n",
    "- Deep Learning Preprocessing: Use cuML for preprocessing steps in machine learning workflows, significantly reducing bottlenecks when training deep learning models.\n",
    "- Time Series Forecasting: Speed up training on time series models that involve massive datasets.\n",
    "  \n",
    "\n",
    "### Short Comings of cuML\n",
    "\n",
    "- **GPU Requirement:** cuML is designed to run on NVIDIA GPUs, which means you need access to compatible hardware. For those without NVIDIA GPUs, cuML is not an option, limiting its accessibility compared to CPU-based libraries like scikit-learn.\n",
    "\n",
    "- **CUDA Dependency:** The library depends on CUDA, NVIDIAâ€™s parallel computing platform and programming model. This means users must have a compatible CUDA version installed, which can introduce compatibility issues and additional setup complexity.\n",
    "\n",
    "- **GPU Memory Limitations:** The performance and scalability of cuML algorithms are directly tied to the GPUâ€™s memory capacity. For very large datasets, this could become a bottleneck, as the entire dataset and intermediate computations need to fit into GPU memory, which is typically more limited than system RAM.\n",
    "\n",
    "- **Limited Algorithm Selection:** While cuML offers a range of commonly used machine learning algorithms, its selection is not as comprehensive as scikit-learnâ€™s. Certain niche or very new algorithms might not be available, which could be a limitation for some projects.\n",
    "\n",
    "- **Scaling Challenges:** While cuML supports multi-GPU configurations for some algorithms, scaling out to multiple GPUs can introduce additional complexity in terms of setup and code. Managing data distribution and aggregation across GPUs can be challenging, particularly for algorithms that are not inherently designed for distributed computing.\n",
    "\n",
    "- **Integration with Other Libraries:** Data scientists often use a wide range of tools and libraries in their workflow. cuMLâ€™s integration with other Python libraries is generally good, especially within the RAPIDS ecosystem, but there can be challenges when integrating with libraries that are not GPU-aware, requiring additional data transfers between CPU and GPU memory.\n",
    "\n",
    "- **Ecosystem Compatibility:** Projects deeply integrated with other machine learning and data processing frameworks may encounter challenges incorporating cuML, especially if those frameworks do not natively support GPU acceleration or have specific dependencies on CPU-based algorithms.\n",
    "\n",
    "- **Familiarity with GPU Computing:** To fully leverage cuML and troubleshoot any issues that arise, users may need a basic understanding of GPU computing principles, which can be a learning curve for those only familiar with CPU-based computing.\n",
    "\n",
    "- **Documentation and Community Support:** While the RAPIDS ecosystem is growing, the documentation and community support for cuML might not be as extensive or mature as for more established libraries like scikit-learn. This can make solving specific problems or understanding advanced features more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc7c4a",
   "metadata": {},
   "source": [
    "## Advanced Features of cuML\n",
    "\n",
    "### Multi-GPU Support \n",
    "\n",
    "cuML supports multi-GPU setups, allowing you to scale your computations further. This is especially useful for extremely large datasets or complex models that benefit from distributed processing.\n",
    "\n",
    "### Integration wth Other RAPIDS Libraries \n",
    "\n",
    "cuML integrates well with other RAPIDS libraries like cuDF (for data manipulation), cuGraph (for graph analytics), and cuSpatial (for spatial data). This synergy allows you to build comprehensive data science workflows entirely on the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad62b3-1b6b-4cc4-a0a6-fdf22a73660e",
   "metadata": {},
   "source": [
    "## Links to Handy References\n",
    "\n",
    "cuML Documentation: https://docs.rapids.ai/api/cuml/stable/\n",
    "\n",
    "cuML API Reference: https://docs.rapids.ai/api/cuml/stable/api/ \n",
    "\n",
    "Scikit-learn Documentation: https://scikit-learn.org/stable/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2163b-aa0f-4b9c-aa34-b846d87e4b58",
   "metadata": {},
   "source": [
    "<img src=\"images/chapter-08/nvidia-cuda-ml.jpg\" style=\"width:800px;\"/>\n",
    "\n",
    "# Coding Guide\n",
    "\n",
    "### Installation \n",
    "Please use the cuDF RAPIDS Installation Guide for intallation instructions appropriate to your hardware and Python environment: https://docs.rapids.ai/install/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dd711-7df3-43a4-b09a-c62f5e428750",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "## Create a simple DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a9f35-768f-4557-a811-cbad4f881f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "# Creating a random DataFrame\n",
    "data = cudf.DataFrame({\n",
    "    'x1': np.random.rand(1000),\n",
    "    'x2': np.random.rand(1000),\n",
    "    'y': np.random.randint(0, 2, size=1000)\n",
    "})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d686ad-e50c-4cc3-b0cd-d15645880ec3",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Challenge: modify the number of rows in the DataFrame in the frame above and observe how it changes the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fa0e1-1799-4042-8912-4b641c56c2ff",
   "metadata": {},
   "source": [
    "## Train a Machine Learning Model - Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b010ca2-0d92-4022-9d8c-df2a8ee71113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.linear_model import LogisticRegression\n",
    "from cuml.model_selection import train_test_split\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X = data[['x1', 'x2']]\n",
    "y = data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae215da-fba6-4bf9-bd81-cc963c97544d",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Challenge: Try different parameters, different solvers or adding regularization. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8228e",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance Using Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c15d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics.accuracy import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581af87",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3baa2",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Mean Cross-Validation Score: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f49675-ffd6-40f5-82bc-9414c9226abc",
   "metadata": {},
   "source": [
    "## Comparing GPU & CPU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82220ef3-dede-4873-973a-9fd591eb236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# Timing the GPU model training\n",
    "start_time = time.time()\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "gpu_time = time.time() - start_time\n",
    "print(f\"GPU Training Time: {gpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df91585-6ff0-40f7-a86d-ca91cc473e97",
   "metadata": {},
   "source": [
    "Model fit on the CPU instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68adf5d-e293-442c-b9db-38bd31002961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Create a large random DataFrame using pandas\n",
    "data_pd = pd.DataFrame({\n",
    "    'x1': np.random.rand(1000000),\n",
    "    'x2': np.random.rand(1000000),\n",
    "    'y': np.random.randint(0, 2, size=1000000)\n",
    "})\n",
    "\n",
    "X_pd = data_pd[['x1', 'x2']]\n",
    "y_pd = data_pd['y']\n",
    "X_train_pd, X_test_pd, y_train_pd, y_test_pd = train_test_split(X_pd, y_pd, test_size=0.2)\n",
    "\n",
    "\n",
    "# Timing the CPU model training\n",
    "start_time = time.time()\n",
    "cpu_model = SklearnLogisticRegression()\n",
    "cpu_model.fit(X_train_pd, y_train_pd)\n",
    "\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"CPU Training Time: {cpu_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95927351-bcd1-4112-9623-575805242562",
   "metadata": {},
   "source": [
    "**NOTE:** Although on surface level the code looked almost identical, the model using cuML was almost 100x as fast as the one using just scikit-learn. cuML on a GPU can significantly outperform traditional CPU-based machine learning libraries, especially with large datasets. The time savings become more pronounced as the data size increases, showcasing the advantages of leveraging GPU acceleration for machine learning tasks.\n",
    "\n",
    "### ðŸ’¡ Challenge: if you have a specific dataset or model in mind, you could run the above examples to see the time differences firsthand!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389da4b",
   "metadata": {},
   "source": [
    "## Saving & Loading Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872d2f0-b82f-46a0-8b66-02ff3f9d2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model using cuML's joblib.\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5b4c3",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def886fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Using K-Means Clustering\n",
    "# Let's demonstrate K-Means clustering using cuML.\n",
    "\n",
    "from cuml.cluster import KMeans\n",
    "from cupy import cp \n",
    "\n",
    "# Generate synthetic data for clustering\n",
    "X_clustering = cp.random.rand(10000, 2)  # 10,000 samples, 2 features\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X_clustering)\n",
    "\n",
    "# Predict cluster labels\n",
    "labels = kmeans.predict(X_clustering)\n",
    "\n",
    "# Display the first few labels\n",
    "print(labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e8620-eab5-4296-bb52-3828f56d8d87",
   "metadata": {},
   "source": [
    "## PCA for Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546a704-035f-4086-ba7f-3c315cac85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š PCA for Dimensionality Reduction\n",
    "# Perform PCA on the dataset for dimensionality reduction.\n",
    "\n",
    "from cuml.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"PCA Transformed Shape: {X_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b639bca-7c5b-4a4a-a3eb-815861c2ea22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
