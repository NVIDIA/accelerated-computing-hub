{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e1d2ba-2981-44db-a02c-8a1e68fe5725",
   "metadata": {},
   "source": [
    "# Chapter 9: Developer Tools\n",
    "\n",
    "## CUDA Python Correctness\n",
    "\n",
    "CUDA code can sometimes introduce various errors that are not detected by the compiler, such as \n",
    "- Memory access violations\n",
    "- Memory leaks\n",
    "- Data race conditions\n",
    "- Incorrect API usage\n",
    "\n",
    "These errors can lead to incorrect program behavior, crashes, or performance degradation. [Compute Sanitizer](https://developer.nvidia.com/compute-sanitizer) is a suite of runtime error detection tools provided by NVIDIA to help developers identify and debug such issues in CUDA applications.\n",
    "\n",
    "## CUDA Python Performance\n",
    "\n",
    "In order to achieve optimal performance in CUDA, you must consider several factors:\n",
    "- Localizing memory access in order to minimize memory latency.\n",
    "- Maximizing the number of active threads per multiprocessor to ensure high utilization of your hardware.\n",
    "- Minimization of conditional branching.\n",
    "\n",
    "In order to overcome the bottleneck between CPU and GPU across the PCIe bus, we want to:\n",
    "- Minimize the volume of data transferred.  Transferring data in large batches can minimize the number of data transfer operations.\n",
    "- Organize data in a way that complements the hardware architecture.\n",
    "- Utilize asynchronous transfer features that will allow computation and data transfer to occur simultaneously.  Overlapping data transfers with computation can hide latencies caused by data transfers.\n",
    "\n",
    "[Nsight Systems](https://developer.nvidia.com/nsight-systems) and [Nsight Compute](https://developer.nvidia.com/nsight-compute) are the tools used to detect the bottlenecks and performance flaws in Cuda code.\n",
    "\n",
    "## Common Pitfalls\n",
    "The most common mistake is running a CPU-only code on a GPU node. Only codes that have been explicitly written to run on a GPU can take advantage of a GPU. Ensure your codes are using the correct GPU accelerated libraries, drivers, and hardware.\n",
    "\n",
    "**Zero GPU Utilization**\n",
    "Check to make sure your software is GPU enabled.  Only codes that have been explicitly written to use GPUs can take advantage of them.\n",
    "Make sure your software environment is properly configured. In some cases certain libraries must be available for your code to run on GPUs. Check your dependencies, version of CUDA Toolkit, and your software environment requirements.\n",
    " \n",
    "**Low GPU Utilization** (e.g. less than ~15%)\n",
    "Using more GPUs than necessary.  You can find the optimal number of GPUs and CPU-cores by performing a scaling analysis.\n",
    "Check your process’s throughput.  If you are writing output to slow memory, making unnecessary copies, or switching between your CPU and GPU, you may see low utilization.\n",
    "\n",
    "**Memory Errors**\n",
    "Access Violation Errors.  Reading or writing to memory locations that are not allowed or permitted can result in unpredictable behavior and system crashes.\n",
    "Memory Leaks.  When memory is allocated but not correctly deallocated, the application will consume GPU memory resources, but not utilize them.  The allocated memory will not be available for further computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e39da-3c98-40c7-98d4-0494115ab2ef",
   "metadata": {},
   "source": [
    "# Getting Started with Developer Tools for CUDA Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618f7db-5a8d-42a6-b666-19c83234fe68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "This steps in this document assume the user has an environment capable of running CuPy and Numba code on a GPU. See those resepective projects to set them up.\n",
    "\n",
    "- [Nsight Systems](https://developer.nvidia.com/nsight-systems) (also available in the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit))\n",
    "- [Nsight Compute](https://developer.nvidia.com/nsight-compute) (also available in the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit))\n",
    "- [Compute Sanitizer](https://developer.nvidia.com/compute-sanitizer) (also available in the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit))\n",
    "- [nvtx Python bindings](https://pypi.org/project/nvtx/)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476852d1-6373-432f-943a-3593a056838b",
   "metadata": {},
   "source": [
    "## Profiling with Nsight Systems\n",
    "\n",
    "[Nsight Systems](https://developer.nvidia.com/nsight-systems) is a platform profiling tool designed to give users a high-level, time-correlated view of the performance activity of their entire platform. This includes CPU, GPU, Memory, Networking, OS and application-level metrics. It helps identify the largest opportunities to optimize, and tune to scale efficiently across all available resources. This tutorial will only scratch the surface of what Nsight Systems is capable of. For full details see the [documentation](https://docs.nvidia.com/nsight-systems/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a956fc6-6066-475d-94df-bb598fad05c7",
   "metadata": {},
   "source": [
    "## Setting up a profile with the Nsight Systems GUI\n",
    "\n",
    "After opening the Nsight Systems GUI, select the target machine for profiling. This can be the local machine or a remote server. This example uses the local target. To profile a Python workload with Nsight Systems, set the “Command line with arguments:” field to point to the Python interpreter and the Python file to run including any arguments. Make sure the Python executable is in an environment with all the necessary dependencies for the application. For example: “C:\\Users\\myusername\\AppData\\Local\\miniconda3\\python.exe C:\\Users\\myusername\\cupyTests\\cupyProfilingStep1.py \\<args if needed\\>\"\n",
    "\n",
    "Also fill in the “Working directory” where the Python executable should run. \n",
    "\n",
    "**Recommended settings/flags**\n",
    "\n",
    "A good initial set of flags for profiling Python include:\n",
    "- Collect CPU context switch trace\n",
    "- Collect CUDA trace\n",
    "- Collect GPU metrics\n",
    "- Python profiling options:\n",
    "  - Collect Python backtrace samples\n",
    "\n",
    "You can learn more about all the options [here](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#profiling-from-the-gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df2d7a-1feb-41ea-b136-65729a8e5c1a",
   "metadata": {},
   "source": [
    "## Checking CUDA Python correctness with Compute Sanitizer\n",
    "\n",
    "Compute sanitizer is suite of command line tools used for detection of code correctness. Available tools are:\n",
    "- **Memcheck** Detects memory access errors, such as out-of-bounds accesses and misaligned memory accesses\n",
    "- **Racecheck** Identifies potential data races in shared memory, which can cause nondeterministic behavior.\n",
    "- **Initcheck** Finds uninitialized memory accesses that might lead to undefined behavior.\n",
    "- **Synccheck** Detects invalid synchronization patterns that could lead to deadlocks or race conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07940860-6db2-4afa-a8ee-13ab48255e7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CuPy Profiling Example\n",
    "\n",
    "In this example, we create two CuPy arrays. Then sort one of them and take the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f4487-5f04-454e-b6f6-1245d94686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cupy as cp\n",
    "\n",
    "\n",
    "def create_array(x, y) :\n",
    "    return cp.random.random((x, y),dtype=cp.float32)\n",
    "\n",
    "def sort_array(a) :\n",
    "    return cp.sort(a)\n",
    "\n",
    "def run_program() :\n",
    "    print(\"init step...\")\n",
    "    arr1 = create_array(10_000, 10_000)\n",
    "    arr2 = create_array(10_000, 10_000)\n",
    "\n",
    "    print(\"sort step...\")\n",
    "    arr1 = sort_array(arr1)\n",
    "\n",
    "    print(\"dot step...\")\n",
    "    arr3 = cp.dot(arr1, arr2)\n",
    "    \n",
    "    print(\"done\")\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    run_program()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54919a18-ee89-475f-abef-98bc4a5e64b2",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1 - Profiling a CuPy workload**\n",
    "\n",
    "First, run an initial profile of this CuPy sample using the setup and flags described above. If launching a profile for the GUI is not an option, a profile can also be launched from the command line. An example CLI command to run this analysis is below. Some flags may vary depending on your specific setup.\n",
    "\n",
    "*nsys profile --gpu-metrics-device=all --python-sampling=true --python-sampling-frequency=1000 --trace=cuda --cpuctxsw=process-tree python \"/home/myusername/cupytest1.py\"*\n",
    "\n",
    "\n",
    "Once the profile completes, find the Python process thread under the **Processes** row on the timeline. Zoom in to the active portion of the Python thread by left-clicking and dragging across the area of interest to select it. Then right-click to \"Zoom into selection\". If you hover over a sample in the **Python Backtrace** row, a popup will appear with the call stack that was currently executing when the sample was taken.\n",
    "\n",
    "![cupy1](images/chapter-09/cupy-profiling-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02262227-93ae-4a13-965a-b3cd5e32d10b",
   "metadata": {},
   "source": [
    "CuPy will call CUDA kernels under the hood as it executes. Nsight Systems will automatically detect these. Expand the **CUDA HW** row to see where the kernels are scheduled.\n",
    "\n",
    "![cupy2](images/chapter-09/cupy-profiling-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a8420-9ed3-40a9-bc12-53f7deb86928",
   "metadata": {},
   "source": [
    "Look at the **GPU Metrics > GPU Active** and **SM Instructions** rows to verify that the GPU is being used. You can hover over a spot in this row to see the % Utilization.\n",
    "\n",
    "![cupy3](images/chapter-09/cupy-profiling-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416a9a1-6758-41bf-abae-06b8fa0a2be3",
   "metadata": {},
   "source": [
    "**Step 2 - Adding nvtx**\n",
    "\n",
    "Nsight Systems can automatically detect CUDA kernels as well as APIs from many other frameworks or libraries. Additionally, the [nvtx](https://github.com/NVIDIA/NVTX) annotation module gives users the ability to markup their own applications to see personalized trace events and ranges on the timeline. The [nvtx Python module](https://pypi.org/project/nvtx/) is available through pip and can be installed with the command:\n",
    "\n",
    "*pip install nvtx*\n",
    "\n",
    "The code below adds nvtx to the CuPy application, with colored ranges defined around various phases of the workload. Run a profile of this new version to see nvtx on the timeline. If using the CLI, update the flag to \"--trace=nvtx,cuda\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629cb5c6-3c72-4658-b3cb-ea9352c89ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init step...\n",
      "sort step...\n",
      "dot step...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cupy as cp\n",
    "import nvtx\n",
    "\n",
    "def create_array(x, y) :\n",
    "    return cp.random.random((x, y),dtype=cp.float32)\n",
    "\n",
    "def sort_array(a) :\n",
    "    return cp.sort(a)\n",
    "\n",
    "def run_program() :\n",
    "    print(\"init step...\")\n",
    "    nvtx.push_range(\"init_step\", color='green')\n",
    "    arr1 = create_array(10_000, 10_000)\n",
    "    arr2 = create_array(10_000, 10_000)\n",
    "    nvtx.pop_range()\n",
    "\n",
    "    print(\"sort step...\")\n",
    "    nvtx.push_range(\"sort_step\", color='yellow')\n",
    "    arr1 = sort_array(arr1)\n",
    "    nvtx.pop_range()\n",
    "\n",
    "    nvtx.push_range(\"dot_step\", color='magenta')\n",
    "    print(\"dot step...\")\n",
    "    arr3 = cp.dot(arr1, arr2)\n",
    "    nvtx.pop_range()\n",
    "    \n",
    "    print(\"done\")\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    nvtx.push_range(\"run_program\", color='white')\n",
    "    run_program()\n",
    "    nvtx.pop_range()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ce1c0-48f7-4c7d-a33a-68aeb9a4e88e",
   "metadata": {},
   "source": [
    "The **NVTX** row for the CPU thread of the Python process shows when the CPU is inside one of these ranges. The **NVTX** row under the CUDA HW section shows when these ranges are active on the GPU. Notice that they are not exactly lined up because of GPU execution scheduling. You can also see how the CUDA kernels map to these various nvtx ranges that represent the phases of our workload.\n",
    "\n",
    "In this particular example, we can see in the **GPU Metrics > SM Instructions > Tensor Active** row that the Tensor cores on the GPU are not active while the kernels are running. Tensor cores can add a lot of performance to computation-intensive kernels. The next step will be to get them active. \n",
    "\n",
    "![cupy4](images/chapter-09/cupy-profiling-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0f27b-10e2-461e-a7c3-aefcd8c515f4",
   "metadata": {},
   "source": [
    "**Step 3 - Enabling Tensor cores** \n",
    "\n",
    "The [CuPy documentation](https://docs.cupy.dev/en/stable/reference/environment.html#envvar-CUPY_TF32) describes how to enable Tensor cores with an environment variable. <file> adds the following line:\n",
    "- os.environ[\"CUPY_TF32\"] = \"1\"\n",
    "\n",
    "Run another Nsight Systems profile to see the activity of the Tensor cores with this version.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfd753-3f4d-4152-a580-7608dd5e14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cupy as cp\n",
    "import nvtx\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def create_array(x, y) :\n",
    "    return cp.random.random((x, y),dtype=cp.float32)\n",
    "\n",
    "def sort_array(a) :\n",
    "    return cp.sort(a)\n",
    "\n",
    "def run_program() :\n",
    "    print(\"init step...\")\n",
    "    nvtx.push_range(\"init_step\", color='green')\n",
    "    arr1 = create_array(10_000, 10_000)\n",
    "    arr2 = create_array(10_000, 10_000)\n",
    "    nvtx.pop_range()\n",
    "\n",
    "    print(\"sort step...\")\n",
    "    nvtx.push_range(\"sort_step\", color='yellow')\n",
    "    arr1 = sort_array(arr1)\n",
    "    nvtx.pop_range()\n",
    "\n",
    "    nvtx.push_range(\"dot_step\", color='magenta')\n",
    "    print(\"dot step...\")\n",
    "    arr3 = cp.dot(arr1, arr2)\n",
    "    nvtx.pop_range()\n",
    "    \n",
    "    print(\"done\")\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ[\"CUPY_TF32\"] = \"1\"\n",
    "    nvtx.push_range(\"run_program\", color='white')\n",
    "    run_program()\n",
    "    nvtx.pop_range()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad24b03-9d44-455c-a2c2-9e65fcd6749a",
   "metadata": {},
   "source": [
    "![cupy5](images/chapter-09/cupy-profiling-5.png)\n",
    "\n",
    "**Notice** that the tensor cores are now being used during the dot product and the runtime of the dot range on the GPU is shorter 312ms ->116ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ddda9-e5c5-4de8-8790-f228139421f7",
   "metadata": {},
   "source": [
    "**Step 4 - Using an Annotation File** \n",
    "Nsight Systems can also automatically trace specific functions from Python modules, in this case CuPy, with an annotation file. This example points to the file “cupy_annotations.json” which contains:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"_comment\": \"CuPy Annotations\",\n",
    "        \n",
    "        \"module\": \"cupy\",\n",
    "   \"color\": \"black\",\n",
    "        \"functions\": [\"random.random\",\"dot\",\"sort\"]\n",
    "    }\n",
    "\n",
    "]\n",
    "```\n",
    "This json object indicates that the functions “random.random”, “dot”, and, “sort” from the module “cupy” should be traced and displayed as a black range on the timeline. Add this file to the “Python Functions trace” field in the configuration as shown below. \n",
    "\n",
    "![cupy6](images/chapter-09/cupy-profiling-6.png)\n",
    "\n",
    "To do this from the CLI, add a flag like \" --python-functions-trace=\"/home/myusername/cupy_annotations.json\" \"\n",
    "Run another profile to see the automatic tracing.\n",
    "\n",
    "![cupy7](images/chapter-09/cupy-profiling-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390c8c8-b628-4843-9cec-83fafa37e557",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Numba Profiling Example\n",
    "\n",
    "While Nsight Systems shows platform-wide profile information and some GPU-specific data, like GPU metrics, it does not dive deep into the GPU kernels themselves. That’s where [Nsight Compute](https://developer.nvidia.com/nsight-compute) comes in. Nsight Compute does detailed performance analysis of kernels as they run on the GPU. Historically, these have been written in native languages like C, but new technologies like Numba are enabling Python developers to write kernels as well. This section will describe how to profile Numba kernels with Nsight Compute. For details on Nsight Compute, check out the [Nsight Compute Documentation](https://docs.nvidia.com/nsight-compute/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7a1bd-8e18-4feb-a914-6edb197bf837",
   "metadata": {},
   "source": [
    "**Setting up a profile with the Nsight Compute GUI**\n",
    "\n",
    "To profile a Numba application with Nsight Compute, open the “Connect” dialog from the GUI. Select the python interpreter binary as the “Application Executable”. Ensure this interpreter runs in the environment with all the necessary dependencies for the application, for example the Conda shell supporting Numba. Then fill in the “Working Directory” field and put your Python file and any additional command line arguments in the “Command Line Arguments” field. This tells Nsight Compute how to launch your workload for profiling.\n",
    "\n",
    "![numba1](images/chapter-09/numba-profiling-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a738d9d-be5f-4401-ae07-f434de507ec5",
   "metadata": {},
   "source": [
    "**Recommended settings/flags**\n",
    "\n",
    "Nsight Compute has a lot of options to configure your profile. This guide isn’t designed to cover all of them, but there is a lot of additional information in the [documentation](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#command-line-options). A good starting point for Numba profiling is to choose the **Profile** activity. In the **Filter > Kernel Base Name dropdown select “Demangled”**. In the **Other > Enable CPU Call Stack** select Yes and **Other > CPU Call Stack Types** select All or Python.\n",
    "\n",
    "The **Metrics** tab is where you will choose what performance metrics to collect. The metrics are grouped into sets, and the detailed set is a good starting point. You can learn more about the metrics in the [kernel profiling guide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html). After updating these settings, click **Launch** to start the automatic profiling process. Nsight Compute will profile each kernel it encounters via a multi-pass replay mechanism and will report the results once complete. If profiling from the GUI is not an option, you can configure a profile from the GUI and copy the approprite command from the \"Command Line:\" in the **Common** tab. An example command for this profile might be:\n",
    "\n",
    "*ncu --config-file off --export \"\\home\\myusername\\r%i\" --force-overwrite --launch-count 3 --set detailed --call-stack --call-stack-type native --call-stack-type python --nvtx --import-source yes \\home\\myusername\\numbaTest1.py*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c38613-a275-408e-9222-cc9f86b5e2fd",
   "metadata": {},
   "source": [
    "### Sample Nsight Compute Profile Walkthrough\n",
    "\n",
    "In this simple example, there is a Numba kernel doing vector addition. It takes in three vectors, adds two together, and returns the sum in the third vector. Notice that the \"@cuda.jit\" decorator has the parameter “(lineinfo=True)”. This is important for resolving kernel performance data to lines of source code. With the setup described above, launch a profile to see the performance of the kernel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8e2dd-242c-437c-9d23-c6a2309ed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit(lineinfo=True)\n",
    "def vecadd(a, b, c):\n",
    "    tid = cuda.grid(1)\n",
    "    size = len(c)\n",
    "    if tid < size:\n",
    "        c[tid] = a[tid] + b[tid]\n",
    "\n",
    "def run_program() :\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "\n",
    "    N = 500000\n",
    "\n",
    "\n",
    "    a = cuda.to_device(np.random.random(N))\n",
    "    b = cuda.to_device(np.random.random(N))\n",
    "    #a = cuda.to_device(np.float32(np.random.random(N)))\n",
    "    #b = cuda.to_device(np.float32(np.random.random(N)))\n",
    "    c = cuda.device_array_like(a)\n",
    "\n",
    "\n",
    "    vecadd.forall(len(a))(a, b, c)\n",
    "    print(c.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da495c9-a287-4458-b11e-c941bb46fb46",
   "metadata": {},
   "source": [
    "When the profile completes, the **Summary** page shows an overview of the kernels profiled. In this example, it’s only one. Expanding the “Demangled Name” column shows that this is the “vecadd” kernel that we wrote with Numba. The Summary has some basic information including the kernel duration and compute and memory throughput. It also lists top performance rules that were triggered and estimated speedups for correcting them. \n",
    "\n",
    "![numba2](images/chapter-09/numba-profiling-2.png)\n",
    "\n",
    "Double clicking on the kernel will open the **Details** page with much more information.\n",
    "\n",
    "The “GPU Speed of Light Throughput” section at the top shows that this kernel has much higher Memory usage than Compute. The Memory Workload Analysis section shows significant traffic to device memory. \n",
    "\n",
    "![numba3](images/chapter-09/numba-profiling-3.png)\n",
    "\n",
    "The Compute Workload Analysis section shows the majority of the compute is using the FP64 pipeline. \n",
    "\n",
    "![numba4](images/chapter-09/numba-profiling-4.png)\n",
    "\n",
    "The Source Counters section at the bottom shows the source locations with the most stalls and clicking on one opens the **Source** page. \n",
    "\n",
    "![numba5](images/chapter-09/numba-profiling-5.png)\n",
    "\n",
    "Since this was a very simple kernel, most of the stalls are on the addition statement, but with more complex kernels, this level of detail is invaluable. Additionally, the **Context** page will show the CPU call stack that led to this kernel being executed. \n",
    "\n",
    "![numba6](images/chapter-09/numba-profiling-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500f841-445b-4d4d-8cb8-23d55fd626a2",
   "metadata": {},
   "source": [
    "For this example, we did not specify the data type in Numpy which defaulted to FP64. This caused an increase in memory traffic that was unintended. To manually switch to using the FP32 datatype switch these lines:\n",
    "    \n",
    "    a = cuda.to_device(np.random.random(N))\n",
    "    b = cuda.to_device(np.random.random(N))\n",
    "    \n",
    "to this:\n",
    "\n",
    "    a = cuda.to_device(np.float32(np.random.random(N)))\n",
    "    b = cuda.to_device(np.float32(np.random.random(N)))\n",
    "\n",
    "After switching to the FP32 datatype and rerunning a profile, we can see that the runtime of the kernel decreased significantly as did the memory traffic. Setting the initial result to the [Baseline](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#id7) and opening up the new result will automatically compare the two. Notice that the FP64 usage has disapperared and the kernel has sped up from 59us to 33us. \n",
    "\n",
    "![Img7](images/chapter-09/numba-profiling-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee29d2-f52c-45d0-a3bc-e021a8bb3ad1",
   "metadata": {},
   "source": [
    "Nsight Compute has an abundance of performance data and built-in expertise. Each section on the Details page has detailed information for a particular category of metrics including Guided Analysis rules and descriptions. The best way to learn about all these features is to try it out on your workload and use the documentation and collateral to assist.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
