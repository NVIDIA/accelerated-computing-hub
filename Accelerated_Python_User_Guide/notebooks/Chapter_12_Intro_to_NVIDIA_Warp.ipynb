{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Introduction to NVIDIA Warp\n",
    "\n",
    "## Overview\n",
    "\n",
    "[NVIDIA Warp](https://github.com/NVIDIA/warp) is an open-source Python developer framework purpose-built for developing high-performance simulation and AI workloads.\n",
    "\n",
    "Warp offers coders a clear and expressive programming model to write GPU-accelerated, kernel-based programs for simulation AI, robotics, and machine learning (ML).\n",
    "\n",
    "Some of the main features of Warp include:\n",
    "\n",
    "* **Performance** on-par with native CUDA C++ code through a combination of **just-in-time** (JIT) compilation, **CUDA-X** libraries integration, and transparent kernel fusion\n",
    "* **Ease of use** through a high-level programming model in Python, built-in data structures and algorithms for spatial computing, and support for tile-based programming\n",
    "* **Support for advanced simulation and AI workloads** through **automatic differentiation** and interoperability with other ML and accelerated Python frameworks\n",
    "\n",
    "This notebook provides readers with an overview of the main features of the library.\n",
    "\n",
    "Topics covered:\n",
    "\n",
    "* Warp fundamentals: Data model, execution model, basic syntax\n",
    "* Authoring basic kernels in Warp\n",
    "* Using Warp efficiently with other Python-based frameworks such as NumPy, PyTorch, and Jax\n",
    "* Automatic differentiation in Warp\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "Warp is a framework from NVIDIA for writing high-performance simulation and graphics code in Python. Central to Warp is a kernel-based programming model in which Python functions are just-in-time (JIT) compiled into efficient code that can run on CPUs and NVIDIA GPUs using C++/CUDA as an intermediate representation. Warp also features a **reverse-mode automatic differentiation** system, which allows researchers to write differentiable simulators that can optionally be incorporated into machine-learning pipelines to train neural networks using PyTorch or JAX. Researchers have applied Warp in areas like physics simulation, perception, robotics, and geometry processing.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "Warp is a lightweight library whose only required dependency is [NumPy](https://numpy.org/). In contrast to many other accelerated Python libraries, Warp comes prepackaged with the necessary compilers to compile code for the CPU or GPU rather than requiring\n",
    "additional libraries to be installed in the development environment.\n",
    "\n",
    "Most users install Warp from the Python Package Index (PyPI), where it is available as [warp-lang](https://pypi.org/project/warp-lang/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of Warp from PyPI\n",
    "!pip install warp-lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the `warp-lang` package, we can `import warp` in a Python script to begin using it.\n",
    "\n",
    "Typically, the import alias `wp` is used for Warp.\n",
    "\n",
    "We will explicitly initialize Warp using `wp.init()` to ensure\n",
    "that an NVIDIA GPU is detected in the notebook environment since the rest of the notebook assumes that at least one is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warp as wp\n",
    "\n",
    "wp.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell should list `\"cpu\"` and `\"cuda:0\"` devices under the `Devices:` section.\n",
    "\n",
    "The `wp.init()` call is not required to be called in users script. It will be implicitly be called the first time a function that requires Warp to be initialized is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional ways to obtain a pre-built Warp installation\n",
    "\n",
    "Community-maintained Conda packages for Warp are available on the [conda-forge](https://anaconda.org/conda-forge/warp-lang) channel:\n",
    "\n",
    "```\n",
    "# Install warp-lang specifically built against CUDA Toolkit 12.6\n",
    "$ conda install conda-forge::warp-lang=*=*cuda126*\n",
    "\n",
    "# Install warp-lang specifically built against CUDA Toolkit 11.8\n",
    "$ conda install conda-forge::warp-lang=*=*cuda118*\n",
    "```\n",
    "\n",
    "Bleeding-edge nightly packages are published on the NVIDIA Python Package Index. This can be a way to get the latest features from the `main` branch prior to a new release, but these packages have not been tested as thoroughly as the releases published on PyPI:\n",
    "\n",
    "```\n",
    "$ pip install -U --pre warp-lang --extra-index-url=https://pypi.nvidia.com/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing additional dependencies for this notebook\n",
    "\n",
    "This introductory notebook makes use of some other Python packages. Before proceeding, please ensure that these dependencies are installed in the Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A particle-simulation example in Warp\n",
    "\n",
    "We will begin by looking at a basic Warp program that solves for the motion of a group of particles\n",
    "under the influence of gravity $g$ and non-gravitational forces $f_n$.\n",
    "\n",
    "This example is meant to give us an overall sense what a Warp program looks like.\n",
    "In subsequent sections, we will look at the basic concepts in more detail.\n",
    "\n",
    "The update equations are:\n",
    "\n",
    "\\begin{align*}\n",
    "a_{n} &=  f_n / m + g \\\\\n",
    "v_{n+1} &= v_n + a_n \\Delta t \\\\\n",
    "x_{n+1} &= x_n + v_{n+1} \\Delta t.\n",
    "\\end{align*}\n",
    "\n",
    "We will assume a simple drag force $f_n = -b v_n$.\n",
    "\n",
    "The particle positions and velocities will be initialized to random values.\n",
    "\n",
    "The program then updates the positions and velocities of each particle for each time step of size $\\Delta t$ for 100 steps using semi-implicit Euler integration.\n",
    "At the end of the program, the final positions of the particles are printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "num_particles = 10_000_000  # Number of particles\n",
    "num_steps = 100\n",
    "\n",
    "mass = 0.1  # Mass per particle [kg]\n",
    "g = 9.81  # Gravitational acceleration [m/s^2]\n",
    "b = 0.05  # Drag Coefficient [kg/s]\n",
    "\n",
    "dt = 0.01 * (2 * mass / b)\n",
    "\n",
    "gravity = wp.vec3([0.0, 0.0, -g])\n",
    "\n",
    "# Initial positions: random values between -1.0 and 1.0 for x, y, and z\n",
    "rng = np.random.default_rng(12345)\n",
    "positions_np = rng.uniform(low=-1.0, high=1.0, size=(num_particles, 3))\n",
    "positions = wp.array(positions_np, dtype=wp.vec3)\n",
    "\n",
    "# Initial velocities: random values between -0.5 and 0.5 m/s for vx, vy, and vz\n",
    "velocities_np = rng.uniform(low=-0.5, high=0.5, size=(num_particles, 3))\n",
    "velocities = wp.array(velocities_np, dtype=wp.vec3)\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def integrate(positions: wp.array(dtype=wp.vec3), velocities: wp.array(dtype=wp.vec3)):\n",
    "    i = wp.tid()\n",
    "\n",
    "    acceleration = (-b * velocities[i]) / mass + gravity\n",
    "    velocities[i] += acceleration * dt\n",
    "    positions[i] += velocities[i] * dt\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "    wp.launch(integrate, dim=(num_particles,), inputs=[positions, velocities])\n",
    "\n",
    "print(f\"Final positions: {positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the main sections of the program in more detail.\n",
    "\n",
    "### Defining program constants\n",
    "\n",
    "The first section of code defines some constants for the program:\n",
    "\n",
    "```python\n",
    "num_particles = 10_000_000  # Number of particles\n",
    "num_steps = 100\n",
    "\n",
    "mass = 0.1  # Mass per particle [kg]\n",
    "g = 9.81  # Gravitational acceleration [m/s^2]\n",
    "b = 0.05  # Drag Coefficient [kg/s]\n",
    "\n",
    "dt = 0.01 * (2 * mass / b)\n",
    "\n",
    "gravity = wp.vec3([0.0, 0.0, -g])\n",
    "```\n",
    "\n",
    "Everything but the final line is plain Python. On the final line, we see that gravity was defined as `gravity = wp.vec3([0.0, 0.0, -g])`.\n",
    "\n",
    "`wp.vec3` is a **built-in** data type provided by Warp that represents a vector made of three 32-bit floating point values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating arrays for the particle positions and velocities\n",
    "\n",
    "Next, we see that we allocate some **arrays** to represent the positions and velocities of the particles:\n",
    "\n",
    "```python\n",
    "# Initial positions: random values between -1.0 and 1.0 for x, y, and z\n",
    "rng = np.random.default_rng(12345)\n",
    "positions_np = rng.uniform(low=-1.0, high=1.0, size=(num_particles, 3))\n",
    "positions = wp.array(positions_np, dtype=wp.vec3)\n",
    "\n",
    "# Initial velocities: random values between -0.5 and 0.5 m/s for vx, vy, and vz\n",
    "velocities_np = rng.uniform(low=-0.5, high=0.5, size=(num_particles, 3))\n",
    "velocities = wp.array(velocities_np, dtype=wp.vec3)\n",
    "```\n",
    "\n",
    "Arrays in Warp are the fundamental way to represent data and can be created using the `wp.array()` constructor.\n",
    "Like NumPy, arrays can be multi-dimensional and all elements of an array must be of the same **data type**.\n",
    "\n",
    "Note that for both the positions and velocities, we first created NumPy arrays containing the data we wanted to initialize the particles with and then passed these NumPy arrays into the `wp.array()` constructor along with the data type of `wp.vec3` (same type as the `gravity` variable). This is one of the common ways that Warp can **interoperate** with NumPy.\n",
    "\n",
    "The memory allocations that represent the `positions` and `velocities` array end up on the GPU in our example, which is important because we want to use the GPU to update the particles in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a kernel\n",
    "\n",
    "With arrays allocated for the particle data, we then define the computation on that data by writing a **kernel**, which is essentially a function that gets compiled and executed across many threads on the GPU. Actually, kernels in Warp can also be compiled and executed on the CPU, but it will currently be limited to single-threaded execution.\n",
    "\n",
    "Kernels in Warp are defined by decorating a Python function with `@wp.kernel`. Valid kernels in Warp must also obey additional restrictions, such as:\n",
    "\n",
    "- Use a subset of the Python language\n",
    "- Arguments must be typed\n",
    "- Cannot `return` anything\n",
    "\n",
    "When a kernel is *launched*, the body of the kernel is executed a certain number of times in parallel as specified through the **kernel launch dimensions** passed to `wp.launch()`.\n",
    "\n",
    "In contrast, ordinary Python functions only get executed once when called.\n",
    "\n",
    "We define our `integrate` kernel to accept the `positions` and `velocities` arrays as inputs. Inside the kernel, we use `wp.tid()` to get the current thread ID, which tells us which particle (array element) this particular thread should process. This allows each thread to work on a different particle simultaneously.\n",
    "\n",
    "```python\n",
    "@wp.kernel\n",
    "def integrate(positions: wp.array(dtype=wp.vec3), velocities: wp.array(dtype=wp.vec3)):\n",
    "    i = wp.tid()\n",
    "\n",
    "    acceleration = (-b * velocities[i]) / mass + gravity\n",
    "    velocities[i] += acceleration * dt\n",
    "    positions[i] += velocities[i] * dt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the `integrate` kernel\n",
    "\n",
    "We use the `wp.launch()` function to run our kernel on the GPU. This function takes three main arguments: the kernel function to execute, the number of parallel threads to launch (specified by `dim`), and the input arguments that match the kernel's function signature.\n",
    "\n",
    "```python\n",
    "for step in range(num_steps):\n",
    "    wp.launch(integrate, dim=(num_particles,), inputs=[positions, velocities])\n",
    "```\n",
    "\n",
    "The `integrate` kernel is launched inside a for loop to simulate the particle motion over time. Each iteration of the loop represents one time step, updating the positions and velocities of all particles based on the physics equations. This process repeats for `num_steps` iterations to simulate the full duration of the particle system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the results\n",
    "\n",
    "After completing all the simulation time steps, we can print out the final particle positions:\n",
    "\n",
    "```python\n",
    "print(f\"Final positions: {positions}\")\n",
    "```\n",
    "\n",
    "This simple print statement works seamlessly because Warp automatically handles the data transfer from GPU memory back to CPU memory.\n",
    "The `positions` array, which was stored on the GPU during computation, is transparently copied to the CPU so we can display its values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this basic example, we saw the fundamental building blocks of a Warp program:\n",
    "\n",
    "- **Data management**: Using `wp.array()` to create and store data on the GPU\n",
    "- **Computation**: Defining kernels as Python functions decorated with `@wp.kernel` to perform parallel operations\n",
    "- **Execution**: Launching kernels with `wp.launch()` to run computations across multiple GPU threads\n",
    "\n",
    "Together, these components enable high-performance parallel computing on GPUs while maintaining Python's ease of use.\n",
    "In the following sections, we will explore each of these concepts in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Kernels\n",
    "\n",
    "In Warp, computational kernels are defined as Python functions and annotated with the `@wp.kernel` decorator.\n",
    "\n",
    "Python functions that define Warp kernels must obey some additional restrictions, such as:\n",
    "\n",
    "- Use a subset of the Python language\n",
    "- Arguments must be typed\n",
    "- Cannot `return` anything\n",
    "\n",
    "Conceptually, Warp kernels are similar to CUDA kernels. When a kernel is *launched*, the body of the kernel\n",
    "is executed a certain number of times in parallel as specified through the **kernel launch dimensions**.\n",
    "\n",
    "In contrast, ordinary Python functions only get executed once when called.\n",
    "\n",
    "Like CUDA kernels, Warp kernels do not return a value. Instead, each thread in a kernel can modify data in **global memory**\n",
    "wrapped by the Warp arrays passed as arguments to the kernel.\n",
    "\n",
    "The following notebook cell contains one of the simplest Warp kernels possible. It fills out an array with the thread index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def fill_kernel(x: wp.array(dtype=int)):\n",
    "    i = wp.tid()  # Get the thread index\n",
    "    x[i] = i\n",
    "\n",
    "\n",
    "x = wp.zeros(10, dtype=int)\n",
    "\n",
    "print(f\"Initial x: {x.numpy()}\")\n",
    "wp.launch(fill_kernel, dim=[10], inputs=[x])\n",
    "print(f\"Final x: {x.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of kernels on the GPU\n",
    "\n",
    "At this point, we should understand how kernels in Warp map over to hardware units on the GPU.\n",
    "\n",
    "- **Warp** maps the grid dimensions provided in `wp.launch()` into a **one-dimensional CUDA kernel grid**\n",
    "- The **CUDA grid** is decomposed into individual **thread blocks** of uniform size, which are executed independently from each other\n",
    "  - Warp defaults the CUDA grid to 256 threads per block. While you can adjust this up to 1024, whether a different number improves performance is specific to your kernel's workload and the GPU architecture.\n",
    "- Each **thread block** is assigned for execution on a **streaming multiprocessor (SM)**\n",
    "  - Each block can be scheduled on any available SM in any order\n",
    "  - Once a thread block begins executing on an SM, it will run to completion on that same SM\n",
    "  - A SM can often run multiple thread blocks **concurrently**\n",
    "- A SM processes a thread block by dividing it into **hardware warps**, each consisting of 32 threads that execute instructions in lockstep (SIMT - Single Instruction, Multiple Thread).\n",
    "  - This is not an important detail for the purposes of using **NVIDIA Warp**, but it's worth mentioning this potential source of naming confusion.\n",
    "- Individual threads within a hardware warp execute their instructions on **CUDA cores** within the SM\n",
    "\n",
    "![Execution Hierarchy on a GPU](images/chapter-02//gpu-kernel-exec.png)\n",
    "\n",
    "*Image credit: [NVIDIA Developer Blog](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The compilation pipeline and the kernel cache\n",
    "\n",
    "When a kernel is launched for the first time, all kernels in the module that have been defined so far will be translated to native C++/CUDA code and **just-in-time compiled**.\n",
    "\n",
    "Both the C++/CUDA source and compiled objects are stored as files in the *kernel cache*, which is located in `wp.config.kernel_cache_dir` so that subsequent launches do not have to incur a code generation and compilation overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.config.kernel_cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After launching `fill_kernel`, a `.cu` file and a `.ptx` or `.cubin` file corresponding to the CUDA source generated by Warp \n",
    "and the compiler output from the NVIDIA Runtime Compilation (NVRTC) library will appear in the kernel cache.\n",
    "\n",
    "The following image shows the Warp compilation pipeline:\n",
    "\n",
    "<div style=\"background-color:white;\">\n",
    "    <img src=\"./images/chapter-12/warp-compilation-pipeline.svg\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the generated code.\n",
    "\n",
    "We will use a separate script that runs a finite-difference kernel. The kernel cache will be changed from the default location so we can look at the cache from the sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile Chapter_12_finite_difference.py\n",
    "\n",
    "import warp as wp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "wp.config.kernel_cache_dir = os.path.join(\n",
    "    os.path.dirname(os.path.realpath(__file__)), \"Chapter_12_finite_difference_example_cache\"\n",
    ")\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def finite_difference(dx: float, u: wp.array(dtype=float), u_out: wp.array(dtype=float)):\n",
    "    i = wp.tid()\n",
    "    total_points = u.shape[0]\n",
    "    u_out[i] = (u[(i + 1) % total_points] - u[(i - 1 + total_points) % total_points]) / (2.0 * dx)\n",
    "\n",
    "\n",
    "sin_array_np = np.sin(np.linspace(0, 2 * np.pi, 1024))\n",
    "sin_array_wp = wp.array(sin_array_np, dtype=float)\n",
    "\n",
    "u_out_wp = wp.empty_like(sin_array_wp)\n",
    "\n",
    "dx = 2 * np.pi / 1024\n",
    "\n",
    "wp.launch(finite_difference, sin_array_wp.shape, inputs=[dx, sin_array_wp], outputs=[u_out_wp])\n",
    "\n",
    "print(f\"{sin_array_wp.numpy()=}\")\n",
    "print(f\"{u_out_wp.numpy()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Chapter_12_finite_difference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the files that are saved in the kernel cache located in the `Chapter_12_finite_difference_example_cache` directory.\n",
    "\n",
    "Note that by default, Warp generates both a  **forward** and **backward** version of each kernel.\n",
    "\n",
    "The **forward** version is what you are probably accustomed to seeing. The **backward** version is used in automatic differentiation (more on this later).\n",
    "\n",
    "Now, we will make a small modification to the file to use a second-order finite-difference when computing `u_out`.\n",
    "\n",
    "Everything else will be the same (filename, the name of the `finite_difference` kernel, problem resolution, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Chapter_12_finite_difference.py\n",
    "\n",
    "import warp as wp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "wp.config.kernel_cache_dir = os.path.join(\n",
    "    os.path.dirname(os.path.realpath(__file__)), \"Chapter_12_finite_difference_example_cache\"\n",
    ")\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def finite_difference(dx: float, u: wp.array(dtype=float), u_out: wp.array(dtype=float)):\n",
    "    i = wp.tid()\n",
    "    total_points = u.shape[0]\n",
    "    u_out[i] = (\n",
    "        u[(i + 1) % total_points] - 2.0 * u[i] + u[(i - 1 + total_points) % total_points]\n",
    "    ) / (dx * dx)\n",
    "\n",
    "\n",
    "sin_array_np = np.sin(np.linspace(0, 2 * np.pi, 1024))\n",
    "sin_array_wp = wp.array(sin_array_np, dtype=float)\n",
    "\n",
    "u_out_wp = wp.empty_like(sin_array_wp)\n",
    "\n",
    "dx = 2 * np.pi / 1024\n",
    "\n",
    "wp.launch(finite_difference, sin_array_wp.shape, inputs=[dx, sin_array_wp], outputs=[u_out_wp])\n",
    "\n",
    "print(f\"{sin_array_wp.numpy()=}\")\n",
    "print(f\"{u_out_wp.numpy()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Chapter_12_finite_difference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the log from the previous cell, we should see a line like:\n",
    "\n",
    "```\n",
    "Module __main__ ea6bc0d load on device 'cuda:0' took 305.15 ms  (compiled)\n",
    "```\n",
    "\n",
    "Changing the contents of the `finite_difference` resulted in a new hash (e.g. `05beb6e`) for the module.\n",
    "\n",
    "Since the kernel cache did not already contain compiled code for the `__main__` module with hash `05beb6e`,\n",
    "the Python code was translated to CUDA C++ and compiled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversions inside Warp kernels\n",
    "\n",
    "Because Warp kernels are compiled to native C++/CUDA code, all the function input arguments should be *typed*.\n",
    "\n",
    "This allows Warp to generate fast code that executes at essentially native speeds.\n",
    "\n",
    "An exception will be raised if there are type mismatches, as Warp does not automatically perform type conversions for the user.\n",
    "\n",
    "Let's see what happens if `fill_kernel` expects an `int` array, but we give it a `float` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def fill_kernel(x: wp.array(dtype=int)):\n",
    "    i = wp.tid()\n",
    "    x[i] = i\n",
    "\n",
    "\n",
    "x = wp.array(shape=10, dtype=float)\n",
    "\n",
    "wp.launch(fill_kernel, dim=[10], inputs=[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you will need to cast variables to a different type inside Warp kernels, e.g. to multiply a `float` with an `int`\n",
    "\n",
    "```python\n",
    "    prod[i] = float(int_array[i])*float_array[i]\n",
    "```\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "```python\n",
    "    prod[i] = wp.float32(int_array[i])*float_array[i]\n",
    "```\n",
    "\n",
    "As an exercise, fix the type issue in the following kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def sin_kernel(dx: float, result: wp.array(dtype=float)):\n",
    "    i = wp.tid()\n",
    "\n",
    "    # Convert to a position\n",
    "    x_pos = i * dx\n",
    "\n",
    "    result[i] = wp.sin(2.0 * wp.PI * x_pos)\n",
    "\n",
    "\n",
    "dx = 2.0\n",
    "result_array = wp.empty(10, dtype=float)\n",
    "\n",
    "wp.launch(sin_kernel, result_array.shape, inputs=[dx, result_array])\n",
    "\n",
    "print(result_array.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using generics to create more flexible kernels\n",
    "\n",
    "For convenience and to improve code reusability, Warp supports the use of `typing.Any` instead of concrete types.\n",
    "\n",
    "See the [Generics documentation](https://nvidia.github.io/warp/modules/generics.html) for more information.\n",
    "\n",
    "The below example defines a single generic kernel and launches it three times on different data types.\n",
    "\n",
    "Note the use of `type()` in type conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def sin_kernel(dx: Any, result: wp.array(dtype=Any)):\n",
    "    i = wp.tid()\n",
    "\n",
    "    # Convert to a position\n",
    "    x_pos = type(dx)(i) * dx\n",
    "\n",
    "    result[i] = wp.sin(type(result[i])(2.0 * wp.PI) * x_pos)\n",
    "\n",
    "\n",
    "# wp.float16\n",
    "result_array = wp.empty(10, dtype=wp.float16)\n",
    "wp.launch(sin_kernel, result_array.shape, inputs=[wp.float16(2.0), result_array])\n",
    "print(f\"wp.float16: {result_array.numpy()}\")\n",
    "\n",
    "# float or wp.float32\n",
    "result_array = wp.empty(10, dtype=float)\n",
    "wp.launch(sin_kernel, result_array.shape, inputs=[float(2.0), result_array])\n",
    "print(f\"wp.float32: {result_array.numpy()}\")\n",
    "\n",
    "# wp.float64\n",
    "result_array = wp.empty(10, dtype=wp.float64)\n",
    "wp.launch(sin_kernel, result_array.shape, inputs=[wp.float64(2.0), result_array])\n",
    "print(f\"wp.float64: {result_array.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug printing from inside Warp kernels\n",
    "\n",
    "We can use `wp.printf()` to print C-style formatted strings from inside Warp kernels.\n",
    "\n",
    "To print composite types like vectors and matrices, use `print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def print_tid():\n",
    "    i = wp.tid()\n",
    "    wp.printf(\"Thread Index: %d\\n\", i)\n",
    "\n",
    "\n",
    "wp.launch(print_tid, (10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional kernels\n",
    "\n",
    "So far, we have been launching kernels on 1-D grids, but we can use up to 4-D grids (matching the maximum dimensionality of Warp arrays).\n",
    "\n",
    "To get multi-dimensional thread indices, we make use of tuple unpacking:\n",
    "\n",
    "```python\n",
    "i = wp.tid()\n",
    "i, j = wp.tid()\n",
    "i, j, k = wp.tid()\n",
    "i, j, k, l = wp.tid()\n",
    "```\n",
    "\n",
    "The following example launches a kernel on a 4-D grid on which each thread retrieves and prints out its thread index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def basic_4d_kernel():\n",
    "    i, j, k, w = wp.tid()\n",
    "    wp.printf(\"(%d,%d,%d,%d)\\n\", i, j, k, w)\n",
    "\n",
    "\n",
    "wp.launch(basic_4d_kernel, (2, 2, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `device` keyword\n",
    "\n",
    "The `device` keyword may be used to target a specific device for a kernel launch.\n",
    "\n",
    "Recall at the beginning of this notebook, we saw the `\"cpu\"` and `\"cuda:0\"` devices printed out in the `Devices:` section when we called `wp.init()`. These aliases may be used to launch kernels and allocate arrays with the `device` keyword.\n",
    "\n",
    "All arrays must reside on the same device as the kernel launch.\n",
    "\n",
    "We get an error if we try to launch a kernel on `\"cpu\"` with arrays that are on `\"cuda:0\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def sum_kernel(a: wp.array(dtype=float), sum: wp.array(dtype=float)):\n",
    "    i = wp.tid()\n",
    "    wp.atomic_add(sum, 0, a[i])\n",
    "\n",
    "\n",
    "inputs = wp.ones(10, dtype=float, device=\"cuda:0\")  # ERROR: Needs to be on \"cpu\"\n",
    "sum = wp.empty(1, dtype=float, device=\"cpu\")\n",
    "\n",
    "wp.launch(sum_kernel, inputs.shape, inputs=[inputs, sum], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a Warp API call that accepts a `device` argument (typically array allocations or kernel launches) is not provided a device, something we call the **default device** is used.\n",
    "\n",
    "The default device will be `'cuda:0'` if an NVIDIA GPU was detected by Warp, otherwise it will be `'cpu'`.\n",
    "\n",
    "We can get the default device by calling `wp.get_device()` without an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Arrays\n",
    "\n",
    "Memory allocations are exposed via the `wp.array` type. They are important because **kernels** must write their results to memory instead of directly returning values.\n",
    "\n",
    "Arrays wrap an underlying memory allocation that may live in either host (CPU) or device (GPU) memory.\n",
    "\n",
    "All arrays have an associated data type, which can be a scalar data type (e.g. `float`, `int`) or a composite data type e.g. `vec3`, `matrix33`).\n",
    "\n",
    "We can set the data type using the `dtype` parameter, or we can let Warp infer it when creating an array from existing data (e.g. a Python list or NumPy array).\n",
    "\n",
    "The full list of **scalar data types** that can be used for Warp arrays is currently:\n",
    "\n",
    "| Name       | Description            |\n",
    "|------------|------------------------|\n",
    "| `bool`     | Boolean                |\n",
    "| `int8`     | Signed byte            |\n",
    "| `uint8`    | Unsigned byte          |\n",
    "| `int16`    | Signed short           |\n",
    "| `uint16`   | Unsigned short         |\n",
    "| `int32`    | Signed integer         |\n",
    "| `uint32`   | Unsigned integer       |\n",
    "| `int64`    | Signed long integer    |\n",
    "| `uint64`   | Unsigned long integer  |\n",
    "| `float16`  | Half-precision float   |\n",
    "| `float32`  | Single-precision float |\n",
    "| `float64`  | Double-precision float |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the construction of an under-specified `wp.array()` (both a shape and data type is required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeError: A concrete type is required\n",
    "missing_dtype = wp.array(shape=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works since both shape and dtype are specified. Values are uninitialized.\n",
    "empty_array = wp.array(shape=10, dtype=wp.float32)\n",
    "\n",
    "print(empty_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the following convenience functions are commonly used to construct Warp arrays with a basic initialization choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_b = wp.empty((5, 5), dtype=wp.float32)  # 5 x 5 array, uninitialized\n",
    "print(f\"array_b = {array_b}\")\n",
    "\n",
    "array_a = wp.zeros((5, 5), dtype=wp.float32)  # 5 x 5 array, zero-initialized\n",
    "print(f\"array_a = {array_a}\")\n",
    "\n",
    "array_c = wp.ones((5, 5), dtype=wp.float32)  # 5 x 5 array, one-initialized\n",
    "print(f\"array_c = {array_c}\")\n",
    "\n",
    "array_d = wp.full(shape=25, value=10, dtype=int)  # 25-element array initialized to 10\n",
    "print(f\"array_d = {array_d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that arrays we allocate without explicitly specifying a target `device` will be allocated on the default device, which is an NVIDIA GPU if one was detected on the system.\n",
    "\n",
    "We can verify this by checking the `device.is_cuda` property of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = wp.zeros(10, dtype=wp.int32)\n",
    "print(f\"{test_array.shape=}\")\n",
    "print(f\"{test_array.dtype=}\")\n",
    "print(f\"{test_array.device=}\")\n",
    "print(f\"{test_array.device.is_cuda=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explicitly allocate the same array on the `'cpu'` device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = wp.zeros(10, dtype=wp.int32, device=\"cpu\")\n",
    "print(f\"{test_array.shape=}\")\n",
    "print(f\"{test_array.dtype=}\")\n",
    "print(f\"{test_array.device=}\")\n",
    "print(f\"{test_array.device.is_cuda=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NumPy arrays to initialize Warp arrays\n",
    "\n",
    "One of the libraries that Warp **interoperates** with is NumPy. This means that some Warp functions can accept NumPy arrays.\n",
    "\n",
    "As we saw in the particle-simulation example, it is often convenient to initialize a Warp array from a NumPy array that has the desired values.\n",
    "\n",
    "Simply pass the NumPy array in the first position of the `wp.array()` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu, sigma = 0, 0.1  # mean and standard deviation\n",
    "rng = np.random.default_rng()\n",
    "random_numbers_np = rng.normal(mu, sigma, 1000)\n",
    "\n",
    "random_numbers_wp = wp.array(\n",
    "    random_numbers_np\n",
    ")  # Create a Warp array on the GPU from the NumPy array (Warp type is inferred)\n",
    "\n",
    "print(f\"{random_numbers_wp.device=}\")\n",
    "print(f\"{random_numbers_wp.dtype=}\")\n",
    "print(f\"{random_numbers_wp.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to retrieve the values of a **GPU** array like `test_array` on the **CPU**, we need to copy the data back to **CPU** memory.\n",
    "\n",
    "The `numpy()` method is useful for getting a **temporary NumPy view** of a Warp array.\n",
    "\n",
    "If the Warp array is on the **GPU**, a new array on the **CPU** will first be created, and then the contents of the **GPU** array will be copied into it before returning a NumPy view.\n",
    "\n",
    "If the Warp array is already on the **CPU**, then a zero-copy NumPy view is returned.\n",
    "\n",
    "The `__str__` method of Warp arrays also automatically calls `numpy()` on the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring data from device back to host\n",
    "test_array_cpu = test_array.numpy()\n",
    "\n",
    "print(test_array_cpu)\n",
    "\n",
    "# Automatically calls `numpy()`\n",
    "print(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to allocate an array on the CPU, we can explicitly pass the argument `\"cpu\"` to the `device` parameter.\n",
    "\n",
    "Recall that without specifying a `device` argument, the array will be allocated on the *default device*, which prefers a GPU on the system over the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_array_cpu = wp.array(shape=(4, 4), dtype=wp.float64, device=\"cpu\")\n",
    "empty_array_cpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi-dimensional arrays\n",
    "\n",
    "Multi-dimensional arrays with up to four dimensions are currently supported in Warp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_4d = wp.array(shape=[2, 2, 2, 2], dtype=wp.float32)\n",
    "\n",
    "print(f\"array_4d.shape = {array_4d.shape}\")\n",
    "\n",
    "print(array_4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite types\n",
    "\n",
    "So far, we have been mostly creating Warp arrays based on scalar data types, but *composite* data types are also supported for convenience.\n",
    "\n",
    "Here, we allocate a 10-element array with the data type `wp.vec3`, which is a three-component `wp.float32` vector. We used arrays of `wp.vec3` in the initial particle-simulation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = wp.ones(10, dtype=wp.vec3, device=\"cuda\")\n",
    "\n",
    "print(f\"v.shape = {v.shape}\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying arrays between devices\n",
    "\n",
    "The values in an array can also be copied directly into another array (including between arrays residing on different GPUs) using `wp.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_array = wp.full(shape=10, value=10.0, dtype=float, device=\"cpu\")\n",
    "dest_array = wp.zeros_like(src_array, device=\"cuda:0\")\n",
    "\n",
    "# Print dest_array before the copy\n",
    "print(f\"dest_array (before copy): {dest_array}\")\n",
    "\n",
    "# copy from src_array (CPU) to dest_array (GPU)\n",
    "wp.copy(dest_array, src_array)\n",
    "\n",
    "print(f\"dest_array (after copy): {dest_array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Python-scope API vs. kernel-scope API\n",
    "\n",
    "Some of the Warp API can only be called from the **Python scope** (i.e. outside of Warp user functions and kernels),\n",
    "while others can only be called from the **kernel scope** (i.e. inside Warp kernels and functions).\n",
    "\n",
    "The Python-scope API is documented in the [Python Reference](https://nvidia.github.io/warp/modules/runtime.html),\n",
    "while the functions available to use from Warp kernels is documented in the [Built-Ins Reference](https://nvidia.github.io/warp/modules/functions.html).\n",
    "\n",
    "Generally, the kernel-scope API can also be used in the Python scope. These functions are annotated with a `Python` tag in the Built-Ins Reference.\n",
    "\n",
    "Not all of the Python language is supported inside the kernel scope. Some features haven't been implemented yet, while\n",
    "other features do not map well to the GPU from a performance perspective.\n",
    "\n",
    "See the [Limitations](https://nvidia.github.io/warp/limitations.html) documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def invalid_kernel_api_example(a: wp.array(dtype=float), b: wp.array(dtype=float)):\n",
    "    wp.copy(a, b)  # Invalid: wp.copy() cannot be used in a Warp kernel\n",
    "\n",
    "\n",
    "a = wp.ones(10, dtype=float)\n",
    "b = wp.zeros(10, dtype=float)\n",
    "wp.launch(invalid_kernel_api_example, (1,), inputs=[a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User functions\n",
    "\n",
    "Users can write their own reusable functions which can be called from kernels using the `@wp.func` decorator, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.func\n",
    "def square(x: float):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def test_kernel(x: wp.array(dtype=float), y: wp.array(dtype=float)):\n",
    "    i = wp.tid()\n",
    "\n",
    "    y[i] = square(x[i])\n",
    "\n",
    "\n",
    "x = wp.full((10,), value=10.0, dtype=float)\n",
    "y = wp.empty_like(x)\n",
    "\n",
    "wp.launch(test_kernel, x.shape, inputs=[x], outputs=[y])\n",
    "\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels can call user functions that are defined in the same module or in a different module. As the example shows, return type hints for user functions are **optional**.\n",
    "\n",
    "Anything that can be done in a Warp kernel can also be done in a user function with the exception of `wp.tid()`.\n",
    "\n",
    "The thread index can be passed in through the arguments of a user function if it is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Structs\n",
    "\n",
    "Users can define their own structures using the `@wp.struct` decorator.\n",
    "\n",
    "Structs may be passed as arguments to kernels, e.g. to simplify kernel signatures when many arguments are required.\n",
    "\n",
    "Structs may also be used as a data type for Warp arrays.\n",
    "\n",
    "The following example shows how a struct is created for the simulation parameters required in the `update` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "\n",
    "@wp.struct\n",
    "class SimParameters:\n",
    "    nx: int\n",
    "    dx: float\n",
    "    dt: float\n",
    "    alpha: float\n",
    "\n",
    "\n",
    "sim_params = SimParameters()\n",
    "sim_params.nx = 1024\n",
    "sim_params.dx = 0.1\n",
    "sim_params.alpha = 0.01\n",
    "sim_params.dt = 0.5 * (sim_params.dx * sim_params.dx) / (2 * sim_params.alpha)\n",
    "\n",
    "\n",
    "u_np = np.zeros(sim_params.nx)\n",
    "\n",
    "# Set initial condition: heat in the middle\n",
    "u_np[sim_params.nx // 2] = 100\n",
    "\n",
    "u = wp.array(u_np, dtype=float)\n",
    "u_new = wp.zeros(sim_params.nx, dtype=float)\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def update(sim_params: SimParameters, u: wp.array(dtype=float), u_new: wp.array(dtype=float)):\n",
    "    i = wp.tid()\n",
    "\n",
    "    if (i > 0) and (i < sim_params.nx - 1):\n",
    "        u_new[i] = u[i] + sim_params.alpha * sim_params.dt * (u[i + 1] - 2.0 * u[i] + u[i - 1]) / (\n",
    "            sim_params.dx * sim_params.dx\n",
    "        )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for step_index in range(10000):\n",
    "    wp.launch(update, (sim_params.nx,), inputs=[sim_params, u, u_new])\n",
    "    (u, u_new) = (u_new, u)\n",
    "\n",
    "    if (step_index + 1) % 1000 == 0:\n",
    "        ax.plot(u.numpy(), label=f\"{step_index}\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Automatic differentiation\n",
    "\n",
    "If we want to compute a derivative in a computer program, our main options are:\n",
    "\n",
    "1. Compute derivatives by hand and then coding up the derivative formulas\n",
    "2. Compute derivatives using finite-difference approximations\n",
    "3. Implement the formula in a computer algebra system like Mathematica, then take derivatives using symbolic differentiation\n",
    "4. Use automatic differentiation to obtain exact numerical derivatives\n",
    "\n",
    "As we saw in the earlier finite-difference example, Warp by default generates a forward and backward (adjoint) version of each kernel definition.\n",
    "\n",
    "The backward version of a kernel can be used to compute gradients of loss functions using **reverse-mode automatic differentiation**.\n",
    "\n",
    "Arrays that participate in the chain of computation which require gradients should be created with `requires_grad=True`, for example:\n",
    "\n",
    "```python\n",
    "a = wp.zeros(1024, dtype=wp.vec3, requires_grad=True)\n",
    "```\n",
    "\n",
    "The `wp.Tape` class can then be used to record kernel launches, and replay them to compute the gradient of a scalar loss function with respect to the kernel inputs:\n",
    "\n",
    "```python\n",
    "# forward pass\n",
    "with wp.Tape() as tape:\n",
    "    wp.launch(kernel=compute1, inputs=[a, b])\n",
    "    wp.launch(kernel=compute2, inputs=[c, d])\n",
    "    wp.launch(kernel=loss, inputs=[d, l])\n",
    "\n",
    "# reverse pass\n",
    "tape.backward(l)\n",
    "```\n",
    "\n",
    "After the backward pass has completed, the gradients with respect to the inputs are available from the `array.grad` attribute:\n",
    "\n",
    "```python\n",
    "# gradient of loss with respect to input a\n",
    "print(a.grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, Warp's automatic differentiation capabilities are applied to more complicated algorithms involving branching logic, loops, and function calls, but we will look at a closed-form expression because we can easily compare the results to different methods of obtaining numerical derivatives.\n",
    "\n",
    "Let's consider evaluating the following closed-form function at `x = 0.5`.\n",
    "\n",
    "$$\n",
    "f(x) = \\sin \\left(x^2\\right) \\cdot \\ln(x) + \\frac{x^3}{\\sqrt{1 - x^2}}\n",
    "$$\n",
    "\n",
    "We could have found an analytical expression for the function derivative by hand and implemented a function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def f_grad(x):\n",
    "    return (\n",
    "        x * x * x * x / (1 - x * x) ** (3 / 2)\n",
    "        + 3 * x * x / math.sqrt(1 - x * x)\n",
    "        + 2 * x * math.log(x) * math.cos(x * x)\n",
    "        + math.sin(x * x) / x\n",
    "    )\n",
    "\n",
    "\n",
    "print(f_grad(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have used a finite-difference approximation to evaluate the derivative with a truncation and round-off error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return math.sin(x * x) * math.log(x) + x * x * x / math.sqrt(1.0 - x * x)\n",
    "\n",
    "\n",
    "h = 1e-8\n",
    "\n",
    "f_deriv = (f(0.5 + h) - f(0.5)) / h\n",
    "\n",
    "print(f_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytic approach doesn't scale well as the quantity being calculated grows in complexity, while\n",
    "the numerical approach doesn't scale well with the number of inputs (in additional to the choice of the step size `h` being difficult to choose).\n",
    "\n",
    "If we want to evaluate the derivative using Warp's automatic differentiation feature,\n",
    "we would implement a kernel that performs the function evaluation and writes the result into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.func\n",
    "def f(x: wp.float64):\n",
    "    return wp.sin(x * x) * wp.log(x) + x * x * x / wp.sqrt(wp.float64(1.0) - x * x)\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def compute(x: wp.array(dtype=wp.float64), out: wp.array(dtype=wp.float64)):\n",
    "    i = wp.tid()\n",
    "    out[i] = f(x[i])\n",
    "\n",
    "\n",
    "x = wp.full((1,), value=0.5, dtype=wp.float64, requires_grad=True)\n",
    "out = wp.empty(1, dtype=wp.float64, requires_grad=True)\n",
    "\n",
    "with wp.Tape() as tape:\n",
    "    wp.launch(compute, (1,), inputs=[x, out])\n",
    "\n",
    "tape.backward(loss=out)\n",
    "\n",
    "print(x.grad.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this result is **not** obtained using numerical differentiation. There is no step size.\n",
    "\n",
    "Instead, the program has been run two times:\n",
    "\n",
    "- Once in a *forward mode* when `wp.launch()` is called\n",
    "- Once in a *reverse mode* (because adjoints are propagated in reverse from outputs to inputs) when `tape.backward()` is called\n",
    "\n",
    "**How does Warp know how to evaluate derivatives exactly?** AD systems implement the known derivatives for a finite set of elementary operations. The chain rule is used to combine the elementary derivatives together to get the overall derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Interoperating with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us install the PyTorch package. We will use PyTorch custom operators extensively in this section, so make sure you have PyTorch >= 2.4 installed for this support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warp as wp\n",
    "\n",
    "# Install the latest version of PyTorch from the link below\n",
    "!pip install torch # For PyTorch installation, needs to be >=2.4 for PyTorch custom operators to work\n",
    "# Choose the appropriate installation command for your system configuration from the link below\n",
    "# https://pytorch.org/get-started/locally/\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__ < \"2.4\":\n",
    "    print(\"PyTorch version is less than 2.4, please install PyTorch >= 2.4\")\n",
    "else:\n",
    "    print(\"PyTorch version is 2.4 or greater, all good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary examples \n",
    "Warp provides `wp.to_torch()` and `wp.from_torch()` helper functions to convert arrays to/from PyTorch tensors without copying the underlying data (works both on CPU and GPU). If an associated gradient array exists, that is also converted simultaneously. Some small examples are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp --> PyTorch conversion\n",
    "\n",
    "# Construct a Warp array, including its corresponding gradient array\n",
    "w = wp.array(\n",
    "    [1.0, 2.0, 3.0], dtype=wp.float32, requires_grad=True, device=wp.get_device()\n",
    ")\n",
    "\n",
    "# Fill w.grad with 1.0 for now\n",
    "w.grad.fill_(1.0)\n",
    "\n",
    "# Convert to Torch tensor\n",
    "t = wp.to_torch(w)\n",
    "\n",
    "print(\"t = \", t)\n",
    "print(\"t.grad = \", t.grad)\n",
    "\n",
    "# Set all t.grad to zero in PyTorch\n",
    "t.grad.zero_()\n",
    "\n",
    "print(\n",
    "    \"After zeroing the grad from PyTorch interface, printing from Warp interface \",\n",
    "    w.grad,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch --> Warp conversion\n",
    "\n",
    "# Construct a Torch tensor, and its corresponding gradient tensor\n",
    "t = torch.tensor(\n",
    "    [1.0, 2.0, 3.0],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    ")\n",
    "\n",
    "# Convert Torch tensor to Warp array\n",
    "\n",
    "w = wp.from_torch(t)\n",
    "\n",
    "# Print array value and corresponding gradients\n",
    "print(\"w = \", w)\n",
    "print(\"w.grad = \", w.grad)\n",
    "\n",
    "# Set all w.grad to 1.0 and print from PyTorch interface\n",
    "\n",
    "w.grad.fill_(1.0)\n",
    "print(\n",
    "    \"After setting grad values to 1 from Warp interface, printing from PyTorch interface \",\n",
    "    t.grad,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing custom operators in PyTorch using Warp\n",
    "\n",
    "In this example we will determine the $(x, y)$ values at which the [Rosenbrock function](https://www.sfu.ca/~ssurjano/rosen.html), a non-convex function often used for testing optimization algorithms, attains its minimum value. The function is defined as follows:\n",
    "\n",
    "$$\n",
    "f(x,y) = (a-x)^2 + b(y-x^2)^2\n",
    "$$\n",
    "\n",
    "where a = 1 and b = 100. Analytically, we can find that the minimum value of $f(x,y)$ occurs at $x=y=1$ with $f(1,1)=0$.\n",
    "\n",
    "We will make use of the PyTorch custom operators (available for PyTorch 2.4 or later), that will allow us to incorporate Warp kernel launches (in both forward and backward mode) in a PyTorch graph. PyTorch custom operators allow you to wrap Python functions (in this case, Warp kernel launches) so that they behave like PyTorch native operators. See the [PyTorch docs](https://pytorch.org/tutorials/advanced/python_custom_ops.html#adding-training-support-for-crop) for more information on PyTorch custom operators. This is particularly useful when you have a computational graph that is managed in PyTorch but you want to use Warp kernels in one or more nodes. In the following example, we use Warp to evaluate the Rosenbrock function in both forward as well as the backward pass, while using PyTorch's Adam optimizer to determine the function's minimum.\n",
    "\n",
    "*Note*: it is also possible to subclass `torch.autograd.function` to the same effect.\n",
    "\n",
    "Below, we define the Warp kernel `eval_rosenbrock` in the usual way, and wrap its forward implementation with the custom PyTorch operator `warp_rosenbrock` as well its adjoint with `warp_rosenbrock_backward`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a `wp.func` for evaluating the Rosenbrock function at any given $(x, y)$ point. After that, we also define a `wp.kernel` for evaluating the Rosenbrock function on a collection of $(x,y)$ points in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rosenbrock function and forward kernel in Warp\n",
    "@wp.func\n",
    "def rosenbrock(x: float, y: float):\n",
    "    return (1.0 - x) ** 2.0 + 100.0 * (y - x**2.0) ** 2.0\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def eval_rosenbrock(xy: wp.array(dtype=wp.vec2), z: wp.array(dtype=wp.float32)):\n",
    "    i = wp.tid()\n",
    "    v = xy[i]\n",
    "    z[i] = rosenbrock(v[0], v[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first write the custom operator for the forward pass in PyTorch. The `wp::warp_rosenbrock` custom operator launches the `eval_rosenbrock` kernel through the PyTorch interface. For any custom operator, we also need to register its `FakeTensor` implementation, which allows PyTorch to determine the shape and data type of the output from the custom operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wp is our namespace that groups all our custom operators\n",
    "# warp_rosenbrock is the custom operator we are defining for forward pass\n",
    "# mutates_args is empty since we are not modifying any input arguments in-place\n",
    "@torch.library.custom_op(\"wp::warp_rosenbrock\", mutates_args=())\n",
    "def warp_rosenbrock(xy: torch.Tensor, num_particles: int) -> torch.Tensor:\n",
    "    wp_xy = wp.from_torch(xy, dtype=wp.vec2, requires_grad=False)\n",
    "    wp_z = wp.zeros(num_particles, dtype=wp.float32, requires_grad=False)\n",
    "\n",
    "    wp.launch(eval_rosenbrock, dim=num_particles, inputs=[wp_xy], outputs=[wp_z])\n",
    "\n",
    "    return wp.to_torch(wp_z)\n",
    "\n",
    "\n",
    "# Registers a FakeTensor implementation of warp_rosenbrock operator\n",
    "# Needed to reason out the shape and type of the output from the operator, at compile-time, without actually evaluating the operator\n",
    "# Each custom operator must have a register_fake function\n",
    "@warp_rosenbrock.register_fake\n",
    "def _(xy, num_particles):\n",
    "    return torch.empty(num_particles, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the custom operator for the forward pass, we define the custom operator for the backward pass `wp::warp_rosenbrock_backward` in Warp below. Note the `adjoint=True` in the `wp.launch(...)` kernel call that invokes the backward version of the kernel `eval_rosenbrock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to warp_rosenbrock operator, we define warp_rosenbrock_backward operator that also tracks the gradients w.r.t xy\n",
    "# Notice that adjoint=True in the wp.launch(...) call for eval_rosenbrock\n",
    "@torch.library.custom_op(\"wp::warp_rosenbrock_backward\", mutates_args=())\n",
    "def warp_rosenbrock_backward(\n",
    "    xy: torch.Tensor, num_particles: int, z: torch.Tensor, adj_z: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    wp_xy = wp.from_torch(xy, dtype=wp.vec2)\n",
    "    wp_z = wp.from_torch(z, requires_grad=False)\n",
    "    wp_adj_z = wp.from_torch(adj_z, requires_grad=False)\n",
    "\n",
    "    wp.launch(\n",
    "        eval_rosenbrock,\n",
    "        dim=num_particles,\n",
    "        inputs=[wp_xy],\n",
    "        outputs=[wp_z],\n",
    "        adj_inputs=[wp_xy.grad],\n",
    "        adj_outputs=[wp_adj_z],\n",
    "        adjoint=True,\n",
    "    )\n",
    "    return wp.to_torch(wp_xy.grad)\n",
    "\n",
    "\n",
    "# Similar to the FakeTensor implementation for warp_rosenbrock\n",
    "@warp_rosenbrock_backward.register_fake\n",
    "def _(xy, num_particles, z, adj_z):\n",
    "    return torch.empty_like(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both the forward and backward custom operators defined using `torch.library.custom_op`, we need to register the backward custom operator so that PyTorch knows how to perform the backward pass. Please take a look at the detailed description on the PyTorch website [here](https://docs.pytorch.org/docs/stable/library.html#torch.library.register_autograd). The outputs of the `def backward(...)` function are the gradients of $z(x,y)$ with respect to $x$ and $y$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register backward pass implementation for automatic differentiation\n",
    "# The backward function calls the custom `warp_rosenbrock_backward` operator, defined above, to compute gradients w.r.t. the inputs\n",
    "def backward(ctx, adj_z):\n",
    "    ctx.xy.grad = warp_rosenbrock_backward(ctx.xy, ctx.num_particles, ctx.z, adj_z)\n",
    "    return ctx.xy.grad, None\n",
    "\n",
    "# setup_context builds the context object ctx that stores information from the forward pass needed for the backward function\n",
    "def setup_context(ctx, inputs, output):\n",
    "    ctx.xy, ctx.num_particles = inputs\n",
    "    ctx.z = output\n",
    "\n",
    "# For the warp_rosenbrock operator, we register the backward function as well as the setup_context function defined above\n",
    "warp_rosenbrock.register_autograd(backward, setup_context=setup_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of points in the x-y plane\n",
    "num_particles = 1500\n",
    "\n",
    "# Initial point positions randomly distributed in the x-y plane\n",
    "# As the optimization happens, these points should converge to (1, 1)\n",
    "rng = np.random.default_rng(42)\n",
    "xy = torch.tensor(\n",
    "    rng.normal(size=(num_particles, 2)),\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    "    device=wp.device_to_torch(wp.get_device()),\n",
    ")\n",
    "\n",
    "# PyTorch Adam optimizer at learning rate 5e-2, defined for the xy tensor\n",
    "opt = torch.optim.Adam([xy], lr=5e-2)\n",
    "\n",
    "# Forward pass of the function\n",
    "def forward():\n",
    "    global xy, num_particles\n",
    "    z = warp_rosenbrock(xy, num_particles)\n",
    "    return z\n",
    "\n",
    "\n",
    "# Single step of the optimization\n",
    "# This is your typical optimization step that you might have seen when training other ML models\n",
    "# The key difference here is that we are updating the (x,y) points in the x-y plane to reach the minimum of the Rosenbrock function\n",
    "def step():\n",
    "    opt.zero_grad()  # Set the gradients to zero\n",
    "    z = forward()    # Forward pass that calls def forward(...) and ultimately calls the warp_rosenbrock operator\n",
    "    z.backward(torch.ones_like(z)) # Backward pass that calls the warp_rosenbrock_backward operator\n",
    "    opt.step()      # Update the (x,y) points in-place using the gradients obtained from the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of points in the x-y plane on which the function is evaluated (these points should converge to (1, 1) during optimization)\n",
    "num_particles = 1500\n",
    "\n",
    "# Initial positions of the points in the x-y plane\n",
    "rng = np.random.default_rng(42)\n",
    "xy = torch.tensor(\n",
    "    rng.normal(size=(num_particles, 2)),\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    "    device=wp.device_to_torch(wp.get_device()),\n",
    ")\n",
    "\n",
    "# PyTorch Adam optimizer\n",
    "opt = torch.optim.Adam([xy], lr=5e-2)\n",
    "\n",
    "# Domain\n",
    "min_x, max_x = -2.0, 2.0\n",
    "min_y, max_y = -2.0, 2.0\n",
    "\n",
    "# Create a grid of points\n",
    "x = np.linspace(min_x, max_x, 100)\n",
    "y = np.linspace(min_y, max_y, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XY = np.column_stack((X.flatten(), Y.flatten()))\n",
    "N = len(XY)\n",
    "\n",
    "XY = wp.array(XY, dtype=wp.vec2)\n",
    "Z = wp.empty(N, dtype=wp.float32)\n",
    "\n",
    "# Evaluate the function over the domain\n",
    "wp.launch(eval_rosenbrock, dim=N, inputs=[XY], outputs=[Z])\n",
    "Z = Z.numpy().reshape(X.shape)\n",
    "\n",
    "# Plot the function as a heatmap\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    extent=[min_x, max_x, min_y, max_y],\n",
    "    origin=\"lower\",\n",
    "    interpolation=\"bicubic\",\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    X,\n",
    "    Y,\n",
    "    Z,\n",
    "    extent=[min_x, max_x, min_y, max_y],\n",
    "    levels=150,\n",
    "    colors=\"k\",\n",
    "    alpha=0.5,\n",
    "    linewidths=0.5,\n",
    ")\n",
    "\n",
    "# Plot the optimum as a red star\n",
    "plt.plot(1, 1, \"*\", color=\"r\", markersize=10)\n",
    "\n",
    "plt.title(\"Rosenbrock function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "(mean_marker,) = ax.plot([], [], \"o\", color=\"w\", markersize=5)\n",
    "\n",
    "# Create a scatter plot (initially empty)\n",
    "scatter_plot = ax.scatter([], [], c=\"k\", s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "import IPython\n",
    "\n",
    "plt.rc(\"animation\", html=\"jshtml\")\n",
    "\n",
    "\n",
    "# Function to update the scatter plot\n",
    "def render():\n",
    "    # Compute mean\n",
    "    xy_np = xy.numpy(force=True)\n",
    "    mean_pos = np.mean(xy_np, axis=0)\n",
    "\n",
    "    # Update the scatter plot\n",
    "    scatter_plot.set_offsets(np.c_[xy_np[:, 0], xy_np[:, 1]])\n",
    "    mean_marker.set_data([mean_pos[0]], [mean_pos[1]])\n",
    "\n",
    "\n",
    "# Optimize then render\n",
    "def step_and_render(frame):\n",
    "    for i in range(200):\n",
    "        step()\n",
    "    render()\n",
    "\n",
    "\n",
    "# Create the animation and visualize in Matplotlib\n",
    "plot_anim = matplotlib.animation.FuncAnimation(\n",
    "    fig, step_and_render, frames=30, interval=100\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "IPython.display.display(plot_anim)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the animation above, the red star represents the optimum, the white dot shows the mean of $(x, y)$ coordinates across all 1500 points, and the black dots are the individual points. As the optimization progresses, both the white dot and the individual black dots converge toward the red star, validating our hybrid PyTorch-Warp optimization scheme for the Rosenbrock function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an introduction to the core components of Warp. We also took a look at PyTorch-Warp interoperability towards the end of the notebook. For more examples, see the [Warp example gallery](https://github.com/NVIDIA/warp?tab=readme-ov-file#running-examples) on GitHub.\n",
    "\n",
    "The repository at https://github.com/shi-eric/warp-lanl-tutorial-2025-05 also contains a set of tutorials for Warp.\n",
    "\n",
    "___\n",
    "## References\n",
    "\n",
    "For more information about Warp:\n",
    "\n",
    "* \"NVIDIA/warp: A Python framework for accelerated simulation, data generation and spatial computing.\", GitHub, https://github.com/NVIDIA/warp, Accessed: July 2, 2025.\n",
    "* Warp Developers, \"NVIDIA Warp Documentation,\" GitHub Pages, https://nvidia.github.io/warp/, Accessed: July 2, 2025.\n",
    "* Miles Macklin, Leopold Cambier, Eric Shi, \"Introducing Tile-Based Programming in Warp 1.5.0\", NVIDIA Developer, https://developer.nvidia.com/blog/introducing-tile-based-programming-in-warp-1-5-0/, Accessed: July 2, 2025.\n",
    "* \"Warp: Differentiable Spatial Computing for Python\", ACM Digital Library, https://dl.acm.org/doi/10.1145/3664475.3664543, Accessed: July 2, 2025.\n",
    "* Miles Macklin, \"Warp: Advancing Simulation AI with Differentiable GPU Computing in Python\", NVIDIA On-Demand, https://www.nvidia.com/en-us/on-demand/session/gtc24-s63345/, Accessed: July 2, 2025.\n",
    "* Miles Macklin, \"Warp: A High-performance Python Framework for GPU Simulation and Graphics\", NVIDIA On-Demand, https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41599/, Accessed: July 2, 2025.\n",
    "* Miles Macklin, \"Differentiable Physics Simulation for Learning and Robotics\", NVIDIA On-Demand, https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31838/, Accessed: July 2, 2025.\n",
    "\n",
    "For more information about projects using Warp:\n",
    "\n",
    "* \"nvidia-warp  GitHub Topics\", GitHub, https://github.com/topics/nvidia-warp, Accessed: July 2, 2025.\n",
    "* Warp Developers, \"Publications using Warp,\" GitHub, https://github.com/NVIDIA/warp/blob/main/PUBLICATIONS.md, Accessed: July 2, 2025.\n",
    "\n",
    "For more information about automatic differentiation:\n",
    "\n",
    "- Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind, \"[Automatic differentiation in machine learning: a survey](https://arxiv.org/abs/1502.05767)\", The Journal of Machine Learning Research, 18(153), 1-43, 2018.\n",
    "- Andreas Griewank and Andrea Walther, \"[Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation](https://books.google.com/books?id=qMLUIsgCwvUC)\", 2nd Edition, SIAM, 2008.\n",
    "- Stelian Coros, Miles Macklin, Bernhard Thomaszewski, Nils Threy, \"[Differentiable simulation](https://dl.acm.org/doi/abs/10.1145/3476117.3483433)\", SA '21: SIGGRAPH Asia 2021 Courses, 1-142, 2021.\n",
    "\n",
    "For more information on custom Python operators in PyTorch, please take a look at this [link](https://docs.pytorch.org/tutorials/advanced/python_custom_ops.html)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "warp-cfd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
