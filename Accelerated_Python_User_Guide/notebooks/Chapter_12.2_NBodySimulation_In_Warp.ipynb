{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3cebfbd2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41de3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Chapter 12.2: N-body Simulation using Single Instruction, Multiple Threads (SIMT) and Tile-based Approach in Warp\n",
    "\n",
    "![Example output](images/chapter-12.2/nbody_animation.gif)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous two notebooks, we introduced NVIDIA Warp as a framework for writing high-performance GPU code in Python and implemented a GPU-accelerated 2-D Ising model simulation. Through this parallel GPU implementation, we learned the importance of developing correct parallelization algorithms to faithfully reproduce the underlying physics of any physical system that we are modeling.\n",
    "\n",
    "Following the theme of simulating physical systems to learn the basics of NVIDIA Warp for parallel computing, in this chapter we will implement an N-body simulation in NVIDIA Warp, first using the single instruction, multiple thread (SIMT) model and then using the tile-based model (both in Warp).\n",
    "\n",
    "Through this implementation, you will learn the following:\n",
    "\n",
    "* The simulation of another physical system using Warp, building on the previous notebook on the 2-D Ising model\n",
    "* How to characterize code performance and use some code profiling tools available in Warp\n",
    "* An introduction to the tile-based programming method in Warp\n",
    "\n",
    "By the end of this chapter, you will have two variants of code for N-body simulation, one based on the SIMT paradigm and another based on the tile-based programming model. You will also have learned about tools available in Python and Warp to benchmark your code's time performance in greater detail.\n",
    "\n",
    "It is strongly recommended to work through the [notebook on introduction to NVIDIA Warp](Chapter_12_Intro_to_NVIDIA_Warp.ipynb) and [GPU-Accelerated Ising Model Simulation in NVIDIA Warp](Chapter_12.1_IsingModel_In_Warp.ipynb) before proceeding with this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d431841",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Before we begin this tutorial, let us ensure we have all the necessary packages installed.\n",
    "\n",
    "First, we will install NVIDIA Warp if it is not already available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaea329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NVIDIA Warp\n",
    "%pip install warp-lang\n",
    "\n",
    "# Install Matplotlib for plotting/animation\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292eb53a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Now let us import the necessary libraries and initialize Warp to check if GPU support is available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "# Check for GPU availability\n",
    "if wp.get_cuda_device_count() > 0:\n",
    "    print(\"✓ GPU detected successfully\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5750b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "## Introduction: Exploring avenues for further speed up of an already GPU accelerated code\n",
    "\n",
    "In the last notebook, we went through an example of how to correctly model a physical system (2-D Ising model) using Warp.\n",
    "\n",
    "Now, we will go a step beyond the correctness of the model and explore how to profile our code and accelerate an already GPU-accelerated implementation.\n",
    "\n",
    "Similar to the 2-D Ising model, we choose the N-body simulation for its simplicity while also having underlying physics that makes it amenable to a tile-based programming model.\n",
    "\n",
    "Like the previous notebook, let us begin by understanding the physics behind the N-body simulation, then progressively build our GPU implementation, profile it, analyze potential avenues for further speedup, and finally explore the tile-based programming model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76672792",
   "metadata": {},
   "source": [
    "---\n",
    "## Background: The N-body simulation in physics and its numerical implementation \n",
    "\n",
    "### What is an N-body simulation?\n",
    "\n",
    "An [N-body simulation](https://en.wikipedia.org/wiki/N-body_simulation) models the temporal evolution of a system of particles, typically under the influence of physical forces such as gravity. N-body simulations are widely used in astrophysics to study the dynamics of systems of planets, stars, and galaxies. Readers are strongly encouraged to go through section 31.1 of the [Fast N-Body Simulation with CUDA](https://developer.nvidia.com/gpugems/gpugems3/part-v-physics-simulation/chapter-31-fast-n-body-simulation-cuda) webpage for additional context on why we care about speeding up the N-body simulation.\n",
    "\n",
    "### Numerical setup\n",
    "\n",
    "The N-body simulation in this tutorial evolves a system of particles interacting with each other through gravity. Each particle exerts an attractive gravitational force on every other particle in the system. For a particle given by index $i$ and mass $m_i$, we approximate the net force on the particle $\\mathbf{F}_i$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}_i \\approx G m_i \\sum_{j=1}^{N} \\frac{ m_j \\mathbf{r}_{ij} }{ \\left(\\mathbf{r}_{ij} \\cdot \\mathbf{r}_{ij}  + \\epsilon^2 \\right)^{3/2} },\n",
    "$$\n",
    "where $\\mathbf{r}_{ij} = \\mathbf{x}_j - \\mathbf{x}_i$, $\\mathbf{x}_j$ and $\\mathbf{x}_i$ are the position vectors of particles indexed $j$ and $i$, respectively. $\\epsilon$ is a *softening factor* to prevent numerical instabilities when two particles are very close to each other. For convenience and without any loss of generality, we set $G=1$ and do not explicitly use it in the code.\n",
    "\n",
    "\n",
    "### How to calculate the position of any particle in the system as a function of time?\n",
    "\n",
    "1. For any particle $i$, we can calculate its acceleration at a given time $t_n$ as:\n",
    "$$\n",
    "\\mathbf{a}_i^{n} = \\frac{\\mathbf{F}_i^{n}}{m_{i}} = G \\sum_{j=1}^{N} \\frac{ m_j \\mathbf{r}_{ij} }{ \\left(\\mathbf{r}_{ij} \\cdot \\mathbf{r}_{ij}  + \\epsilon^2 \\right)^{3/2} }.\n",
    "$$\n",
    "\n",
    "In the formula above and subsequent formulae, superscript $n$ refers to the values of variables (forces, positions, accelerations, velocities) at time $t_n$.\n",
    "\n",
    "2. We obtain the velocity of the given particle at time $t_{n+1}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i^{n+1} = \\mathbf{v}_i^{n} + \\mathbf{a}_i^{n}\\Delta t.\n",
    "$$\n",
    "\n",
    "3. Thereafter, the position of the particle is updated at time $t_{n+1}$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^{n+1} = \\mathbf{x}_i^{n} + \\mathbf{v}_i^{n+1}\\Delta t.\n",
    "$$\n",
    "\n",
    "Interested readers can also take a look at the appendix of [Swope et al. 1981](https://apps.dtic.mil/sti/tr/pdf/ADA103095.pdf) (page 53 onward).\n",
    "\n",
    "**Exercise**: Starting with equations 1 to 3, show that the above-mentioned method can equivalently be expressed as\n",
    "$$\n",
    "\\frac{d^2 \\mathbf{x}_i}{d t^{2}} = \\mathbf{a}_i \\approx  \\frac{\\mathbf{x}_i^{n+1} -2 \\mathbf{x}_{i}^n + \\mathbf{x}_{i}^{n-1}}{\\Delta t^{2}} = \\mathbf{a}_i^{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we will first implement a baseline simulation in Warp using a single instruction, multiple threads (SIMT) approach. We will then benchmark the SIMT code and delve into a tile-based paradigm for an even more performant code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49981536",
   "metadata": {},
   "source": [
    "---\n",
    "## A baseline SIMT implementation\n",
    "\n",
    "The baseline implementation consists of mainly these N steps:\n",
    "\n",
    "1. Initialize the N-body simulation parameters and arrays using `def initi_problem (num_bodies)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38354bda",
   "metadata": {},
   "source": [
    "### Initializing the simulation using NumPy and Warp\n",
    "\n",
    "The cell below implements the `def init_problem(num_bodies)` function to initialize positions and velocities of particles using NumPy. The following arrays are allocated on the GPU:\n",
    "\n",
    "- **Position arrays**: We use two position arrays to avoid race conditions in the Warp kernel. These arrays are swapped at each time step.\n",
    "  - Data type: Three-component float vector (`wp.vec3`)\n",
    "  - Shape: `(N,)`\n",
    "  - Initial values: Random positions uniformly distributed in space\n",
    "\n",
    "- **Velocity array**: Stores the velocity of each particle.\n",
    "  - Data type: Three-component float vector (`wp.vec3`)\n",
    "  - Shape: `(N,)`\n",
    "  - Initial values: Random velocities uniformly distributed in [-1, 1]\n",
    "\n",
    "- **Mass array**: While not strictly necessary for this basic example, including masses makes it easier to extend the code to handle variable particle masses.\n",
    "  - Data type: Float (`wp.float32`)\n",
    "  - Shape: `(N,)`\n",
    "  - Initial values: 1.0 (unit mass for all particles)\n",
    "\n",
    "For convenience, we first initialize the arrays on the CPU using NumPy, then transfer them to Warp arrays on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a27f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_problem(num_bodies):\n",
    "    \"\"\"Initialize N-body simulation parameters and arrays.\n",
    "\n",
    "    Creates initial positions, velocities, and masses for a system of particles\n",
    "    that will be used in the N-body gravitational simulation. Positions are\n",
    "    uniformly distributed in a cube with size scaled by particle count to\n",
    "    maintain constant density.\n",
    "\n",
    "    Args:\n",
    "        num_bodies (int): Number of particles in the simulation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pos_array_0 (wp.array): Initial position array on GPU (shape: [num_bodies, 3]).\n",
    "            - pos_array_1 (wp.array): Empty position array for double buffering.\n",
    "            - vel_array (wp.array): Initial velocity array on GPU (shape: [num_bodies, 3]).\n",
    "            - mass_array (wp.array): Mass array on GPU (all particles have unit mass).\n",
    "            - scale (float): Scale factor for the simulation domain.\n",
    "            - init_pos_np (np.ndarray): Initial positions on CPU for visualization.\n",
    "\n",
    "    Note:\n",
    "        Uses a fixed random seed (42) for reproducible results.\n",
    "    \"\"\"\n",
    "    # Initialize random number generator with fixed seed for reproducibility.\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # Calculate scale factor to maintain constant particle density as N increases.\n",
    "    # The cube root scaling ensures constant density in 3D space.\n",
    "    scale = 10.0 * (num_bodies / 1024) ** (1 / 3)\n",
    "\n",
    "    # Generate initial conditions on CPU using NumPy.\n",
    "    # Positions are uniformly distributed within a cube of size [-scale, scale]^3.\n",
    "    init_pos_np = rng.uniform(low=-scale, high=scale, size=(num_bodies, 3))\n",
    "\n",
    "    # Velocities are uniformly distributed in [-1, 1]^3 for initial random motion.\n",
    "    init_vel_np = rng.uniform(low=-1.0, high=1.0, size=(num_bodies, 3))\n",
    "\n",
    "    # All particles have unit mass for simplicity.\n",
    "    mass_array_np = np.ones(num_bodies)\n",
    "\n",
    "    # Transfer data from CPU (NumPy) to GPU (Warp).\n",
    "    pos_array_0 = wp.array(init_pos_np, dtype=wp.vec3)\n",
    "\n",
    "    # Create second position array for double buffering technique.\n",
    "    # This avoids race conditions when updating positions in parallel.\n",
    "    pos_array_1 = wp.empty_like(pos_array_0)\n",
    "\n",
    "    # Transfer velocity and mass data to GPU.\n",
    "    vel_array = wp.array(init_vel_np, dtype=wp.vec3)\n",
    "    mass_array = wp.array(mass_array_np, dtype=wp.float32)\n",
    "\n",
    "    return pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now set the key simulation parameters for the problem at hand.\n",
    "\n",
    "- `dt`: Timestep for the numerical setup\n",
    "- `num_bodies`: Total number of bodies we are interested in simulating\n",
    "- `SOFTENING_SQ`: Softening term to avoid blow-up in $\\mathbf{F}_i$ if two particles are very close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "num_bodies = 1024\n",
    "SOFTENING_SQ = 0.1**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e8b52",
   "metadata": {},
   "source": [
    "Let us know initialize the simulation and visualize the initial distribution of particles in the 3-D cartesian coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "# Call to initialize the position, velocity and mass arrays\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above initializes the position, velocity, and mass arrays for particles for a given number of `num_bodies` particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the initial positions are set, let us visualize the system before moving to the implementation of the numerical scheme. Based on how initial position is set in `def init_problem(num_bodies)`, we should visually see an uniform distribution of particles in a cubical box of length 20 with centered at origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a dark background\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Create a scatter plot\n",
    "scatter_plot = ax.scatter(\n",
    "    init_pos_np[:, 0],\n",
    "    init_pos_np[:, 1],\n",
    "    init_pos_np[:, 2],\n",
    "    c=\"#76b900\",\n",
    "    s=50,  # Set point size\n",
    "    edgecolors=\"black\",  # Black edges for contrast\n",
    "    linewidth=0.5,  # Thin edge lines\n",
    "    marker=\"o\",  # Round markers\n",
    ")\n",
    "\n",
    "# Set axis limits with some padding\n",
    "padding = scale * 0.1\n",
    "ax.set_xlim(-scale - padding, scale + padding)\n",
    "ax.set_ylim(-scale - padding, scale + padding)\n",
    "ax.set_zlim(-scale - padding, scale + padding)\n",
    "\n",
    "# Customize the grid\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Set a viewing angle\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "# Add pane colors for depth perception\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "\n",
    "# Customize tick labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "# Add a subtle box around the plot region\n",
    "ax.set_box_aspect([1, 1, 1])  # Equal aspect ratio\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warp kernel for timestepping and updating the positions of the particles\n",
    "\n",
    "Since we are working with the SIMT paradigm, it makes sense to launch a Warp kernel with a total number of threads equal to `num_bodies`. Each thread then takes care of updating the position of a single particle for a single timestep. We call this Warp kernel $N$ times, with each iteration from $0$ to $N$ corresponding to timesteps $t_1 = \\Delta t$ to $t_{N} = N\\Delta t$ (starting from $t_0=0$).\n",
    "\n",
    "The Warp kernel we are about to implement needs to perform the following steps on each thread for a single particle, given the positions and velocities at time $t_n$:\n",
    "\n",
    "- Calculate the acceleration of the particle $\\mathbf{a}_i^{n}$ at time $t_n$ using the formula described in the Numerical setup section.\n",
    "- Update the velocity of the particle: $\\mathbf{v}_{i}^{n+1} = \\mathbf{v}_{i}^{n} + \\mathbf{a}_{i}^{n}\\Delta t$.\n",
    "- Calculate the updated position: $\\mathbf{x}_{i}^{n+1} = \\mathbf{x}_{i}^{n} + \\mathbf{v}_{i}^{n+1}\\Delta t$.\n",
    "\n",
    "**Exercise**: Stop here and try to implement this Warp kernel and all the corresponding steps yourself. You can then compare your implementation with ours to see similarities and differences. Remember that the same algorithm can be implemented in different ways. In the final time-stepping loop, you can replace our kernel implementation with yours and observe whether the results change or remain similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below implements two key components: (a) a Warp function named `def body_body_interaction(...)` that calculates the net gravitational acceleration of a particle, and (b) a Warp kernel named `def integrate_bodies(...)` that implements the time-stepping algorithm described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.func\n",
    "def body_body_interaction(\n",
    "    num_bodies: int,\n",
    "    masses: wp.array(dtype=float),\n",
    "    body_position: wp.vec3,\n",
    "    positions: wp.array(dtype=wp.vec3),\n",
    "):\n",
    "    \"\"\"Calculate gravitational acceleration on a particle due to all other particles.\n",
    "\n",
    "    Computes the net gravitational acceleration on a particle at the given position\n",
    "    by summing the gravitational forces from all other particles in the system.\n",
    "    Uses a softening factor to prevent numerical instabilities when particles are\n",
    "    very close together.\n",
    "\n",
    "    Args:\n",
    "        num_bodies: Total number of particles in the system.\n",
    "        masses: Array of particle masses indexed by particle ID.\n",
    "        body_position: Position vector of the particle for which acceleration is computed.\n",
    "        positions: Array of position vectors for all particles in the system.\n",
    "\n",
    "    Returns:\n",
    "        wp.vec3: Net acceleration vector acting on the particle.\n",
    "\n",
    "    Note:\n",
    "        The acceleration is computed using Newton's law of gravitation with G=1:\n",
    "        a_i = Σ_j (m_j * r_ij / |r_ij + ε²|^(3/2))\n",
    "        where r_ij is the vector from particle i to particle j, and ε is the\n",
    "        softening factor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize acceleration vector to zero.\n",
    "    acc = wp.vec3(0.0, 0.0, 0.0)\n",
    "\n",
    "    # Sum gravitational contributions from all other particles.\n",
    "    for body_index in range(num_bodies):\n",
    "        # Calculate displacement vector from current particle to other particle.\n",
    "        r = positions[body_index] - body_position\n",
    "\n",
    "        # Add softening factor to squared distance to avoid singularities.\n",
    "        # This prevents blow-up when two particles get close to each other.\n",
    "        dist_sq = wp.length_sq(r) + SOFTENING_SQ\n",
    "\n",
    "        # Calculate inverse distance cubed for gravitational force formula.\n",
    "        inv_dist = 1.0 / wp.sqrt(dist_sq)\n",
    "        inv_dist_cubed = inv_dist * inv_dist * inv_dist\n",
    "\n",
    "        # Accumulate acceleration contribution from this particle.\n",
    "        # Force is proportional to mass and inverse square of distance.\n",
    "        acc = acc + masses[body_index] * inv_dist_cubed * r\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def integrate_bodies(\n",
    "    num_bodies: int,\n",
    "    dt: float,\n",
    "    masses: wp.array(dtype=float),\n",
    "    old_position: wp.array(dtype=wp.vec3),\n",
    "    velocity: wp.array(dtype=wp.vec3),\n",
    "    new_position: wp.array(dtype=wp.vec3),\n",
    "):\n",
    "    \"\"\"Integrate N-body system forward one timestep.\n",
    "\n",
    "    Updates particle velocities and positions for one timestep using the\n",
    "    gravitational forces between all particles. Each thread handles one particle,\n",
    "    computing its acceleration, updating its velocity, and calculating its new\n",
    "    position.\n",
    "\n",
    "    Args:\n",
    "        num_bodies: Total number of particles in the system.\n",
    "        dt: Timestep size for integration.\n",
    "        masses: Array of particle masses (shape: [num_bodies]).\n",
    "        old_position: Current particle positions (shape: [num_bodies]).\n",
    "        velocity: Current particle velocities, updated in-place (shape: [num_bodies]).\n",
    "        new_position: Output array for updated positions (shape: [num_bodies]).\n",
    "\n",
    "    Note:\n",
    "        This kernel uses the SIMT paradigm where each thread processes one particle.\n",
    "        The kernel should be launched with num_bodies threads.\n",
    "\n",
    "        Integration scheme:\n",
    "        1. a_i(t) = F_i(t) / m_i  (compute acceleration)\n",
    "        2. v_i(t+dt) = v_i(t) + a_i(t) * dt  (update velocity)\n",
    "        3. x_i(t+dt) = x_i(t) + v_i(t+dt) * dt  (update position)\n",
    "\n",
    "        Double buffering is used for positions to avoid race conditions during\n",
    "        parallel updates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get thread ID - each thread handles one particle.\n",
    "    i = wp.tid()\n",
    "\n",
    "    # Calculate gravitational acceleration on this particle from all others.\n",
    "    accel = body_body_interaction(num_bodies, masses, old_position[i], old_position)\n",
    "\n",
    "    # Update velocity using forward Euler integration.\n",
    "    # v(t+dt) = v(t) + a(t) * dt\n",
    "    velocity[i] = velocity[i] + accel * dt\n",
    "\n",
    "    # Update position using the newly computed velocity.\n",
    "    # x(t+dt) = x(t) + v(t+dt) * dt\n",
    "    # Note: We use new_position array to avoid race conditions.\n",
    "    new_position[i] = old_position[i] + dt * velocity[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warp kernel launch and timestepping\n",
    "\n",
    "At this point, we have finished writing the only Warp kernel we need for this simulation.\n",
    "\n",
    "The remaining task is to write a loop that repeatedly launches the Warp kernel and swaps the position arrays for the next iteration so that the updated positions become the current positions.\n",
    "\n",
    "For the latter task, we can use this trick:\n",
    "\n",
    "```python\n",
    "(pos_array_0, pos_array_1) = (pos_array_1, pos_array_0)\n",
    "```\n",
    "If you worked through the 2-D Ising model in Warp notebook, you would remember that the above trick was used there as well but for updating the lattice site spins as the Monte Carlo simulation progressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below performs the steps mentioned above. We re-initialize the inital state of the system using `def init_problem(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 1024  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100  # Total number of timesteps for which we run the simulation\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Main simulation loop that integrates particle motion over time\n",
    "for _step_index in range(T):\n",
    "    # Launch GPU kernel with one thread per particle\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    # Swap position buffers for next iteration (double buffering)\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animating the simulation\n",
    "\n",
    "The cell below contains the boilerplate code for animating the N-body simulation we have built. Let's observe how the simulation evolves over time. Note that the Warp kernel call for each timestep is wrapped inside the `def update_plot(...)` function, which calls the `def integrate()` function. This function, in turn, launches the `integrate_bodies` kernel and swaps the arrays for each animation frame (representing a single timestep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "\n",
    "# Create an integrate function that launches the integrate_bodies kernel and swap arrays\n",
    "def integrate():\n",
    "    global pos_array_0, pos_array_1\n",
    "\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "\n",
    "    # Swap arrays\n",
    "    (pos_array_0, pos_array_1) = (pos_array_1, pos_array_0)\n",
    "\n",
    "\n",
    "# Create figure and plot setup\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "scatter_plot = ax.scatter(\n",
    "    init_pos_np[:, 0],\n",
    "    init_pos_np[:, 1],\n",
    "    init_pos_np[:, 2],\n",
    "    c=\"#76b900\",\n",
    "    s=50,\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "# Set axis limits\n",
    "padding = scale * 0.1\n",
    "ax.set_xlim(-scale - padding, scale + padding)\n",
    "ax.set_ylim(-scale - padding, scale + padding)\n",
    "ax.set_zlim(-scale - padding, scale + padding)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "\n",
    "# Update function (like the working solution)\n",
    "def update_plot(frame):\n",
    "    integrate()  # Call the separate integrate function\n",
    "\n",
    "    # Copy updated positions to CPU for visualization\n",
    "    positions_cpu = pos_array_0.numpy()\n",
    "\n",
    "    # Update scatter plot positions\n",
    "    scatter_plot._offsets3d = (\n",
    "        positions_cpu[:, 0],\n",
    "        positions_cpu[:, 1],\n",
    "        positions_cpu[:, 2],\n",
    "    )\n",
    "\n",
    "    return (scatter_plot,)\n",
    "\n",
    "\n",
    "# Create animation (match working solution's parameters)\n",
    "anim = FuncAnimation(\n",
    "    fig,\n",
    "    update_plot,\n",
    "    frames=range(1000),  # Use range() like working solution\n",
    "    interval=10,\n",
    "    blit=True,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79595acc",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmarking the SIMT Code\n",
    "\n",
    "Now that we have established our baseline GPU implementation, let us define a performance metric and benchmark our SIMT implementation. We can choose our performance metric to be the **number of particle (pair-wise) interactions processed per second** ($\\mathrm{PIPS}$). This can be defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{PIPS} = \\frac{N^{2} \\times T}{\\text{Time taken for T timesteps}},\n",
    "$$\n",
    "\n",
    "where $N$ and $T$ are the number of particles in the system and the number of timesteps, respectively.\n",
    "\n",
    "In the cells below, we will explore three different ways of obtaining the $\\textbf{time taken for T timesteps}$:\n",
    "\n",
    "- Method 1 - Use the `time` module's `perf_counter()` or `perf_counter_ns()` functions\n",
    "- Method 2 - Use CUDA events in Warp\n",
    "- Method 3 - Use `wp.ScopedTimer()` as a context object. We can access the `.elapsed` attribute outside the context manager to get the elapsed time in **milliseconds**.\n",
    "\n",
    "Let's go through each of these methods step by step and see what we get for our SIMT code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad518b01",
   "metadata": {},
   "source": [
    "##### Method 1 - Use the `time` module's `perf_counter()` or `perf_counter_ns()` functions\n",
    "\n",
    "We will use the same code from our previous N-body simulation with minor modifications to add timing measurements using `time.perf_counter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 4 * 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Warmup loop for 5 timesteps\n",
    "for _warmup_index in range(5):\n",
    "    # Launch GPU kernel with one thread per particle\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    # Swap position buffers for next iteration (double buffering)\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0\n",
    "\n",
    "\n",
    "# Main simulation loop that integrates particle motion over time\n",
    "start_time = time.perf_counter()  # Start the counter\n",
    "for _step_index in range(T):\n",
    "    # Launch GPU kernel with one thread per particle\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    # Swap position buffers for next iteration (double buffering)\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0\n",
    "\n",
    "time_taken = time.perf_counter() - start_time  # Record the time taken\n",
    "print(f\"Time taken for {T} timesteps: {time_taken} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / time_taken / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 2 - Use CUDA events in Warp\n",
    "\n",
    "Events can be created with the `wp.Event(enable_timing=True)` argument to indicate that they will be used to calculate elapsed times.\n",
    "\n",
    "Events must be recorded onto the stream of operations using `wp.record_event()`.\n",
    "\n",
    "After both events have been recorded, we can use `wp.get_event_elapsed_time(start_event, end_event)` to return the elapsed time between the events in **milliseconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 4 * 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Create events to record the start and end times of the simulation\n",
    "start_event = wp.Event(enable_timing=True)\n",
    "end_event = wp.Event(enable_timing=True)\n",
    "\n",
    "# Warmup loop for 5 timesteps\n",
    "for _warmup_index in range(5):\n",
    "    # Launch GPU kernel with one thread per particle\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    # Swap position buffers for next iteration (double buffering)\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0\n",
    "\n",
    "\n",
    "wp.record_event(start_event)  # Record the start time\n",
    "\n",
    "# Main simulation loop that integrates particle motion over time\n",
    "for _step_index in range(T):\n",
    "    # Launch GPU kernel with one thread per particle\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    # Swap position buffers for next iteration (double buffering)\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0\n",
    "\n",
    "wp.record_event(end_event)  # Record the end time\n",
    "\n",
    "duration_seconds = (\n",
    "    wp.get_event_elapsed_time(start_event, end_event) * 1e-3\n",
    ")  # Calculate the elapsed time in seconds\n",
    "\n",
    "print(f\"Time taken for {T} timesteps: {duration_seconds} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / duration_seconds / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 3: Use `wp.ScopedTimer()` as a context object\n",
    "\n",
    "⚠️Caution⚠️: The `ScopedTimer` does not synchronize the host (CPU) with the device (GPU) by default. We must pass in `synchronize=True` for the `ScopedTimer` to synchronize before and after the timed section of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 4 * 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Warmup loop for 5 timesteps\n",
    "for _warmup_index in range(5):\n",
    "    # Launch GPU kernel with one thread per particle\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    # Swap position buffers for next iteration (double buffering)\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0\n",
    "\n",
    "with wp.ScopedTimer(\n",
    "    \"Integration\", synchronize=True\n",
    ") as timer:  # Create a scoped timer with label \"Integration\"\n",
    "    # Main simulation loop that integrates particle motion over time\n",
    "    for _step_index in range(T):\n",
    "        # Launch GPU kernel with one thread per particle\n",
    "        wp.launch(\n",
    "            integrate_bodies,\n",
    "            dim=(num_bodies,),  # Launch num_bodies threads\n",
    "            inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "        )\n",
    "        # Swap position buffers for next iteration (double buffering)\n",
    "        pos_array_0, pos_array_1 = pos_array_1, pos_array_0\n",
    "\n",
    "\n",
    "duration_seconds = timer.elapsed * 1e-3\n",
    "\n",
    "print(f\"Time taken for {T} timesteps: {duration_seconds} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / duration_seconds / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the PIPS values from methods 1, 2, and 3, you will notice that method 1 produces an unusually large result. This occurs because CUDA kernel launches are asynchronous with respect to the host CPU thread. When a kernel is launched using `wp.launch()`, the function schedules the kernel for execution on the GPU device but returns immediately to the CPU without waiting for the kernel to complete execution.\n",
    "\n",
    "Consequently, when using `time.perf_counter()` to measure execution time, the recorded duration is incorrectly measured as approximately 0.001 seconds, even though the actual kernel execution takes much longer. This leads to an artificially inflated PIPS calculation.\n",
    "\n",
    "In contrast, methods 2 and 3 use `wp.Event` and `wp.ScopedTimer` respectively, which properly synchronize with the GPU device and accurately measure the actual execution time of the kernel operations on the device.\n",
    "\n",
    "Interested readers can read more about concurrency, streams, and synchronization in Warp [here](https://nvidia.github.io/warp/modules/concurrency.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a78493",
   "metadata": {},
   "source": [
    "---\n",
    "## Tile-based programming approach\n",
    "\n",
    "### How can we make our SIMT method even faster?\n",
    "\n",
    "If you analyze the `body_body_interaction(...)` function in our SIMT code above, you will notice that it iterates over all particles to calculate the resulting acceleration for any given particle. Each time the contribution from a particle is calculated, the code must load both the positions (`positions[body_index]`) and masses (`masses[body_index]`) from global memory. \n",
    "\n",
    "One way to reduce this memory access overhead is to load a predefined chunk of masses and positions into shared memory within a thread block, making this data accessible to all threads in the block. With a chunk of particle data now in the faster shared memory, we can calculate acceleration contributions for this chunk much more efficiently than our previous approach, which required loading each individual particle's mass and position from the slower global memory for every computation.\n",
    "\n",
    "Starting from Warp 1.5.0, developers have access to tile-based programming in NVIDIA Warp, which achieves this goal of cooperative operations across threads in a block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick introduction to tile-based programming in Warp\n",
    "\n",
    "Before getting into details of how we can accelerate our N-body simulation using tile-based programming, we will quickly cover a few basics of tile-based programming in Warp using an example of row-wise summation of a matrix. Please also refer to this **https://nvidia.github.io/warp/modules/tiles.html** for a more comprehensive introduction to tile-based programming in Warp.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input matrix is of size $(12, 256)$ with each row $i$ containing an identical entry of value $i$ across all columns. The code snippet below initializes our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_np.shape=(12, 256)\n",
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  1. ...  1.  1.  1.]\n",
      " [ 2.  2.  2. ...  2.  2.  2.]\n",
      " ...\n",
      " [ 9.  9.  9. ...  9.  9.  9.]\n",
      " [10. 10. 10. ... 10. 10. 10.]\n",
      " [11. 11. 11. ... 11. 11. 11.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "N = 256 # Number of columns in our matrix\n",
    "column_vector = np.arange(12, dtype=np.float32).reshape(-1, 1) # Create a column vector of size 12 [0, 1, 2, ..., 11]\n",
    "a_np = np.tile(column_vector, (1, N)) # Tile the column vector 256 times to create a matrix of size (12, 256)\n",
    "a_wp = wp.array(a_np) # Convert our numpy array to a Warp array\n",
    "\n",
    "print(f\"{a_np.shape=}\")\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our row-wise summation should be $[0, 256, 512, \\cdots, 2816]$. Can you confirm this yourself before moving ahead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the tile-based method to do the row-wise summation of our matrix above, can you try to write the SIMT code for the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.  256.  512.  768. 1024. 1280. 1536. 1792. 2048. 2304. 2560. 2816.]\n"
     ]
    }
   ],
   "source": [
    "@wp.kernel\n",
    "def compute(a: wp.array2d(dtype=float), b: wp.array(dtype=float)):\n",
    "    \"\"\"\n",
    "    This kernel computes the row-wise sum of the input matrix a and stores the result in the output array b\n",
    "    \"\"\"\n",
    "    i = wp.tid()  # Obtain the row index\n",
    "\n",
    "    # Sum across the elements of the row\n",
    "    sum = wp.float32(0.0)\n",
    "    for j in range(a.shape[1]):\n",
    "        sum += a[i, j] # Each thread i loads the matrix entry a[i,j] from the global memory\n",
    "\n",
    "    # Store the result in b[i]\n",
    "    b[i] = sum\n",
    "\n",
    "\n",
    "b_wp = wp.empty(a_wp.shape[0], dtype=float) # Creaty an empty Warp array of size 12 to store our result\n",
    "\n",
    "wp.launch(compute, dim=(a_wp.shape[0],), inputs=[a_wp], outputs=[b_wp]) # Launch the kernel with total of 12 threads, one for each row in our matrix\n",
    "    \n",
    "print(b_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tiles, we can use **all threads in a block** to cooperatively load an entire row of data into a tile object and call `wp.tile_sum()` on the tile.\n",
    "\n",
    "Note that in the `compute_tiled` kernel below, we primarily call functions beginning with `wp.tile_`.\n",
    "\n",
    "`wp.launch_tiled()` is also used instead of `wp.launch()`, but all `wp.launch_tiled()` does is to add a trailing dimension of `BLOCK_DIM` to the launch grid. Hence, the following are equivalent:\n",
    "\n",
    "- `wp.launch_tiled(compute, dim=(a_wp.shape[0],), inputs=[a_wp], outputs=[b_wp], block_dim=BLOCK_DIM)`\n",
    "- `wp.launch(compute, dim=(a_wp.shape[0], BLOCK_DIM), inputs=[a_wp], outputs=[b_wp], block_dim=BLOCK_DIM)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.]\n",
      " [ 256.]\n",
      " [ 512.]\n",
      " [ 768.]\n",
      " [1024.]\n",
      " [1280.]\n",
      " [1536.]\n",
      " [1792.]\n",
      " [2048.]\n",
      " [2304.]\n",
      " [2560.]\n",
      " [2816.]]\n"
     ]
    }
   ],
   "source": [
    "# Both TILE_SIZE and BLOCK_DIM needs to be run-time constants\n",
    "TILE_SIZE = 256  # Size of a single tile of data to laaded into either the private registers of each thread in the block or the shared memory of the block\n",
    "BLOCK_DIM = 256  # Number of threads in a block\n",
    "\n",
    "@wp.kernel\n",
    "def compute_tiled(a: wp.array2d(dtype=float), b: wp.array2d(dtype=float)):\n",
    "    i = wp.tid()  # Obtain the row index\n",
    "\n",
    "    # Load a entire row aka tile from global memory using all threads in the block\n",
    "    t = wp.tile_load(a[i], TILE_SIZE,)\n",
    "\n",
    "    # Cooperatively compute the sum of the tile elements; s is a single-element tile\n",
    "    s = wp.tile_sum(t)\n",
    "\n",
    "    # store s in global memory\n",
    "    wp.tile_store(b[i], s)\n",
    "\n",
    "\n",
    "# Same as before, but now we create an empty Warp array of shape (12,1) since wp.tile_sum returns a single element tile\n",
    "b_wp = wp.empty((a_wp.shape[0], 1), dtype=float) \n",
    "\n",
    "# Kernel launch using wp.launch_tiled() with each block being of size BLOCK_DIM\n",
    "wp.launch_tiled(compute_tiled, dim=(a_wp.shape[0],), inputs=[a_wp], outputs=[b_wp], block_dim=BLOCK_DIM)\n",
    "\n",
    "print(b_wp) #Should be equivalent to our SIMT result, except b_wp being of size (12,1) instead of (12,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the `compute_tiled` kernel above, when the code calls `t = wp.tile_load(a[i], TILE_SIZE)`, it triggers a **coordinated memory operation** performed by all threads within the current CUDA thread block (which contains `BLOCK_DIM` threads).\n",
    " \n",
    "The objective is to load `TILE_SIZE` elements from array `a`, starting at global memory index `i`, directly into the **private registers** of these threads.\n",
    " \n",
    "Each thread in the block is responsible for loading a specific portion of the `TILE_SIZE` elements.\n",
    " \n",
    "- When `TILE_SIZE=256` and `BLOCK_DIM=256`, each thread loads exactly one element from `a` into its register.\n",
    "- When `TILE_SIZE=256` and `BLOCK_DIM=64`, each thread loads four elements from `a` into its register.\n",
    "- When `TILE_SIZE=256` and `BLOCK_DIM=512`, only the first 256 threads load one element each from `a` into their registers, while the remaining 256 threads remain idle.\n",
    " \n",
    "Since `TILE_SIZE` elements must ultimately be loaded, the work distribution depends on `BLOCK_DIM`. If `TILE_SIZE >= BLOCK_DIM`, each thread loads `TILE_SIZE/BLOCK_DIM` elements. Otherwise, only the first `TILE_SIZE` threads load one element each, while the remaining threads load nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e8a98f7",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we examined different attempts to simulate the Ising model in two dimensions.\n",
    "\n",
    "The main goal of this exercise was to illuminate some considerations when writing algorithms that run **correctly** and **efficiently** on a GPU.\n",
    "\n",
    "Starting from a sequential Python version, we encountered a fundamental challenge: while separating the lattice into \"current\" and \"updated\" grids eliminates race conditions, this approach inadvertently alters the underlying physics, producing an algorithm that no longer faithfully reproduces the statistical mechanics of the original sequential implementation.\n",
    "\n",
    "Finally, we saw that a checkerboard algorithm allows us to recover the intended physics while addressing the data dependency issues that hindered the initial approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0df8d",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "You can find the Warp GitHub repository and documentation below:\n",
    "* \"NVIDIA/warp: A Python framework for accelerated simulation, data generation and spatial computing.\", GitHub, https://github.com/NVIDIA/warp.\n",
    "* Warp Developers, \"NVIDIA Warp Documentation,\" GitHub Pages, https://nvidia.github.io/warp.\n",
    "\n",
    "Note: This tutorial was inspired by [Romero et al.'s](https://www.sciencedirect.com/science/article/abs/pii/S0010465520302228) work on GPU-accelerated 2-D Ising model simulations."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "warp-cfd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
