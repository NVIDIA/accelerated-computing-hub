{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3cebfbd2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41de3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Chapter 12.2: N-body Simulation using Single Instruction, Multiple Threads (SIMT) and Tile-based Approach in Warp\n",
    "\n",
    "![Example output](images/chapter-12.2/nbody_animation.gif)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the previous two notebooks, we introduced NVIDIA Warp as a framework for writing high-performance GPU code in Python and implemented a GPU-accelerated 2-D Ising model simulation. Through this parallel GPU implementation, we learned the importance of developing correct parallelization algorithms to faithfully reproduce the underlying physics of any physical system that we are modeling.\n",
    "\n",
    "Following the theme of simulating physical systems to learn the basics of NVIDIA Warp for parallel computing, in this chapter we will implement an N-body simulation in NVIDIA Warp, first using the single instruction, multiple thread (SIMT) model and then using the tile-based model (both in Warp).\n",
    "\n",
    "Through this implementation, you will learn the following:\n",
    "\n",
    "* The simulation of another physical system using Warp, building on the previous notebook on the 2-D Ising model\n",
    "* How to characterize code performance and use some code profiling tools available in Warp\n",
    "* An introduction to the tile-based programming method in Warp\n",
    "\n",
    "By the end of this chapter, you will have two variants of code for N-body simulation, one based on the SIMT paradigm and another based on the tile-based programming model. You will also have learned about tools available in Python and Warp to benchmark your code's performance in greater detail.\n",
    "\n",
    "It is strongly recommended to work through the [notebook on introduction to NVIDIA Warp](Chapter_12_Intro_to_NVIDIA_Warp.ipynb) and [GPU-Accelerated Ising Model Simulation in NVIDIA Warp](Chapter_12.1_IsingModel_In_Warp.ipynb) before proceeding with this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d431841",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Before we begin this tutorial, let us ensure we have all the necessary packages installed.\n",
    "\n",
    "First, we will install NVIDIA Warp if it is not already available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaea329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NVIDIA Warp\n",
    "%pip install warp-lang\n",
    "\n",
    "# Install Matplotlib for plotting/animation\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292eb53a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Now let us import the necessary libraries and initialize Warp to check if GPU support is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "# Check for GPU availability\n",
    "if wp.get_cuda_device_count() > 0:\n",
    "    print(\"✓ GPU detected successfully\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5750b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "## Introduction: Exploring avenues for further speedup of an already GPU-accelerated code\n",
    "\n",
    "In the last notebook, we went through an example of how to correctly model a physical system (2-D Ising model) using Warp.\n",
    "\n",
    "Now, we will go a step beyond the correctness of the model and explore how to profile our code and accelerate an already GPU-accelerated implementation.\n",
    "\n",
    "Similar to the 2-D Ising model, we choose the N-body simulation for its simplicity while also having underlying physics that makes it amenable to a tile-based programming model.\n",
    "\n",
    "Like the previous notebook, let us begin by understanding the physics behind the N-body simulation, then progressively build our GPU implementation, profile it, analyze potential avenues for further speedup, and finally explore the tile-based programming model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76672792",
   "metadata": {},
   "source": [
    "---\n",
    "## Background: The N-body simulation in physics and its numerical implementation \n",
    "\n",
    "### What is an N-body simulation?\n",
    "\n",
    "An [N-body simulation](https://en.wikipedia.org/wiki/N-body_simulation) models the temporal evolution of a system of particles, typically under the influence of physical forces such as gravity. N-body simulations are widely used in astrophysics to study the dynamics of systems of planets, stars, and galaxies. Readers are strongly encouraged to go through section 31.1 of the [Fast N-Body Simulation with CUDA](https://developer.nvidia.com/gpugems/gpugems3/part-v-physics-simulation/chapter-31-fast-n-body-simulation-cuda) webpage for additional context on why we care about achieving speedup for the N-body simulation.\n",
    "\n",
    "### Numerical setup\n",
    "\n",
    "The N-body simulation in this tutorial evolves a system of particles interacting with each other through gravity. Each particle exerts an attractive gravitational force on every other particle in the system. For a particle given by index $i$ and mass $m_i$, we approximate the net force on the particle $\\mathbf{F}_i$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}_i \\approx G m_i \\sum_{j=1}^{N} \\frac{ m_j \\mathbf{r}_{ij} }{ \\left(\\mathbf{r}_{ij} \\cdot \\mathbf{r}_{ij}  + \\epsilon^2 \\right)^{3/2} },\n",
    "$$\n",
    "where $\\mathbf{r}_{ij} = \\mathbf{x}_j - \\mathbf{x}_i$, $\\mathbf{x}_j$ and $\\mathbf{x}_i$ are the position vectors of particles indexed $j$ and $i$, respectively. $\\epsilon$ is a *softening factor* to prevent numerical instabilities when two particles are very close to each other. For convenience and without any loss of generality, we set $G=1$ and do not explicitly use it in the code.\n",
    "\n",
    "\n",
    "### How to calculate the position of any particle in the system as a function of time?\n",
    "\n",
    "1. For any particle $i$, we can calculate its acceleration at a given time $t_n$ as:\n",
    "$$\n",
    "\\mathbf{a}_i^{n} = \\frac{\\mathbf{F}_i^{n}}{m_{i}} = G \\sum_{j=1}^{N} \\frac{ m_j \\mathbf{r}_{ij} }{ \\left(\\mathbf{r}_{ij} \\cdot \\mathbf{r}_{ij}  + \\epsilon^2 \\right)^{3/2} }.\n",
    "$$\n",
    "\n",
    "In the formula above and subsequent formulae, superscript $n$ refers to the values of variables (forces, positions, accelerations, velocities) at time $t_n$.\n",
    "\n",
    "2. We obtain the velocity of the given particle at time $t_{n+1}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i^{n+1} = \\mathbf{v}_i^{n} + \\mathbf{a}_i^{n}\\Delta t.\n",
    "$$\n",
    "\n",
    "3. Thereafter, the position of the particle is updated at time $t_{n+1}$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^{n+1} = \\mathbf{x}_i^{n} + \\mathbf{v}_i^{n+1}\\Delta t.\n",
    "$$\n",
    "\n",
    "Interested readers can also take a look at the appendix of [Swope et al. 1982](https://apps.dtic.mil/sti/tr/pdf/ADA103095.pdf) (page 53 onward).\n",
    "\n",
    "**Exercise**: Starting with equations 1 to 3, show that the above-mentioned method can equivalently be expressed as\n",
    "$$\n",
    "\\frac{d^2 \\mathbf{x}_i}{d t^{2}} = \\mathbf{a}_i \\approx  \\frac{\\mathbf{x}_i^{n+1} -2 \\mathbf{x}_{i}^n + \\mathbf{x}_{i}^{n-1}}{\\Delta t^{2}} = \\mathbf{a}_i^{n}\n",
    "$$\n",
    "\n",
    "In the next sections, we will first implement a baseline simulation in Warp using a single instruction, multiple threads (SIMT) approach. We will then benchmark the SIMT code and delve into a tile-based paradigm for an even more performant code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49981536",
   "metadata": {},
   "source": [
    "---\n",
    "## A baseline SIMT implementation\n",
    "\n",
    "The baseline implementation consists mainly of these steps:\n",
    "\n",
    "1. Initialize the N-body simulation parameters and arrays using `def init_problem(num_bodies)` function.\n",
    "\n",
    "2. Write a Warp function for calculating the net gravitational force acting on any particle, called `def body_body_interaction(...)`.\n",
    "\n",
    "3. Write a Warp kernel to update the position of any particle in the system, called `def integrate_bodies(...)`.\n",
    "\n",
    "4. Putting all three components above together.\n",
    "\n",
    "In the subsequent cells, we will go through these steps one-by-one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38354bda",
   "metadata": {},
   "source": [
    "### Initializing the simulation using NumPy and Warp\n",
    "\n",
    "The cell below implements the `def init_problem(num_bodies)` function to initialize positions and velocities of particles using NumPy. The following arrays are allocated on the GPU:\n",
    "\n",
    "- **Position arrays**: We use two position arrays to avoid race conditions in the Warp kernel. These arrays are swapped at each timestep.\n",
    "  - Data type: Three-component float vector (`wp.vec3`)\n",
    "  - Shape: `(N,)`\n",
    "  - Initial values: Random positions uniformly distributed in space\n",
    "\n",
    "- **Velocity array**: Stores the velocity of each particle.\n",
    "  - Data type: Three-component float vector (`wp.vec3`)\n",
    "  - Shape: `(N,)`\n",
    "  - Initial values: Random velocities uniformly distributed in [-1, 1]\n",
    "\n",
    "- **Mass array**: While not strictly necessary for this basic example, including masses makes it easier to extend the code to handle variable particle masses.\n",
    "  - Data type: Float (`wp.float32`)\n",
    "  - Shape: `(N,)`\n",
    "  - Initial values: 1.0 (unit mass for all particles)\n",
    "\n",
    "For convenience, we first initialize the arrays on the CPU using NumPy, then transfer them to Warp arrays on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a27f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_problem(num_bodies):\n",
    "    \"\"\"Initialize N-body simulation parameters and arrays.\n",
    "\n",
    "    Creates initial positions, velocities, and masses for a system of particles\n",
    "    that will be used in the N-body gravitational simulation. Positions are\n",
    "    uniformly distributed in a cube with size scaled by particle count to\n",
    "    maintain constant density.\n",
    "\n",
    "    Args:\n",
    "        num_bodies (int): Number of particles in the simulation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pos_array_0 (wp.array): Initial position array on GPU (shape: [num_bodies, 3]).\n",
    "            - pos_array_1 (wp.array): Empty position array for double buffering.\n",
    "            - vel_array (wp.array): Initial velocity array on GPU (shape: [num_bodies, 3]).\n",
    "            - mass_array (wp.array): Mass array on GPU (all particles have unit mass).\n",
    "            - scale (float): Scale factor for the simulation domain.\n",
    "            - init_pos_np (np.ndarray): Initial positions on CPU for visualization.\n",
    "\n",
    "    Note:\n",
    "        Uses a fixed random seed (42) for reproducible results.\n",
    "    \"\"\"\n",
    "    # Initialize random number generator with fixed seed for reproducibility.\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # Calculate scale factor to maintain constant particle density as N increases.\n",
    "    # The cube root scaling ensures constant density in 3D space.\n",
    "    scale = 10.0 * (num_bodies / 1024) ** (1 / 3)\n",
    "\n",
    "    # Generate initial conditions on CPU using NumPy.\n",
    "    # Positions are uniformly distributed within a cube of size [-scale, scale]^3.\n",
    "    init_pos_np = rng.uniform(low=-scale, high=scale, size=(num_bodies, 3))\n",
    "\n",
    "    # Velocities are uniformly distributed in [-1, 1]^3 for initial random motion.\n",
    "    init_vel_np = rng.uniform(low=-1.0, high=1.0, size=(num_bodies, 3))\n",
    "\n",
    "    # All particles have unit mass for simplicity.\n",
    "    mass_array_np = np.ones(num_bodies)\n",
    "\n",
    "    # Transfer data from CPU (NumPy) to GPU (Warp).\n",
    "    pos_array_0 = wp.array(init_pos_np, dtype=wp.vec3)\n",
    "\n",
    "    # Create second position array for double buffering technique.\n",
    "    # This avoids race conditions when updating positions in parallel.\n",
    "    pos_array_1 = wp.empty_like(pos_array_0)\n",
    "\n",
    "    # Transfer velocity and mass data to GPU.\n",
    "    vel_array = wp.array(init_vel_np, dtype=wp.vec3)\n",
    "    mass_array = wp.array(mass_array_np, dtype=wp.float32)\n",
    "\n",
    "    return pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now set the key simulation parameters for the problem at hand.\n",
    "\n",
    "- `dt`: timestep for the numerical setup\n",
    "- `num_bodies`: Total number of bodies we are interested in simulating\n",
    "- `SOFTENING_SQ`: Softening term to avoid blowup in $\\mathbf{F}_i$ if two particles are very close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "num_bodies = 1024\n",
    "SOFTENING_SQ = 0.1**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e8b52",
   "metadata": {},
   "source": [
    "Let us now initialize the simulation and visualize the initial distribution of particles in the 3-D Cartesian coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "# Call to initialize the position, velocity and mass arrays\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above initializes the position, velocity, and mass arrays for particles for a given number of `num_bodies` particles.\n",
    "\n",
    "Once the initial positions are set, let us visualize the system before moving to the implementation of the numerical scheme. Based on how initial position is set in `def init_problem(num_bodies)`, we should visually see a uniform distribution of particles in a cubical box of length 20 centered at the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a dark background\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Create a scatter plot\n",
    "scatter_plot = ax.scatter(\n",
    "    init_pos_np[:, 0],\n",
    "    init_pos_np[:, 1],\n",
    "    init_pos_np[:, 2],\n",
    "    c=\"#76b900\",\n",
    "    s=50,  # Set point size\n",
    "    edgecolors=\"black\",  # Black edges for contrast\n",
    "    linewidth=0.5,  # Thin edge lines\n",
    "    marker=\"o\",  # Round markers\n",
    ")\n",
    "\n",
    "# Set axis limits with some padding\n",
    "padding = scale * 0.1\n",
    "ax.set_xlim(-scale - padding, scale + padding)\n",
    "ax.set_ylim(-scale - padding, scale + padding)\n",
    "ax.set_zlim(-scale - padding, scale + padding)\n",
    "\n",
    "# Customize the grid\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Set a viewing angle\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "# Add pane colors for depth perception\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "\n",
    "# Customize tick labels\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "# Add a subtle box around the plot region\n",
    "ax.set_box_aspect([1, 1, 1])  # Equal aspect ratio\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warp kernel for timestepping and updating the positions of the particles\n",
    "\n",
    "Since we are working with the SIMT paradigm, it makes sense to launch a Warp kernel with a total number of threads equal to `num_bodies`. Each thread then takes care of updating the position of a single particle for a single timestep. We call this Warp kernel $N$ times, with each iteration from $0$ to $N$ corresponding to timesteps $t_1 = \\Delta t$ to $t_{N} = N\\Delta t$ (starting from $t_0=0$).\n",
    "\n",
    "The Warp kernel we are about to implement needs to perform the following steps on each thread for a single particle, given the positions and velocities at time $t_n$:\n",
    "\n",
    "- Calculate the acceleration of the particle $\\mathbf{a}_i^{n}$ at time $t_n$ using the formula described in the Numerical setup section.\n",
    "- Update the velocity of the particle: $\\mathbf{v}_{i}^{n+1} = \\mathbf{v}_{i}^{n} + \\mathbf{a}_{i}^{n}\\Delta t$.\n",
    "- Calculate the updated position: $\\mathbf{x}_{i}^{n+1} = \\mathbf{x}_{i}^{n} + \\mathbf{v}_{i}^{n+1}\\Delta t$.\n",
    "\n",
    "**Exercise**: Stop here and try to implement this Warp kernel and all the corresponding steps yourself. You can then compare your implementation with ours to see similarities and differences. Remember that the same algorithm can be implemented in different ways. In the final timestepping loop, you can replace our kernel implementation with yours and observe whether the results change or remain similar.\n",
    "\n",
    "The cell below implements two key components: (a) a Warp function named `def body_body_interaction(...)` that calculates the net gravitational acceleration of a particle, and (b) a Warp kernel named `def integrate_bodies(...)` that implements the timestepping algorithm described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.func\n",
    "def body_body_interaction(\n",
    "    num_bodies: int,\n",
    "    masses: wp.array(dtype=float),\n",
    "    body_position: wp.vec3,\n",
    "    positions: wp.array(dtype=wp.vec3),\n",
    "):\n",
    "    \"\"\"Calculate gravitational acceleration on a particle due to all other particles.\n",
    "\n",
    "    Computes the net gravitational acceleration on a particle at the given position\n",
    "    by summing the gravitational forces from all other particles in the system.\n",
    "    Uses a softening factor to prevent numerical instabilities when particles are\n",
    "    very close together.\n",
    "\n",
    "    Args:\n",
    "        num_bodies: Total number of particles in the system.\n",
    "        masses: Array of particle masses indexed by particle ID.\n",
    "        body_position: Position vector of the particle for which acceleration is computed.\n",
    "        positions: Array of position vectors for all particles in the system.\n",
    "\n",
    "    Returns:\n",
    "        wp.vec3: Net acceleration vector acting on the particle.\n",
    "\n",
    "    Note:\n",
    "        The acceleration is computed using Newton's law of gravitation with G=1:\n",
    "        a_i = Σ_j (m_j * r_ij / |r_ij + ε²|^(3/2))\n",
    "        where r_ij is the vector from particle i to particle j, and ε is the\n",
    "        softening factor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize acceleration vector to zero.\n",
    "    acc = wp.vec3(0.0, 0.0, 0.0)\n",
    "\n",
    "    # Sum gravitational contributions from all other particles.\n",
    "    for body_index in range(num_bodies):\n",
    "        # Calculate displacement vector from current particle to other particle.\n",
    "        r = positions[body_index] - body_position\n",
    "\n",
    "        # Add softening factor to squared distance to avoid singularities.\n",
    "        # This prevents blowup when two particles get close to each other.\n",
    "        dist_sq = wp.length_sq(r) + SOFTENING_SQ\n",
    "\n",
    "        # Calculate inverse distance cubed for gravitational force formula.\n",
    "        inv_dist = 1.0 / wp.sqrt(dist_sq)\n",
    "        inv_dist_cubed = inv_dist * inv_dist * inv_dist\n",
    "\n",
    "        # Accumulate acceleration contribution from this particle.\n",
    "        # Force is proportional to mass and inverse square of distance.\n",
    "        acc = acc + masses[body_index] * inv_dist_cubed * r\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def integrate_bodies(\n",
    "    num_bodies: int,\n",
    "    dt: float,\n",
    "    masses: wp.array(dtype=float),\n",
    "    old_position: wp.array(dtype=wp.vec3),\n",
    "    velocity: wp.array(dtype=wp.vec3),\n",
    "    new_position: wp.array(dtype=wp.vec3),\n",
    "):\n",
    "    \"\"\"Integrate N-body system forward one timestep.\n",
    "\n",
    "    Updates particle velocities and positions for one timestep using the\n",
    "    gravitational forces between all particles. Each thread handles one particle,\n",
    "    computing its acceleration, updating its velocity, and calculating its new\n",
    "    position.\n",
    "\n",
    "    Args:\n",
    "        num_bodies: Total number of particles in the system.\n",
    "        dt: Timestep size for integration.\n",
    "        masses: Array of particle masses (shape: [num_bodies]).\n",
    "        old_position: Current particle positions (shape: [num_bodies]).\n",
    "        velocity: Current particle velocities, updated in-place (shape: [num_bodies]).\n",
    "        new_position: Output array for updated positions (shape: [num_bodies]).\n",
    "\n",
    "    Note:\n",
    "        This kernel uses the SIMT paradigm where each thread processes one particle.\n",
    "        The kernel should be launched with num_bodies threads.\n",
    "\n",
    "        Integration scheme:\n",
    "        1. a_i(t) = F_i(t) / m_i  (compute acceleration)\n",
    "        2. v_i(t+dt) = v_i(t) + a_i(t) * dt  (update velocity)\n",
    "        3. x_i(t+dt) = x_i(t) + v_i(t+dt) * dt  (update position)\n",
    "\n",
    "        Double buffering is used for positions to avoid race conditions during\n",
    "        parallel updates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get thread ID - each thread handles one particle.\n",
    "    i = wp.tid()\n",
    "\n",
    "    # Calculate gravitational acceleration on this particle from all others.\n",
    "    accel = body_body_interaction(num_bodies, masses, old_position[i], old_position)\n",
    "\n",
    "    # Update velocity using forward Euler integration.\n",
    "    # v(t+dt) = v(t) + a(t) * dt\n",
    "    velocity[i] = velocity[i] + accel * dt\n",
    "\n",
    "    # Update position using the newly computed velocity.\n",
    "    # x(t+dt) = x(t) + v(t+dt) * dt\n",
    "    # Note: We use new_position array to avoid race conditions.\n",
    "    new_position[i] = old_position[i] + dt * velocity[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warp kernel launch and timestepping\n",
    "\n",
    "At this point, we have finished writing the only Warp kernel we need for this simulation.\n",
    "\n",
    "The remaining task is to write a loop that repeatedly launches the Warp kernel and swaps the position arrays for the next iteration so that the updated positions become the current positions.\n",
    "\n",
    "For the latter task, we can use this trick:\n",
    "\n",
    "```python\n",
    "(pos_array_0, pos_array_1) = (pos_array_1, pos_array_0)\n",
    "```\n",
    "If you worked through the 2-D Ising model in Warp notebook, you would remember that the above trick was used there as well but for updating the lattice site spins as the Monte Carlo simulation progressed.\n",
    "\n",
    "The cell below performs the steps mentioned above. We re-initialize the initial state of the system using `def init_problem(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 1024  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100  # Total number of timesteps for which we run the simulation\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Main simulation loop\n",
    "for _step_index in range(T):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animating the simulation\n",
    "\n",
    "The cell below contains the boilerplate code for animating the N-body simulation we have built. Let's observe how the simulation evolves over time. Note that the Warp kernel call for each timestep is wrapped inside the `def update_plot(...)` function, which calls the `def integrate()` function. This function, in turn, launches the `integrate_bodies` kernel and swaps the arrays for each animation frame (representing a single timestep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "\n",
    "# Create an integrate function that launches the integrate_bodies kernel and swap arrays\n",
    "def integrate():\n",
    "    global pos_array_0, pos_array_1\n",
    "\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "\n",
    "    # Swap arrays\n",
    "    (pos_array_0, pos_array_1) = (pos_array_1, pos_array_0)\n",
    "\n",
    "\n",
    "# Create figure and plot setup\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "scatter_plot = ax.scatter(\n",
    "    init_pos_np[:, 0],\n",
    "    init_pos_np[:, 1],\n",
    "    init_pos_np[:, 2],\n",
    "    c=\"#76b900\",\n",
    "    s=50,\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "# Set axis limits\n",
    "padding = scale * 0.1\n",
    "ax.set_xlim(-scale - padding, scale + padding)\n",
    "ax.set_ylim(-scale - padding, scale + padding)\n",
    "ax.set_zlim(-scale - padding, scale + padding)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "\n",
    "# Update function (like the working solution)\n",
    "def update_plot(frame):\n",
    "    integrate()  # Call the separate integrate function\n",
    "\n",
    "    # Copy updated positions to CPU for visualization\n",
    "    positions_cpu = pos_array_0.numpy()\n",
    "\n",
    "    # Update scatter plot positions\n",
    "    scatter_plot._offsets3d = (\n",
    "        positions_cpu[:, 0],\n",
    "        positions_cpu[:, 1],\n",
    "        positions_cpu[:, 2],\n",
    "    )\n",
    "\n",
    "    return (scatter_plot,)\n",
    "\n",
    "\n",
    "# Create animation (match working solution's parameters)\n",
    "anim = FuncAnimation(\n",
    "    fig,\n",
    "    update_plot,\n",
    "    frames=range(1000),  # Use range() like working solution\n",
    "    interval=10,\n",
    "    blit=True,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79595acc",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmarking the SIMT code\n",
    "\n",
    "Now that we have established our baseline GPU implementation, let us define a performance metric and benchmark our SIMT implementation. We can choose our performance metric to be the **number of particle (pairwise) interactions processed per second** ($\\mathrm{PIPS}$). This can be defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{PIPS} = \\frac{N^{2} \\times T}{\\text{Time taken for T timesteps}},\n",
    "$$\n",
    "\n",
    "where $N$ and $T$ are the number of particles in the system and the number of timesteps, respectively.\n",
    "\n",
    "In the cells below, we will explore three different ways of obtaining the $\\textbf{time taken for T timesteps}$:\n",
    "\n",
    "- Method 1 - Use the `time` module's `perf_counter()` or `perf_counter_ns()` functions\n",
    "- Method 2 - Use CUDA events in Warp\n",
    "- Method 3 - Use `wp.ScopedTimer()` as a context object. We can access the `.elapsed` attribute outside the context manager to get the elapsed time in **milliseconds**.\n",
    "\n",
    "Let's go through each of these methods step by step and see what we get for our SIMT code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad518b01",
   "metadata": {},
   "source": [
    "### Method 1 - Use the `time` module's `perf_counter()` or `perf_counter_ns()` functions\n",
    "\n",
    "We will use the same code from our previous N-body simulation with minor modifications to add timing measurements using `time.perf_counter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Warmup (5 steps)\n",
    "for _warmup_index in range(5):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "\n",
    "# Main simulation loop\n",
    "start_time = time.perf_counter()\n",
    "for _step_index in range(T):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "time_taken = time.perf_counter() - start_time\n",
    "print(f\"Time taken for {T} timesteps: {time_taken} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / time_taken / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Use CUDA events in Warp\n",
    "\n",
    "Events can be created with the `wp.Event(enable_timing=True)` argument to indicate that they will be used to calculate elapsed times.\n",
    "\n",
    "Events must be recorded onto the stream of operations using `wp.record_event()`.\n",
    "\n",
    "After both events have been recorded, we can use `wp.get_event_elapsed_time(start_event, end_event)` to return the elapsed time between the events in **milliseconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Create events to record the start and end times of the simulation\n",
    "start_event = wp.Event(enable_timing=True)\n",
    "end_event = wp.Event(enable_timing=True)\n",
    "\n",
    "# Warmup (5 steps)\n",
    "for _warmup_index in range(5):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "\n",
    "wp.record_event(start_event)\n",
    "\n",
    "# Main simulation loop\n",
    "for _step_index in range(T):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "wp.record_event(end_event)\n",
    "\n",
    "duration_seconds = (\n",
    "    wp.get_event_elapsed_time(start_event, end_event) * 1e-3\n",
    ")  # Calculate the elapsed time in seconds\n",
    "\n",
    "print(f\"Time taken for {T} timesteps: {duration_seconds} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / duration_seconds / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Use `wp.ScopedTimer()` as a context object\n",
    "\n",
    "⚠️Caution⚠️: The `ScopedTimer` does not synchronize the host (CPU) with the device (GPU) by default. We must pass in `synchronize=True` for the `ScopedTimer` to synchronize before and after the timed section of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Warmup (5 steps)\n",
    "for _warmup_index in range(5):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "with wp.ScopedTimer(\"Integration\", synchronize=True) as timer:\n",
    "    # Main simulation loop\n",
    "    for _step_index in range(T):\n",
    "        wp.launch(\n",
    "            integrate_bodies,\n",
    "            dim=(num_bodies,),\n",
    "            inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "        )\n",
    "        pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "\n",
    "duration_seconds = timer.elapsed * 1e-3\n",
    "\n",
    "print(f\"Time taken for {T} timesteps: {duration_seconds} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / duration_seconds / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the PIPS values from methods 1, 2, and 3, you will notice that method 1 produces an unusually large result. This occurs because CUDA kernel launches are asynchronous with respect to the host CPU thread. When a kernel is launched using `wp.launch()`, the function schedules the kernel for execution on the GPU device but returns immediately to the CPU without waiting for the kernel to complete execution.\n",
    "\n",
    "Consequently, when using `time.perf_counter()` to measure execution time, the recorded duration is incorrectly measured, even though the actual kernel execution takes much longer. This leads to an artificially inflated PIPS calculation.\n",
    "\n",
    "In contrast, methods 2 and 3 use `wp.Event` and `wp.ScopedTimer` respectively, which properly synchronize with the GPU device and accurately measure the actual execution time of the kernel operations on the device.\n",
    "\n",
    "Interested readers can read more about concurrency, streams, and synchronization in Warp [here](https://nvidia.github.io/warp/modules/concurrency.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a78493",
   "metadata": {},
   "source": [
    "---\n",
    "## Tile-based programming approach\n",
    "\n",
    "### How can we make our SIMT method even faster?\n",
    "\n",
    "If you analyze the `body_body_interaction(...)` function in our SIMT code above, you will notice that it iterates over all particles to calculate the resulting acceleration for any given particle. Each time the contribution from a particle is calculated, the code must load both the positions (`positions[body_index]`) and masses (`masses[body_index]`) from global memory. \n",
    "\n",
    "One way to reduce this memory access overhead is to load a predefined chunk of masses and positions into shared memory within a thread block, making this data accessible to all threads in the block. With a chunk of particle data now in the faster shared memory, we can calculate acceleration contributions for this chunk much more efficiently than our previous approach, which required loading each individual particle's mass and position from the slower global memory for every computation.\n",
    "\n",
    "Starting from Warp 1.5.0, developers have access to tile-based programming in NVIDIA Warp, which achieves this goal of cooperative operations across threads in a block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick introduction to tile-based programming in Warp\n",
    "\n",
    "Before getting into details of how we can accelerate our N-body simulation using tile-based programming, we will quickly cover a few basics of tile-based programming in Warp using an example of row-wise summation of a matrix. Please also refer to the [Warp documentation on tiles](https://nvidia.github.io/warp/modules/tiles.html) for a more comprehensive introduction to tile-based programming in Warp.\n",
    "\n",
    "Our input matrix is of size $(12, 256)$ with each row $i$ containing an identical entry of value $i$ across all columns. The code snippet below initializes our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warp as wp\n",
    "\n",
    "N = 256  # Number of columns in our matrix\n",
    "column_vector = np.arange(12, dtype=np.float32).reshape(\n",
    "    -1, 1\n",
    ")  # Create a column vector of size 12 [0, 1, 2, ..., 11]\n",
    "a_np = np.tile(\n",
    "    column_vector, (1, N)\n",
    ")  # Tile the column vector 256 times to create a matrix of size (12, 256)\n",
    "a_wp = wp.array(a_np)  # Convert our numpy array to a Warp array\n",
    "\n",
    "print(f\"{a_np.shape=}\")\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our row-wise summation should be $[0, 256, 512, \\cdots, 2816]$. Can you confirm this yourself before moving ahead?\n",
    "\n",
    "Before implementing the tile-based method to do the row-wise summation of our matrix above, can you try to write the SIMT code for the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wp.kernel\n",
    "def compute(a: wp.array2d(dtype=float), b: wp.array(dtype=float)):\n",
    "    \"\"\"Compute row-wise sum of matrix using SIMT approach.\"\"\"\n",
    "    i = wp.tid()  # Get row index\n",
    "    \n",
    "    # Sum elements in row i\n",
    "    sum = wp.float32(0.0)\n",
    "    for j in range(a.shape[1]):\n",
    "        sum += a[i, j]\n",
    "    \n",
    "    b[i] = sum\n",
    "\n",
    "\n",
    "# Create output array\n",
    "b_wp = wp.empty(a_wp.shape[0], dtype=float)\n",
    "\n",
    "# Launch kernel with one thread per row\n",
    "wp.launch(compute, dim=(a_wp.shape[0],), inputs=[a_wp], outputs=[b_wp])\n",
    "\n",
    "print(b_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tiles, we can use **all threads in a block** to cooperatively load an entire row of data into a tile object and call `wp.tile_sum()` on the tile.\n",
    "\n",
    "Note that in the `compute_tiled` kernel below, we primarily call functions beginning with `wp.tile_`.\n",
    "\n",
    "`wp.launch_tiled()` is also used instead of `wp.launch()`, but all `wp.launch_tiled()` does is to add a trailing dimension of `BLOCK_DIM` to the launch grid. Hence, the following are equivalent:\n",
    "\n",
    "- `wp.launch_tiled(compute, dim=(a_wp.shape[0],), inputs=[a_wp], outputs=[b_wp], block_dim=BLOCK_DIM)`\n",
    "- `wp.launch(compute, dim=(a_wp.shape[0], BLOCK_DIM), inputs=[a_wp], outputs=[b_wp], block_dim=BLOCK_DIM)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both TILE_SIZE and BLOCK_DIM need to be runtime constants\n",
    "TILE_SIZE = 256  # Size of a single tile of data to load into registers or shared memory\n",
    "BLOCK_DIM = 256  # Number of threads in a block\n",
    "\n",
    "@wp.kernel\n",
    "def compute_tiled(a: wp.array2d(dtype=float), b: wp.array2d(dtype=float)):\n",
    "    \"\"\"Compute row-wise sum of matrix using tile-based approach.\"\"\"\n",
    "    i = wp.tid()  # Get row index\n",
    "    \n",
    "    # Load entire row into tile using all threads cooperatively\n",
    "    t = wp.tile_load(a[i], TILE_SIZE)\n",
    "    \n",
    "    # Compute sum of tile elements\n",
    "    s = wp.tile_sum(t)\n",
    "    \n",
    "    # Store result\n",
    "    wp.tile_store(b[i], s)\n",
    "\n",
    "\n",
    "# Create output array (shape is (N, 1) for tile operations)\n",
    "b_wp = wp.empty((a_wp.shape[0], 1), dtype=float)\n",
    "\n",
    "# Launch tiled kernel\n",
    "wp.launch_tiled(\n",
    "    compute_tiled,\n",
    "    dim=(a_wp.shape[0],),\n",
    "    inputs=[a_wp],\n",
    "    outputs=[b_wp],\n",
    "    block_dim=BLOCK_DIM,\n",
    ")\n",
    "\n",
    "print(b_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the `compute_tiled` kernel above, when the code calls `t = wp.tile_load(a[i], TILE_SIZE)`, it triggers a **coordinated memory operation** performed by all threads within the current CUDA thread block (which contains `BLOCK_DIM` threads).\n",
    "\n",
    "The objective is to load `TILE_SIZE` elements from array `a`, starting at global memory index `i`, directly into the **private registers** of these threads.\n",
    "\n",
    "Each thread in the block is responsible for loading a specific portion of the `TILE_SIZE` elements.\n",
    "\n",
    "- When `TILE_SIZE=256` and `BLOCK_DIM=256`, each thread loads exactly one element from `a` into its register.\n",
    "- When `TILE_SIZE=256` and `BLOCK_DIM=64`, each thread loads four elements from `a` into its register.\n",
    "- When `TILE_SIZE=256` and `BLOCK_DIM=512`, only the first 256 threads load one element each from `a` into their registers, while the remaining 256 threads remain idle.\n",
    "\n",
    "Since `TILE_SIZE` elements must ultimately be loaded, the work distribution depends on `BLOCK_DIM`. If `TILE_SIZE >= BLOCK_DIM`, each thread loads `TILE_SIZE/BLOCK_DIM` elements. Otherwise, only the first `TILE_SIZE` threads load one element each, while the remaining threads load nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tile-based programming to N-body simulation\n",
    "\n",
    "For our N-body simulation, recall that there is a Warp kernel `def integrate_bodies(...)` that calls the function `def body_body_interaction(...)`. Specifically, in the function `def body_body_interaction(...)`, we would like to load `TILE_SIZE` particle positions from global memory to the shared memory of a single CUDA block. The data for these particle positions in shared memory is accessible by all threads in the block.\n",
    "\n",
    "The loop over body interactions is now done in **phases**: After each thread computes the interactions of the particle being updated with the `TILE_SIZE` particle positions that have been loaded into shared memory, another set of `TILE_SIZE` positions is read from global memory and loaded into shared memory.\n",
    "\n",
    "We have created a new version of our `def body_body_interaction(...)` function, now called `def body_body_interaction_tile_based(...)` as shown below. We copy the `def integrate_bodies(...)` content to a new kernel `def integrate_bodies_tile_based(...)` where the only change is the call to the function `def body_body_interaction_tile_based(...)` instead of `def body_body_interaction(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 256  # Tile size selected to be the same as the default block size\n",
    "BLOCK_DIM = 256  # Size of a CUDA block\n",
    "\n",
    "@wp.func\n",
    "def body_body_interaction_tile_based(\n",
    "    num_bodies: int,\n",
    "    masses: wp.array(dtype=float),\n",
    "    body_position: wp.vec3,\n",
    "    positions: wp.array(dtype=wp.vec3),\n",
    "):\n",
    "    \"\"\"Return the acceleration of the particle at position `body_position`.\"\"\"\n",
    "    acc = wp.vec3(0.0, 0.0, 0.0)\n",
    "\n",
    "    for k in range(num_bodies / TILE_SIZE):\n",
    "        pos_tile = wp.tile_load(positions, shape=TILE_SIZE, offset=k * TILE_SIZE)\n",
    "        mass_tile = wp.tile_load(masses, shape=TILE_SIZE, offset=k * TILE_SIZE)\n",
    "\n",
    "        for body_index in range(TILE_SIZE):\n",
    "            r = pos_tile[body_index] - body_position\n",
    "\n",
    "            dist_sq = wp.length_sq(r) + SOFTENING_SQ\n",
    "\n",
    "            inv_dist = 1.0 / wp.sqrt(dist_sq)\n",
    "            inv_dist_cubed = inv_dist * inv_dist * inv_dist\n",
    "\n",
    "            acc = acc + mass_tile[body_index] * inv_dist_cubed * r\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "@wp.kernel\n",
    "def integrate_bodies_tile_based(\n",
    "    num_bodies: int,\n",
    "    dt: float,\n",
    "    masses: wp.array(dtype=float),\n",
    "    old_position: wp.array(dtype=wp.vec3),\n",
    "    velocity: wp.array(dtype=wp.vec3),\n",
    "    new_position: wp.array(dtype=wp.vec3),\n",
    "):\n",
    "    \"\"\"Integrate N-body system forward one timestep using tile-based approach.\n",
    "    Same as integrate_bodies(...) but uses tile-based acceleration calculation.\n",
    "    \"\"\"\n",
    "    i = wp.tid()\n",
    "\n",
    "    # Calculate gravitational acceleration on this particle from all others.\n",
    "    # Now using tile-based acceleration calculation\n",
    "    accel = body_body_interaction_tile_based(\n",
    "        num_bodies, masses, old_position[i], old_position\n",
    "    )\n",
    "\n",
    "    # Update velocity using forward Euler integration.\n",
    "    # v(t+dt) = v(t) + a(t) * dt\n",
    "    velocity[i] = velocity[i] + accel * dt\n",
    "\n",
    "    # Update position using the newly computed velocity.\n",
    "    # x(t+dt) = x(t) + v(t+dt) * dt\n",
    "    # Note: We use new_position array to avoid race conditions.\n",
    "    new_position[i] = old_position[i] + dt * velocity[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the tile-based N-body simulation results against the SIMT implementation\n",
    "\n",
    "Before analyzing the speedup from the tile-based programming approach for our N-body simulation, let us first validate the approach. In the cell below, we run both the SIMT code and the tile-based code for 100 timesteps, append the output from the Warp kernels in both cases to numpy arrays, and then calculate the L2 norm of the difference between the two results averaged over 100 timesteps.\n",
    "\n",
    "**Prerequisite** -- Please make sure that you have run the cell that defines both `body_body_interaction` and `integrate_bodies` in the **A baseline SIMT implementation** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100  # Number of timesteps to simulate\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses for tile-based approach\n",
    "(\n",
    "    pos_array_0,\n",
    "    pos_array_1,\n",
    "    vel_array,\n",
    "    mass_array,\n",
    "    scale,\n",
    "    init_pos_np,\n",
    ") = init_problem(num_bodies)\n",
    "\n",
    "# Initialize separate arrays for SIMT approach comparison\n",
    "# # Note that since seed is set in init_problem, we will get the same initial configurations of the particles \n",
    "\n",
    "(   pos_array_0_SIMT,\n",
    "    pos_array_1_SIMT,\n",
    "    vel_array_SIMT,\n",
    "    mass_array_SIMT,\n",
    "    scale_SIMT,\n",
    "    init_pos_np_SIMT,\n",
    ") = init_problem(num_bodies)\n",
    "\n",
    "\n",
    "# Run SIMT-based approach and store positions over time\n",
    "pos_array_over_time = []\n",
    "for _step_index in range(T):\n",
    "    # Launch SIMT-based kernel\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[\n",
    "            num_bodies,\n",
    "            dt,\n",
    "            mass_array,\n",
    "            pos_array_0,\n",
    "            vel_array,\n",
    "            pos_array_1,\n",
    "        ],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "    pos_array_over_time.append(pos_array_0.numpy())\n",
    "\n",
    "\n",
    "# Run tile-based approach and store positions over time for comparison\n",
    "pos_array_over_time_SIMT = []\n",
    "for _step_index in range(T):\n",
    "    # Launch tile-based kernel\n",
    "    wp.launch(\n",
    "        integrate_bodies_tile_based,\n",
    "        dim=(num_bodies,),  # Launch num_bodies threads\n",
    "        inputs=[\n",
    "            num_bodies,\n",
    "            dt,\n",
    "            mass_array_SIMT,\n",
    "            pos_array_0_SIMT,\n",
    "            vel_array_SIMT,\n",
    "            pos_array_1_SIMT,\n",
    "        ],\n",
    "        block_dim=BLOCK_DIM,  # Use specified block dimension for tiling\n",
    "    )\n",
    "    pos_array_0_SIMT, pos_array_1_SIMT = pos_array_1_SIMT, pos_array_0_SIMT  # Swap buffers\n",
    "    pos_array_over_time_SIMT.append(pos_array_0_SIMT.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert position arrays to numpy arrays for comparison\n",
    "pos_array_over_time = np.asarray(pos_array_over_time)\n",
    "pos_array_over_time_SIMT = np.asarray(pos_array_over_time_SIMT)\n",
    "\n",
    "# Calculate L2 error between SIMT and tile-based approaches\n",
    "# Compute norm across spatial dimensions (axis=2) then average over all particles and timesteps\n",
    "l2_error = np.mean(\n",
    "    np.linalg.norm((pos_array_over_time - pos_array_over_time_SIMT), axis=(2))\n",
    ")\n",
    "print(l2_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `l2_error` between the SIMT code and tile-based code should come out to be zero, confirming that our tile-based approach for N-body simulation accurately reproduces the physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the performance gain from tile-based approach against the SIMT implementation\n",
    "\n",
    "Now that we have verified that our tile-based approach produces accurate results, let us benchmark its performance against the SIMT implementation. For this, we will use the Method 3 approach of using `wp.ScopedTimer()` as a context object from the **Benchmarking the SIMT code** section above.\n",
    "\n",
    "In the code below, you can also replace the kernel launch call `wp.launch(integrate_bodies_tile_based ...)` with `wp.launch(integrate_bodies, ...)`. Just make sure that the cell containing both `def integrate_bodies(...)` and `def body_body_interaction(...)` has been executed before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01  # Time step for numerical integration\n",
    "num_bodies = 65536  # Number of particles in the N-body system\n",
    "SOFTENING_SQ = (\n",
    "    0.1**2\n",
    ")  # Softening parameter squared to prevent division by a very small number if two particles come closer\n",
    "T = 100\n",
    "\n",
    "\n",
    "# Initialize problem with particle positions, velocities, and masses\n",
    "pos_array_0, pos_array_1, vel_array, mass_array, scale, init_pos_np = init_problem(\n",
    "    num_bodies\n",
    ")\n",
    "\n",
    "# Warmup (5 steps)\n",
    "for _warmup_index in range(5):\n",
    "    wp.launch(\n",
    "        integrate_bodies,\n",
    "        dim=(num_bodies,),\n",
    "        inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "    )\n",
    "    pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "\n",
    "with wp.ScopedTimer(\"Tile-based benchmarking\", synchronize=True) as timer:\n",
    "    # Main simulation loop\n",
    "    for _step_index in range(T):\n",
    "        wp.launch(\n",
    "            integrate_bodies_tile_based,  # Replace with integrate_bodies for SIMT\n",
    "            dim=(num_bodies,),\n",
    "            inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "            block_dim=BLOCK_DIM\n",
    "        )\n",
    "        pos_array_0, pos_array_1 = pos_array_1, pos_array_0  # Swap buffers\n",
    "\n",
    "duration_seconds = timer.elapsed * 1e-3\n",
    "print(f\"Time taken for {T} timesteps: {duration_seconds} seconds\")\n",
    "print(f\"PIPS: {num_bodies**2 * T / duration_seconds / 1e9} billion PIPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the PIPS obtained here using the tile-based approach with those obtained from Method 3 in the **Benchmarking the SIMT code section**. On my RTX 5000 laptop, the PIPS value for the tile-based approach is approximately 280 billion compared to approximately 220 billion for the SIMT code. Thus, we can see that the tile-based approach gives a speedup of approximately 27% over our SIMT code. Note that this number is for 65536 particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autotuning Tile Parameters\n",
    "\n",
    "In the previous example, note that both the `TILE_SIZE` and `BLOCK_DIM` were set to 256.\n",
    "\n",
    "To further optimize the performance of the tile approach, we can explore optimal values for the free parameters:\n",
    "\n",
    "- Tile size (how many total positions to load from global memory at once)\n",
    "- Threads per block (how many CUDA threads are assigned to each block)\n",
    "\n",
    "The following code runs the N-body problem over a few different combinations of tile and block sizes.\n",
    "\n",
    "For getting the PIPS, we use Method 2 in the cell below.\n",
    "\n",
    "First, we need to define a `create_integrate_kernel` function that takes `tile_size` as an input parameter. This function will define both the `body_body_interaction_tile_based` Warp function for the given `TILE_SIZE` and return the corresponding `integrate_bodies_tile_based` Warp kernel that calls this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_integrate_kernel(tile_size: int):\n",
    "    \"\"\"Returns the tile-based integration kernel with a given tile size\n",
    "\n",
    "    For a given tile size, this function defines both the body_body_interaction_tile_based(...) function\n",
    "    and the integrate_bodies_tile_based(...) kernel.\n",
    "    \n",
    "    Args:\n",
    "        tile_size (int): The size of the tile to use for the integration kernel\n",
    "    \n",
    "    Returns:\n",
    "        wp.kernel: The tile-based integration kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    TILE_SIZE = tile_size  # Set the TILE_SIZE to the input tile_size to the function\n",
    "\n",
    "    @wp.func\n",
    "    def body_body_interaction_tile_based(\n",
    "        num_bodies: int,\n",
    "        masses: wp.array(dtype=float),\n",
    "        body_position: wp.vec3,\n",
    "        positions: wp.array(dtype=wp.vec3),\n",
    "    ):\n",
    "        \"\"\"Same as the body_body_interaction_tile_based function defined earlier.\n",
    "        \"\"\"\n",
    "        acc = wp.vec3(0.0, 0.0, 0.0)\n",
    "\n",
    "        for k in range(num_bodies / TILE_SIZE):\n",
    "            # TILE_SIZE = tile_size instead of 256 now\n",
    "            pos_tile = wp.tile_load(positions, shape=TILE_SIZE, offset=k * TILE_SIZE) \n",
    "            mass_tile = wp.tile_load(masses, shape=TILE_SIZE, offset=k * TILE_SIZE)\n",
    "\n",
    "            for body_index in range(TILE_SIZE):\n",
    "                r = pos_tile[body_index] - body_position\n",
    "\n",
    "                dist_sq = wp.length_sq(r) + SOFTENING_SQ\n",
    "\n",
    "                inv_dist = 1.0 / wp.sqrt(dist_sq)\n",
    "                inv_dist_cubed = inv_dist * inv_dist * inv_dist\n",
    "\n",
    "                acc = acc + mass_tile[body_index] * inv_dist_cubed * r\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "    @wp.kernel\n",
    "    def integrate_bodies_tile_based(\n",
    "        num_bodies: int,\n",
    "        dt: float,\n",
    "        masses: wp.array(dtype=float),\n",
    "        old_position: wp.array(dtype=wp.vec3),\n",
    "        velocity: wp.array(dtype=wp.vec3),\n",
    "        new_position: wp.array(dtype=wp.vec3),\n",
    "    ):\n",
    "        \"\"\"Same as integrate_bodies(...) but uses tile-based acceleration calculation.\n",
    "        \"\"\"\n",
    "        i = wp.tid()\n",
    "\n",
    "        accel = body_body_interaction_tile_based(\n",
    "            num_bodies, masses, old_position[i], old_position\n",
    "        )\n",
    "\n",
    "        # Update velocity using forward Euler integration.\n",
    "        # v(t+dt) = v(t) + a(t) * dt\n",
    "        velocity[i] = velocity[i] + accel * dt\n",
    "\n",
    "        # Update position using the newly computed velocity.\n",
    "        # x(t+dt) = x(t) + v(t+dt) * dt\n",
    "        # Note: We use new_position array to avoid race conditions.\n",
    "        new_position[i] = old_position[i] + dt * velocity[i]\n",
    "    return integrate_bodies_tile_based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build a `def benchmark_warp(...)` function that takes a given combination of `TILE_SIZE` and `BLOCK_DIM`, runs the N-body problem for 100 steps after an initial 5 warmup steps, and records the PIPS using Method 2 defined in the **Benchmarking the SIMT code** section above. The `def benchmark_warp(...)` function calls the `def create_integrate_kernel(...)` function defined above to obtain the corresponding Warp integration kernel with the specified `TILE_SIZE` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_warp(config: list[int]):\n",
    "    \"\"\"Benchmark N-body simulation performance with given tile and block dimensions.\n",
    "    \n",
    "    This function runs an N-body simulation for 100 steps after 5 warmup steps\n",
    "    and measures the performance in interactions per second.\n",
    "    \n",
    "    Args:\n",
    "        config (list[int]): Configuration parameters where:\n",
    "            - config[0]: TILE_DIM - tile dimension for tiled computation\n",
    "            - config[1]: BLOCK_DIM - block dimension for GPU kernel launch\n",
    "    \n",
    "    Returns:\n",
    "        float: Number of particle interactions per second achieved during\n",
    "               the benchmark run.\n",
    "    \"\"\"\n",
    "    # Extract configuration parameters\n",
    "    TILE_SIZE = config[0]\n",
    "    BLOCK_DIM = config[1]\n",
    "\n",
    "    # Create the integration kernel with specified tile dimension\n",
    "    integrate = create_integrate_kernel(TILE_SIZE)\n",
    "\n",
    "    # Number of simulation steps to run for benchmarking\n",
    "    total_steps = 100\n",
    "\n",
    "    # Reset the problem to initial state\n",
    "    (\n",
    "        pos_array_0,\n",
    "        pos_array_1,\n",
    "        vel_array,\n",
    "        mass_array,\n",
    "        scale,\n",
    "        init_pos_np,\n",
    "    ) = init_problem(num_bodies)\n",
    "    \n",
    "    # Using the Method 2 to get the PIPS\n",
    "    start_event = wp.Event(enable_timing=True) \n",
    "    end_event = wp.Event(enable_timing=True)\n",
    "\n",
    "    for _warmup_index in range(5):\n",
    "        wp.launch(\n",
    "            integrate,\n",
    "            dim=(num_bodies,),\n",
    "            inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "            block_dim=BLOCK_DIM,\n",
    "        )\n",
    "\n",
    "        # Swap position arrays for next iteration (double buffering)\n",
    "        (pos_array_0, pos_array_1) = (pos_array_1, pos_array_0)\n",
    "\n",
    "    # Record the start time for benchmark measurement\n",
    "    wp.record_event(start_event)\n",
    "\n",
    "    # Run the actual benchmark simulation steps\n",
    "    for _step_index in range(total_steps):\n",
    "        wp.launch(\n",
    "            integrate,\n",
    "            dim=(num_bodies,),\n",
    "            inputs=[num_bodies, dt, mass_array, pos_array_0, vel_array, pos_array_1],\n",
    "            block_dim=BLOCK_DIM,\n",
    "        )\n",
    "\n",
    "        # Swap arrays\n",
    "        (pos_array_0, pos_array_1) = (pos_array_1, pos_array_0)\n",
    "\n",
    "    # Record the end event\n",
    "    wp.record_event(end_event)\n",
    "\n",
    "    duration_seconds = wp.get_event_elapsed_time(start_event, end_event) * 1e-3\n",
    "\n",
    "    # Calculate performance metric: total particle interactions per second\n",
    "    # Each step involves num_bodies * num_bodies interactions\n",
    "    interactions_per_second = num_bodies * num_bodies * total_steps / duration_seconds\n",
    "\n",
    "    return interactions_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive matplotlib widgets for notebook\n",
    "%matplotlib widget\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define parameter ranges for performance testing\n",
    "# Combinations to run for block_dim and tile_size\n",
    "block_list = [64, 256, 512, 1024]\n",
    "tile_list = [64, 256, 512, 1024]\n",
    "\n",
    "# Generate all possible combinations of tile and block sizes\n",
    "configs = list(product(tile_list, block_list))\n",
    "\n",
    "# Run benchmarks for each configuration and collect results\n",
    "results = []\n",
    "for c in configs:\n",
    "    perf_result = benchmark_warp(c)\n",
    "    results.append(perf_result / 1e9)  # Convert the output PIPS to billions\n",
    "\n",
    "# Create labels for x-axis showing tile and block size combinations\n",
    "labels = [f\"Tile{c[0]}/Block{c[1]}\" for c in configs]\n",
    "\n",
    "# Create the bar chart with performance results\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(results)), results, color=\"#76b900\")\n",
    "plt.xlabel(\"Configuration\")\n",
    "plt.ylabel(\"Performance (Billion interactions per second)\")\n",
    "plt.title(\"N-Body Simulation Performance with Different Tile and Block Sizes\")\n",
    "plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on top of each bar for better readability\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 5,\n",
    "        f\"{results[i]:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        rotation=0,\n",
    "    )\n",
    "\n",
    "# Apply formatting and styling to the plot\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your specific GPU, what combination of `TILE_SIZE` and `BLOCK_DIM` gave you the best speedup?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a98f7",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored two different approaches to implementing the N-body simulation using NVIDIA Warp.\n",
    "\n",
    "We started with a SIMT (Single Instruction, Multiple Threads) implementation where each thread computes the acceleration for one particle by iterating through all other particles. While this approach would achieves significant speedup over CPU implementation, it can be accelerated further by utilizing a tile-based approach.\n",
    "\n",
    "We then explored the tile-based programming approach in Warp that leverages shared memory and cooperative thread operations to achieve better performance. By loading chunks of particle data into shared memory that all threads in a block can access, we reduced global memory accesses and achieved significant speedup over the SIMT approach.\n",
    "\n",
    "Through this implementation, we learned:\n",
    "- How to profile GPU code using two different timing methods in Warp (CUDA events and Warp's `ScopedTimer`)\n",
    "- The importance of memory access patterns in GPU performance\n",
    "- How tile-based programming can improve performance for algorithms where threads can cooperate together on a chunk of data for certain operations\n",
    "\n",
    "The N-body simulation serves as an excellent example of how advanced GPU programming techniques can be applied to accelerate scientific computing workloads while maintaining the simplicity and readability of Python code in Warp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0df8d",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "You can find the Warp GitHub repository and documentation below:\n",
    "* \"NVIDIA/warp: A Python framework for accelerated simulation, data generation and spatial computing.\", GitHub, https://github.com/NVIDIA/warp.\n",
    "* Warp Developers, \"NVIDIA Warp Documentation,\" GitHub Pages, https://nvidia.github.io/warp.\n",
    "\n",
    "For more information on N-body simulations and GPU optimizations:\n",
    "* Lars Nyland, Mark Harris, and Jan Prins. \"Fast N-Body Simulation with CUDA.\" In GPU Gems 3, Chapter 31. NVIDIA Developer Zone. https://developer.nvidia.com/gpugems/gpugems3/part-v-physics-simulation/chapter-31-fast-n-body-simulation-cuda"
   ]
  }
 ],
 "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
