{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial vs Parallel\n",
    "\n",
    "## Content\n",
    "* [Segmented Sum](#Segmented-Sum)\n",
    "* [Reduce by Key](#Reduce-by-Key)\n",
    "* [Exercise: Segmented Sum Optimization](01.05.02-Exercise-Segmented-Sum-Optimization.ipynb)\n",
    "* [Exercise: Segmented Mean](01.05.03-Exercise-Segmented-Mean.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"): # If running in Google Colab:\n",
    "  !mkdir -p Sources\n",
    "  !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/cuda-cpp-tutorial/notebooks/01.05-Serial-vs-Parallel/Sources/ach.h -nv -O Sources/ach.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "At this point, we have discussed how to on-ramp to GPU programming with parallel algorithms.\n",
    "We've also covered techniques that can help you extend these parallel algorithms to meet your specific use cases.\n",
    "As you find more applications for parallel algorithms, there's a possibility that you will get unexpected performance.\n",
    "To avoid unexpected performance results, \n",
    "you'll need a firm understanding of the difference between serial and parallel execution.\n",
    "To see what we mean, let's consider another example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmented Sum\n",
    "This time, let's say we are interested in the segment close to the heat source. \n",
    "Let's try changing the grid size to \"zoom\" in into that part.\n",
    "Instead of looking at the visualization, we'll be looking at the total temperature in each row.\n",
    "This row-based computation can be framed as a segmented problem. A *segmented sum* is defined as taking a single input array and, given a segment size, calculating the sum of each segment.\n",
    "\n",
    "![Segmented Sum](Images/segmented-sum.svg \"Segmented Sum\")\n",
    "\n",
    "We could build a segmented sum on top of `thrust::tabulate`. \n",
    "The `tabulate` algorithm receives a sequence and a function.\n",
    "It then applies this function to index of each element in the sequence, and stores the result into the provided sequence.\n",
    "For example, after the following invocation:\n",
    "\n",
    "```c++\n",
    "thrust::universal_vector<int> vec(4);\n",
    "thrust::tabulate(\n",
    "   thrust::device, vec.begin(), vec.end(), \n",
    "   []__host__ __device__(int index) -> int { \n",
    "      return index * 2; \n",
    "   });\n",
    "```\n",
    "\n",
    "`vec` would store `{0, 2, 4, 6}`. \n",
    "We can use this algorithm to implement our segmented sum as follows:\n",
    "\n",
    "```c++\n",
    "thrust::universal_vector<float> sums(num_segments);\n",
    "thrust::tabulate(\n",
    "   thrust::device, sums.begin(), sums.end(), \n",
    "   []__host__ __device__(int segment_id) -> float {\n",
    "      return compute_sum_for(segment_id);\n",
    "   });\n",
    "```\n",
    "\n",
    "As we implement the algorithm, let's consider its performance from a new perspective.\n",
    "Reduction is a memory-bound algorithm.\n",
    "This means that instead of analyzing its performance in terms of elapsed time,\n",
    "we could take a look at how many bytes does our implementation process in a second.\n",
    "This metric is called _achieved throughput_. \n",
    "By contrasting it with the peak theoretical bandwidth of our GPU,\n",
    "we'll understand if our implementation is efficient or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/naive-segmented-sum.cpp\n",
    "#include <cstdio>\n",
    "#include <chrono>\n",
    "\n",
    "#include <thrust/tabulate.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <thrust/universal_vector.h>\n",
    "\n",
    "thrust::universal_vector<float> row_temperataures(\n",
    "    int height, int width,\n",
    "    const thrust::universal_vector<float>& temp)\n",
    "{\n",
    "    // allocate vector to store sums\n",
    "    thrust::universal_vector<float> sums(height);\n",
    "\n",
    "    // take raw pointer to `temp`\n",
    "    const float *d_temp_ptr = thrust::raw_pointer_cast(temp.data());\n",
    "\n",
    "    // compute row sum\n",
    "    thrust::tabulate(thrust::device, sums.begin(), sums.end(), [=]__host__ __device__(int row_id) {\n",
    "        float sum = 0;\n",
    "        for (int i = 0; i < width; i++) {\n",
    "            sum += d_temp_ptr[row_id * width + i];\n",
    "        }\n",
    "        return sum;\n",
    "    });\n",
    "\n",
    "    return sums;\n",
    "}\n",
    "\n",
    "thrust::universal_vector<float> init(int height, int width) {\n",
    "  const float low = 15.0;\n",
    "  const float high = 90.0;\n",
    "  thrust::universal_vector<float> temp(height * width, low);\n",
    "  thrust::fill(thrust::device, temp.begin(), temp.begin() + width, high);\n",
    "  return temp;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int height = 16;\n",
    "    int width = 16777216;\n",
    "    thrust::universal_vector<float> temp = init(height, width);\n",
    "\n",
    "    auto begin = std::chrono::high_resolution_clock::now();\n",
    "    thrust::universal_vector<float> sums = row_temperataures(height, width, temp);\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    const double seconds = std::chrono::duration<double>(end - begin).count();\n",
    "    const double gigabytes = static_cast<double>(temp.size() * sizeof(float)) / 1024 / 1024 / 1024;\n",
    "    const double throughput = gigabytes / seconds;\n",
    "\n",
    "    std::printf(\"computed in %g s\\n\", seconds);\n",
    "    std::printf(\"achieved throughput: %g GB/s\\n\", throughput);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --extended-lambda -o /tmp/a.out Sources/naive-segmented-sum.cpp -x cu -arch=native # build executable\n",
    "!/tmp/a.out # run executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the achieved throughput and contrast it with maximal bandwidth.\n",
    "Out implementation achieves less than a percent of what GPU can provide.\n",
    "The reason our implementation underperforms is due to the way we used `thrust::tabulate`:\n",
    "\n",
    "```c++\n",
    "thrust::tabulate(thrust::device, sums.begin(), sums.end(), [=]__host__ __device__(int segment_id) {\n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < segment_size; i++) {\n",
    "        sum += d_values_ptr[segment_id * segment_size + i];\n",
    "    }\n",
    "    return sum; \n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce by Key\n",
    "\n",
    "GPUs are massively parallel processors.\n",
    "That said, code that ends up being executed by GPU doesn't get magically parallelized.\n",
    "The `for` loop in the operator we provided to `thrust::tabulate` is executed sequentially. \n",
    "Tabulate could process each of the 16 elements in parallel, while the operator processes over 16 million elements.\n",
    "To fix performance, let's try increasing parallelism.\n",
    "\n",
    "To do that, we can try the `thrust::reduce_by_key` algorithm, which is a generalization of the `thrust::reduce` algorithm. \n",
    "Instead of reducing the sequence into a single value,\n",
    "it allows you to reduce segments of values. \n",
    "To distinguish these segments, you have to provide keys. \n",
    "Consecutive keys that are equal form a *segment*.\n",
    "As the output, `reduce_by_key` returns one value per segment. \n",
    "\n",
    "For example:\n",
    "\n",
    "```c++\n",
    "int in_keys[] = {1, 1, 1, 3, 3};\n",
    "int in_vals[] = {1, 2, 3, 4, 5};\n",
    "int out_keys[2];\n",
    "int out_vals[2];\n",
    "\n",
    "thrust::reduce_by_key(in_keys, in_keys + 5, in_vals, out_keys, out_vals);\n",
    "// out_keys = {1, 3}\n",
    "// out_vals = {6, 9}\n",
    "```\n",
    "\n",
    "Lets try to frame our segmented sum in terms of reduce by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/reduce-by-key.cpp\n",
    "#include \"ach.h\"\n",
    "\n",
    "thrust::universal_vector<float> row_temperatures(\n",
    "    int height, int width,\n",
    "    thrust::universal_vector<int>& row_ids,\n",
    "    thrust::universal_vector<float>& temp)\n",
    "{\n",
    "    thrust::universal_vector<float> sums(height);\n",
    "    thrust::reduce_by_key(\n",
    "        thrust::device,\n",
    "        row_ids.begin(), row_ids.end(),   // input keys\n",
    "        temp.begin(),                     // input values\n",
    "        thrust::make_discard_iterator(),  // output keys\n",
    "        sums.begin());                    // output values\n",
    "\n",
    "    return sums;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --extended-lambda -o /tmp/a.out Sources/reduce-by-key.cpp -x cu -arch=native # build executable\n",
    "!/tmp/a.out # run executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not interested in output keys, so we made a `discard` iterator. \n",
    "This technique often helps you save memory bandwidth when you don't need certain parts of the algorithm's output.\n",
    "Speaking of bandwidth, we've got much better results now. \n",
    "That's because we eliminated the serialization that was dominating execution time. \n",
    "However, there's still an issue: Now we are reading keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Proceed to [the next exercise](01.05.02-Exercise-Segmented-Sum-Optimization.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
